# ì£¼ì˜ ë¬´ì‹œ
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# ì„ë² ë”© ëª¨ë¸
from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer("jhgan/ko-sroberta-multitask")

# Milvus ì—°ê²° ì„¤ì •
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, list_collections, utility
import requests
from dotenv import load_dotenv
import os
import pandas as pd
from konlpy.tag import Okt
import re
from groq import Groq
from sklearn.metrics.pairwise import cosine_similarity
from langgraph.graph import StateGraph, END
from typing import Dict, Any, List, Optional
from langchain_community.tools.tavily_search import TavilySearchResults

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("KAMIS_API_KEY")
api_id = os.getenv("KAMIS_ID")
groq_api_key = os.getenv(f"OPENAI_KEY1")
milvus_host = os.getenv("MILVUS_HOST", "localhost")
milvus_port = os.getenv("MILVUS_PORT", "19530")
collection_name = "market_price_docs"


# CSV íŒŒì¼ ì„ë² ë”© ë° Milvusì— ì €ì¥
def embed_and_store_csv(csv_path="sales/info_20240812.csv"):
    df = pd.read_csv(csv_path, encoding="euc-kr")
    df['í’ˆëª©'] = df['í’ˆëª©'].fillna("ì •ë³´ ì—†ìŒ")
    docs = []
    for _, row in df.iterrows():
        doc = f"{row['íŒë§¤ì¥ ì´ë¦„']} ({row['ì£¼ì†Œ']} / ì£¼ìš” í’ˆëª©: {row['í’ˆëª©']})"
        docs.append(doc)
    if docs:
        embeddings = embedder.encode(docs)
        collection.insert([embeddings.tolist(), docs], fields=["embedding", "text"])

def check_collection():
    global collection
    connections.connect("default", host=milvus_host, port=milvus_port)

    if collection_name in utility.list_collections():
        collection = Collection(collection_name)
        collection.load()
        
        # ì‹¤ì œ ì¿¼ë¦¬ë¡œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            print(f"ğŸ” ì»¬ë ‰ì…˜ '{collection_name}' ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì¤‘...")
            
            # ì‹¤ì œ ì¿¼ë¦¬ë¡œ ë°ì´í„° í™•ì¸
            sample_results = collection.query(
                expr="id >= 0",
                output_fields=["id", "text"],
                limit=1
            )
            
            has_data = len(sample_results) > 0
            print(f"ì¿¼ë¦¬ ê²°ê³¼: {len(sample_results)}ê°œ")
            print(f"ğŸ” ë°ì´í„° ì¡´ì¬ ì—¬ë¶€: {'ìˆìŒ' if has_data else 'ì—†ìŒ'}")
            
            if has_data:
                print(f"âœ… ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ë³´ì¡´í•©ë‹ˆë‹¤.")
                return collection
            else:
                print(f"âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                utility.drop_collection(collection_name)
                print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì™„ë£Œ")
                
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                fields = [
                    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
                    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512),
                ]
                schema = CollectionSchema(fields, "ì‹œì¥ ê°€ê²© ë¬¸ì„œ ì»¬ë ‰ì…˜")
                collection = Collection(collection_name, schema)
                print(f"ğŸ”„ ì»¬ë ‰ì…˜ '{collection_name}' ì¬ìƒì„± ì™„ë£Œ")
                
                # ë°ì´í„° ì‚½ì…
                embed_and_store_csv()
                print(f"âœ… ë°ì´í„° ì‚½ì… ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ë³´ì¡´í•©ë‹ˆë‹¤.")
            return collection
    else:
        # ì»¬ë ‰ì…˜ì´ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512),
        ]
        schema = CollectionSchema(fields, "ì‹œì¥ ê°€ê²© ë¬¸ì„œ ì»¬ë ‰ì…˜")
        collection = Collection(collection_name, schema)
        print(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        embed_and_store_csv()

    # ì»¬ë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„± (ì„ë² ë”© í•„ë“œì— ëŒ€í•´)
    if not collection.has_index():
        index_params = {
            "metric_type": "IP",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    return collection

# ì§ˆë¬¸ ë¶„ë¥˜ í•¨ìˆ˜
def classify_question_simple(query: str) -> str:
    """í•µì‹¬ ì˜ë„ í‚¤ì›Œë“œë§Œìœ¼ë¡œ ì§ˆë¬¸ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    query_lower = query.lower()
    
    # í•µì‹¬ ì˜ë„ í‚¤ì›Œë“œ (ê°€ì¥ ì¤‘ìš”í•œ ê²ƒë“¤ë§Œ)
    selling_intent = ['íŒ”ê³  ì‹¶ì–´', 'íŒ” ìˆ˜ ìˆ', 'ê±°ë˜', 'íŒë§¤', 'ë§¤ë§¤', 'íŒ”ë˜','íŒ”ê³ ì‹¶ì–´','íŒ”ìˆ˜ ìˆ','íŒ”ìˆ˜ìˆ','íŒ” ìˆ˜ìˆ', 'íŒ”ê¹Œ', 'íŒ”ë©´', 'íŒŒëŠ”ê²Œ', 'íŒŒëŠ” ê²ƒ', 'íŒŒëŠ”ê²ƒ']
    price_intent = ['ê°€ê²©', 'ì‹œì„¸', 'ì–¼ë§ˆ', 'ê°’', 'ì›']
    location_intent = ['íŒŒëŠ” ê³³', 'íŒë§¤ì ', 'ì§ë§¤ì¥', 'ì‹œì¥', 'ì–´ë””', 'íŒŒëŠ”ê³³','íŒë§¤ì²˜']
    
    # "ë†ì‘ë¬¼"ì´ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
    if "ë†ì‘ë¬¼" in query_lower:
        if any(keyword in query_lower for keyword in selling_intent):
            return "íŒë§¤ì²˜" # "ë†ì‘ë¬¼ì„ íŒ”ê³  ì‹¶ì–´" â†’ íŒë§¤ì²˜
        else:
            return "ì •ë³´ ë¶€ì¡±"  # "ë†ì‘ë¬¼"ë§Œ
    
    # ì¼ë°˜ì ì¸ ë¶„ë¥˜ ë¡œì§
    if any(keyword in query_lower for keyword in selling_intent):
        return "ì‹œì„¸+íŒë§¤ì²˜"  # êµ¬ì²´ì ì¸ ì‘ë¬¼ëª… + íŒ”ê³  ì‹¶ë‹¤ëŠ” ì˜ë„
    
    if any(keyword in query_lower for keyword in price_intent):
        if any(keyword in query_lower for keyword in location_intent):
            return "ì‹œì„¸+íŒë§¤ì²˜"  # ê°€ê²© + ìœ„ì¹˜ ëª¨ë‘ ìš”êµ¬
        else:
            return "ì‹œì„¸"  # ê°€ê²©ë§Œ ìš”êµ¬
    
    if any(keyword in query_lower for keyword in location_intent):
        return "íŒë§¤ì²˜"  # ìœ„ì¹˜ë§Œ ìš”êµ¬
    
    return "ì‹œì„¸+íŒë§¤ì²˜"

# api ìš”ì²­
def fetch_api_data(query=None):
    url = "http://www.kamis.or.kr/service/price/xml.do?action=dailySalesList"
    params = {
        "p_cert_key": api_key,
        "p_cert_id": api_id,
        "p_returntype": "json"
    }
    response = requests.get(url, params=params)
    docs = []
    if response.status_code == 200:
        data = response.json()
        items = []
        price = data.get("price", {})
        if isinstance(price, dict):
            items = price.get("item", [])
        elif isinstance(price, list):
            items = price
        if isinstance(items, dict):
            items = [items]

        def safe_val(val):
            if isinstance(val, list):
                return val[0] if val else ""
            return val if val is not None else ""

        # ì¿¼ë¦¬ ê¸°ë°˜ í•„í„°ë§
        keywords = extract_keywords(query)
        filtered_items = []
        for item in items:
            item_name_full = safe_val(item.get('item_name', ''))
            item_name_parts = item_name_full.split('/')
            item_names = [part.strip() for part in item_name_parts]
            match_count = sum([q == name for q in keywords for name in item_names])
            partial_count = sum([q in name for q in keywords for name in item_names])
            included_keywords = [q for q in keywords if any(q in name for name in item_names)]
            score = 0
            if keywords:
                if match_count > 0:
                    score = 3 + len(included_keywords)  # ì™„ì „ ì¼ì¹˜ + í‚¤ì›Œë“œ ê°œìˆ˜
                elif partial_count > 0:
                    score = 2 + len(included_keywords)  # ë¶€ë¶„ ì¼ì¹˜ + í‚¤ì›Œë“œ ê°œìˆ˜
                else:
                    score = len(included_keywords)      # í‚¤ì›Œë“œ ì¼ë¶€ë§Œ í¬í•¨
            else:
                score = 0
            filtered_items.append((score, item))

        filtered_items.sort(key=lambda x: x[0], reverse=True)
        filtered_items = [item for _, item in filtered_items]

        for item in filtered_items:
            category = safe_val(item.get('category_name', ''))
            if category not in ['ìˆ˜ì‚°ë¬¼', 'ì¶•ì‚°ë¬¼'] and safe_val(item.get('product_cls_name', '')) != 'ì†Œë§¤':
                direction_raw = safe_val(item.get('direction', ''))
                value_raw = safe_val(item.get('value', ''))
                dpr1 = safe_val(item.get('dpr1', ''))
                dpr2 = safe_val(item.get('dpr2', ''))
                day3 = safe_val(item.get('day3', ''))
                dpr3 = safe_val(item.get('dpr3', ''))
                day4 = safe_val(item.get('day4', ''))
                dpr4 = safe_val(item.get('dpr4', ''))

                try:
                    dpr1_val = int(str(dpr1).replace(',', '').replace(' ', '') or '0')
                    dpr2_val = int(str(dpr2).replace(',', '').replace(' ', '') or '0')
                    diff = abs(dpr1_val - dpr2_val)
                except (ValueError, TypeError):
                    diff = 0
                
                change_str = "ì™€ ë³€ë™ ì—†ëŠ”"
                if str(direction_raw) == "0":
                    change_str = f"ë³´ë‹¤ {value_raw}%({diff}ì›) ê°ì†Œí•œ"
                elif str(direction_raw) == "1":
                    change_str = f"ë³´ë‹¤ {value_raw}%({diff}ì›) ì¦ê°€í•œ"
                
                doc = (
                    f"{safe_val(item.get('item_name', ''))} ({safe_val(item.get('unit', ''))})ì˜ ê°€ê²©ì€ ì–´ì œ"
                    f"{change_str} {dpr1}ì› ì…ë‹ˆë‹¤."
                )
                if dpr3 and str(dpr3).strip() != "" and str(dpr3).strip() != "ì›":
                    doc += f"{day3}ì—ëŠ” {dpr3}ì›, "
                if dpr4 and str(dpr4).strip() != "" and str(dpr4).strip() != "ì›":
                    doc += f"{day4}ì—ëŠ” {dpr4}ì› ì´ì—ˆìŠµë‹ˆë‹¤."
                docs.append(doc)
    else:
        print("API í˜¸ì¶œ ì‹¤íŒ¨:", response.status_code)

    if docs and any(any(k in doc for k in extract_keywords(query)) for doc in docs):
        return docs
    else:
        return ["í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤."]

# Milvusì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
def search_market_docs(query, top_k=3):
    # ì „ì—­ ë³€ìˆ˜ collectionì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë¡œì»¬ì—ì„œ ì²˜ë¦¬
    try:
        # Milvus ì—°ê²°
        connections.connect("default", host=milvus_host, port=milvus_port)
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ
        if collection_name in utility.list_collections():
            local_collection = Collection(collection_name)
            local_collection.load()
            print(f"âœ… Milvus ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"âŒ Milvus ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            connections.disconnect("default")
            return ["íŒë§¤ì  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        
        # ì „ì²´ ì¿¼ë¦¬ë¡œ í•œ ë²ˆë§Œ ê²€ìƒ‰
        all_results = []

        query_nouns = extract_keywords(query)

        # ë¯¸ë¦¬ ì •ì˜ëœ ì§€ì—­ëª… ë¦¬ìŠ¤íŠ¸ì™€ ëª…ì‚¬ í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ ì§€ì—­ëª…ë§Œ ì¶”ì¶œ
        predefined_locations = ['í•¨í‰','ì„œì‚°','ëŒ€ì „', 'ì¶˜ì²œ','ê´‘ì£¼', 'ê²½ì‚°', 'ê°•ë™êµ¬', 'íƒœì•ˆ', 'ì„±ì£¼', 'ì°½ì›', 'ìš©ì¸', 'ìš¸ì£¼', 'ìˆœì²œ', 'ê²½ì£¼', 'ì–‘í‰', 'ìš¸ì‚°ê´‘ì—­', 'ì˜ì•”', 'ê¹€ì œ', 'ê³ ì°½', 'ì „ì£¼', 'í•˜ë™', 'ì œì²œ', 'í™ì„±', 'í™”ì„±', 'ì˜ì™•', 'ë‹´ì–‘', 'ì§„ì£¼', 'ì‚¬ì²œ', 'ë‚¨ì–‘ì£¼', 'ì—¬ìˆ˜', 'ìœ ì„±êµ¬', 'ì •ì', 'í™ì²œ', 'ë‚¨ì›', 'ë™êµ¬', 'ë‹¬ì„œêµ¬', 'ë‚¨í•´', 'ì˜ë™', 'ì„œêµ¬', 'ê³„ë£¡', 'ê³ ì„±', 'ê³ ì–‘', 'í‰íƒ', 'ë‚¨êµ¬', 'ìš¸ì§„', 'ë‚˜ì£¼', 'ì „ë¼ë¶ë„', 'ìµì‚°', 'ë¶€ì—¬', 'ì²­ë„', 'í•©ì²œ', 'í¬í•­', 'ë´‰í™”', 'ë¬¸ê²½', 'ê¹€í•´', 'í•¨ì–‘', 'ë¶êµ¬', 'ì² ì›', 'í™”ìˆœ', 'ìƒì£¼', 'ê²½ë¶ë„', 'ì•ˆì‚°', 'ì²­ì–‘', 'ì¶©ì£¼', 'ê¹€ì²œ', 'ì˜ê´‘', 'ì„±ë‚¨', 'ì „ë¼ë‚¨ë„', 'ë‹¬ì„±', 'ì¸ì œ', 'ì²œì•ˆ', 'ì œì£¼', 'ì›ì£¼', 'ê°€í‰', 'ì™„ì£¼', 'ì œì²œì‹œ', 'ì„±ì£¼êµ°', 'ê³ ì„±êµ°', 'ì§„ì²œ', 'ê±°ì°½', 'ì²­ì£¼', 'ê¹€í¬', 'í™”ì„±ì‹œ', 'ì™„ë„', 'í•¨ì•ˆ', 'ì˜¥ì²œ', 'ê¹€í•´ì‹œ', 'í•´ë‚¨', 'ë¬´ì•ˆ', 'ì˜ˆì‚°', 'ê¸ˆì‚°', 'ê°•ì„œêµ¬', 'ìƒë‹¹êµ¬', 'ì†¡íŒŒêµ¬', 'ê³µë„ì', 'ê³¡ì„±', 'ìš¸ë¦‰êµ°', 'ì„œê·€í¬', 'ì •ì„ ', 'í‰ì°½', 'ì–‘ì£¼', 'í¬ì²œ', 'ì§„ì•ˆ', 'ì„¸ì¢…']
        locations = [kw for kw in query_nouns if kw in predefined_locations or any(suffix in kw for suffix in ['ì‹œ', 'êµ°', 'êµ¬', 'ë„'])]

        # 1. ì§€ì—­ í‚¤ì›Œë“œ ì„ë² ë”© ê²€ìƒ‰
        if locations:
            region_query = " ".join(locations)
            region_vec = embedder.encode([region_query])[0]
            region_results = local_collection.search(
                data=[region_vec],
                anns_field="embedding",
                param={"metric_type": "IP", "params": {"nprobe": 20}},
                limit=200,
                output_fields=["text"],
            )
            if region_results and region_results[0]:
                all_results.extend([hit.entity.get("text") for hit in region_results[0]])

        # 2. ì „ì²´ ì¿¼ë¦¬ ì„ë² ë”© ê²€ìƒ‰
        query_vec = embedder.encode([query])[0]
        query_results = local_collection.search(
            data=[query_vec],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"nprobe": 20}},
            limit=200,
            output_fields=["text"],
        )
        if query_results and query_results[0]:
            all_results.extend([hit.entity.get("text") for hit in query_results[0]])

        # ì¤‘ë³µ ì œê±°
        all_results = list(dict.fromkeys(all_results))

        found_results = []
        for result_text in all_results:
            if any(loc in result_text for loc in locations):
                found_results.append(result_text)

        if not found_results:
            connections.disconnect("default")
            return ["í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."]
        else:
            def overlap_score(result_text):
                item_part = result_text.split('(')[0].strip() if 'ì£¼ìš” í’ˆëª©:' in result_text else result_text
                item_names = [x.strip() for x in re.split(r'[/,]', item_part)]
                query_strip = query.strip()
                query_nouns_set = set(extract_keywords(query_strip))
                current_score = 0
                if query_strip in item_names:
                    current_score += 10000
                for name in item_names:
                    name_nouns_set = set(extract_keywords(name))
                    if query_nouns_set.issubset(name_nouns_set):
                        current_score += 1000
                    current_score += len(query_nouns_set.intersection(name_nouns_set)) * 100
                for name in item_names:
                    if any(qn in name for qn in query_nouns_set):
                        if not any(qn in extract_keywords(name) for qn in query_nouns_set):
                            current_score += 1
                return current_score
    
            found_results.sort(key=overlap_score, reverse=True)
            connections.disconnect("default")
            return found_results[:top_k]
            
    except Exception as e:
        print(f"âŒ Milvus ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        try:
            connections.disconnect("default")
        except:
            pass
        return ["íŒë§¤ì  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

def supplement_missing_info_with_web_search(query: str, missing_info_type: str, existing_context: dict) -> dict:
    """ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤."""
    print(f"ğŸ” ì›¹ ê²€ìƒ‰ìœ¼ë¡œ {missing_info_type} ì •ë³´ ë³´ì™„ ì¤‘...")
    
    supplemented_context = existing_context.copy()
    
    try:
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        
        if not tavily_api_key:
            print("âš ï¸ Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return supplemented_context
        
        tavily_tool = TavilySearchResults(max_results=5, api_key=tavily_api_key)
        
        search_queries = []
        if missing_info_type == "íŒë§¤ì²˜":
            vendor_types = ["ë†ì‚°ë¬¼ ê³µíŒì¥", "ë¡œì»¬í‘¸ë“œ ì§ë§¤ì¥", "ë†í˜‘ ë†ì‚°ë¬¼ì‚°ì§€ìœ í†µì„¼í„°", "ë†ì‚°ë¬¼ ë„ë§¤ì‹œì¥"]
            for v_type in vendor_types:
                search_queries.append(f"{query} {v_type}")
        else:
            search_queries.append(f"{query} {missing_info_type}")

        # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ ì‹¤í–‰ ë° ê²°ê³¼ ì·¨í•©
        all_search_results = []
        seen_urls = set()
        for s_query in search_queries:
            results = tavily_tool.invoke({"query": s_query})
            for result in results:
                url = result.get('url')
                if url not in seen_urls:
                    all_search_results.append(result)
                    seen_urls.add(url)
        
        web_info = []
        if all_search_results:
            for result in all_search_results:
                summary = result.get('content', '')[:150]
                web_info.append(f"ì›¹ ê²€ìƒ‰: {result.get('title', '')} - {summary}... (ì¶œì²˜: {result.get('url')})")

        if web_info:
            if missing_info_type == "íŒë§¤ì²˜":
                supplemented_context['íŒë§¤ì²˜'] = web_info
            elif missing_info_type == "ì‹œì„¸":
                supplemented_context['ì‹¤ì‹œê°„ì‹œì„¸'] = web_info
            
            supplemented_context['used_web_search'] = True
            print(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {missing_info_type} ì •ë³´ ë³´ì™„")
        else:
            print(f"âš ï¸ ì›¹ ê²€ìƒ‰ìœ¼ë¡œë„ {missing_info_type} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return supplemented_context

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(query):
    okt = Okt()
    return okt.nouns(query)

def execute_milvus_search(query: str) -> list[str]:
    """Milvus ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    try:
        connections.connect("default", host=milvus_host, port=milvus_port)
        collection = Collection(collection_name)
        collection.load()
        results = search_market_docs(query, top_k=3)
        connections.disconnect("default")
        return results
    except Exception as e:
        print(f"âŒ Milvus ì—°ê²° ì˜¤ë¥˜: {e}")
        return ["íŒë§¤ì  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

# Groq LLM
class GroqLLM:
    def __init__(self, model="openai/gpt-oss-20b", api_key=None):
        self.model = model
        self.api_key = groq_api_key
        self.client = None

    def invoke(self, prompt: str, context: str = None, system_instruction: str = None):
        messages = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì»¨í…ìŠ¤íŠ¸ì™€ ì§€ì‹œì‚¬í•­ ì „ë‹¬
        if context or system_instruction:
            system_content = ""
            if context:
                system_content += f"[ì°¸ê³  ì •ë³´]\n{context}\n\n"
            if system_instruction:
                system_content += system_instruction
            
            messages.append({
                "role": "system", 
                "content": system_content
            })
        
        # ì‚¬ìš©ì ì§ˆë¬¸
        messages.append({
            "role": "user", 
            "content": prompt
        })
        
        try:
            self.client = Groq(api_key=self.api_key)
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.8,
                max_completion_tokens=1024,
                top_p=0.8,
                reasoning_effort="low",
                stream=True,
                stop=None
            )
            result = "".join(chunk.choices[0].delta.content or "" for chunk in completion)
            return result.strip()
        except Exception as e:
            print(f"API key ì‹¤íŒ¨: {e}")
            return f"LLM í˜¸ì¶œ ì‹¤íŒ¨"

# í”„ë¡¬í”„íŠ¸ ìƒì„±
def make_system_instruction(classification="ì‹œì„¸+íŒë§¤ì²˜"):
    """ì§ˆë¬¸ ë¶„ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    templates = {
        "ì‹œì„¸": {
            "order": "í’ˆëª©/ë“±ë½ìœ¨ â†’ ê°€ê²©ì •ë³´(ì—†ìœ¼ë©´ ìƒëµ) â†’ ì¶œì²˜",
            "exclude": "íŒë§¤ì²˜ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì‹œì„¸ ì§ˆë¬¸ì´ë¯€ë¡œ)",
            "example": "ê°ì(20kg)ì˜ ê°€ê²©ì€ ì–´ì œë³´ë‹¤ 2.8%(1,060ì›) ì¦ê°€í•œ 39,660ì›ì…ë‹ˆë‹¤.\n1ê°œì›”ì „ì—ëŠ” 33,260ì›, 1ë…„ì „ì—ëŠ” 31,576ì›ì´ì—ˆìŠµë‹ˆë‹¤.\n\nì‹œì„¸ ì •ë³´ ì¶œì²˜: https://www.kamis.or.kr/customer/main/main.do"
        },
        "íŒë§¤ì²˜": {
            "order": "íŒë§¤ì²˜ ì •ë³´ â†’ ì¶œì²˜",
            "exclude": "ì‹œì„¸ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (íŒë§¤ì²˜ ì§ˆë¬¸ì´ë¯€ë¡œ)",
            "example": "í•´ë‹¹ ì§€ì—­ì˜ íŒë§¤ì²˜ëŠ” ì¶©ë‚¨ íƒœì•ˆêµ° íƒœì•ˆ ë¡œì»¬í‘¸ë“œ íŒë§¤ì¥(ì¶©ë‚¨ íƒœì•ˆêµ° ë‚¨ë©´ ì•ˆë©´ëŒ€ë¡œ 1641 / ì£¼ìš” í’ˆëª©: ì±„ì†Œ, ê³¼ì¼, ì„œë¥˜) ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n\níŒë§¤ì²˜ ì •ë³´ ì¶œì²˜: https://www.data.go.kr/data/15025997/fileData.do"
        },
        "ì‹œì„¸+íŒë§¤ì²˜": {
            "order": "í’ˆëª©/ë“±ë½ìœ¨ â†’ ê°€ê²©ì •ë³´(ì—†ìœ¼ë©´ ìƒëµ) â†’ íŒë§¤ì²˜ â†’ ì¶œì²˜",
            "exclude": "",
            "example": "ê°ì(20kg)ì˜ ê°€ê²©ì€ ì–´ì œë³´ë‹¤ 2.8%(1,060ì›) ì¦ê°€í•œ 39,660ì›ì…ë‹ˆë‹¤.\n1ê°œì›”ì „ì—ëŠ” 33,260ì›, 1ë…„ì „ì—ëŠ” 31,576ì›ì´ì—ˆìŠµë‹ˆë‹¤.\n\ní•´ë‹¹ ì§€ì—­ì˜ íŒë§¤ì²˜ëŠ” ì¶©ë‚¨ íƒœì•ˆêµ° íƒœì•ˆ ë¡œì»¬í‘¸ë“œ íŒë§¤ì¥(ì¶©ë‚¨ íƒœì•ˆêµ° ë‚¨ë©´ ì•ˆë©´ëŒ€ë¡œ 1641 / ì£¼ìš” í’ˆëª©: ì±„ì†Œ, ê³¼ì¼, ì„œë¥˜) ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n\nì‹œì„¸ ì •ë³´ ì¶œì²˜: https://www.kamis.or.kr/customer/main/main.do\níŒë§¤ì²˜ ì •ë³´ ì¶œì²˜: https://www.data.go.kr/data/15025997/fileData.do"
        }
    }
    
    template = templates.get(classification, templates["ì‹œì„¸+íŒë§¤ì²˜"])
    
    return f"""
    [ì§€ì‹œ]
    - [ì°¸ê³  ì •ë³´]ì˜ ê°€ê²©ê³¼ ë‹¨ìœ„ë¥¼ ì •í™•íˆ ì‚¬ìš©
    - ì—†ëŠ” ì •ë³´ëŠ” ì—†ë‹¤ê³  ì•ˆë‚´
    - ìˆœì„œ: {template['order']}
    {f"- {template['exclude']}" if template['exclude'] else ""}

    [ì˜ˆì‹œ]
    {template['example']}
    """

# LLM í˜¸ì¶œ
def ask_llm_groq(prompt, context="", system_instruction=None, model="openai/gpt-oss-20b"):
    if system_instruction is None:
        system_instruction = make_system_instruction()
    
    llm = GroqLLM(model=model, api_key=groq_api_key)
    return llm.invoke(prompt, context, system_instruction)

# === Agent Node ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ===
# ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
class GraphState(dict):
    query: str = ""
    question_classification: str = ""
    context: Dict[str, Any] = {}
    pred_answer: str = ""
    is_recommend_ok: bool = False
    exit: bool = False
    retry_count: int = 0
    final_answer: str = ""
    needs_web_search: bool = False
    missing_info_types: List[str] = []
    used_web_search: bool = False
    validation_details: Optional[dict] = None

# LangGraph ë…¸ë“œ í•¨ìˆ˜
def node_input_graph(state: GraphState) -> GraphState:
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ state["query"]ê°€ ì´ë¯¸ ì „ë‹¬ëœ ê²½ìš°, ì¶”ê°€ ì…ë ¥ ì—†ì´ ë°”ë¡œ ì‚¬ìš©
    if state.get("query"):
        state["retry_count"] = 0  # ìƒˆë¡œìš´ ì…ë ¥ ì‹œ ì¬ë¶„ì„ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        return state
    query = input("ì‘ë¬¼ ë° ì§€ì—­ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if query.strip().lower() == "exit":
        state["exit"] = True
    else:
        state["query"] = query
        state["retry_count"] = 0
    return state

# ì§ˆë¬¸ ë¶„ë¥˜ í•¨ìˆ˜
def node_classify_question(state: GraphState) -> GraphState:
    """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    query = state["query"]
    print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘...")
    
    classification = classify_question_simple(query)
    state["question_classification"] = classification
    
    print(f"âœ… ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ: {classification}")
    return state

def node_collect_info_graph(state: GraphState) -> GraphState:
    """ì§ˆë¬¸ ë¶„ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    query = state["query"]
    classification = state.get("question_classification", "ì‹œì„¸+íŒë§¤ì²˜")
    
    print(f"ğŸ› ï¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    
    # ë¶„ë¥˜ì— ë”°ë¼ ì§ì ‘ ë„êµ¬ ì‹¤í–‰
    results = {
        "ì‹¤ì‹œê°„ì‹œì„¸": [],
        "íŒë§¤ì²˜": [],
        "ì›¹ê²€ìƒ‰": []
    }
    
    if classification == "ì‹œì„¸":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = fetch_api_data(query)[:1]
        results["íŒë§¤ì²˜"] = ["í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."]
    elif classification == "íŒë§¤ì²˜":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = ["í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤."]
        results["íŒë§¤ì²˜"] = execute_milvus_search(query)
    elif classification == "ì‹œì„¸+íŒë§¤ì²˜":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = fetch_api_data(query)[:1]
        results["íŒë§¤ì²˜"] = execute_milvus_search(query)

    state["context"] = results
    
    # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ê¸°ë¡
    tools_used = []
    if results.get("ì‹¤ì‹œê°„ì‹œì„¸"):
        tools_used.append("ì‹œì„¸ API")
    if results.get("íŒë§¤ì²˜"):
        tools_used.append("íŒë§¤ì²˜ ì •ë³´")
    if results.get("ì›¹ê²€ìƒ‰"):
        tools_used.append("ì›¹ ê²€ìƒ‰")
    
    print(f"âœ… ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ.")
    
    return state

def node_llm_summarize_graph(state: GraphState) -> GraphState:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸ’» ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    classification = state["question_classification"]
    
    # ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶œì²˜ì— ë”°ë¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ êµ¬ì„±
    context = state["context"]
    context_str = ""

    # 1. API/DB ì •ë³´ ë¶„ë¦¬ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì•„ë‹Œ ê²ƒ)
    db_prices = [p for p in context.get("ì‹¤ì‹œê°„ì‹œì„¸", []) if not str(p).startswith('ì›¹ ê²€ìƒ‰:')]
    db_vendors = [v for v in context.get("íŒë§¤ì²˜", []) if not str(v).startswith('ì›¹ ê²€ìƒ‰:')]

    if db_prices:
        context_str += "[ì‹¤ì‹œê°„ì‹œì„¸ ì •ë³´ (API)]\n" + "\n".join(map(str, db_prices)) + "\n\n"
    if db_vendors:
        context_str += "[íŒë§¤ì²˜ ì •ë³´ (DB)]\n" + "\n".join(map(str, db_vendors)) + "\n\n"

    # 2. ì›¹ ê²€ìƒ‰ ì •ë³´ ë¶„ë¦¬
    web_prices = [p for p in context.get("ì‹¤ì‹œê°„ì‹œì„¸", []) if str(p).startswith('ì›¹ ê²€ìƒ‰:')]
    web_vendors = [v for v in context.get("íŒë§¤ì²˜", []) if str(v).startswith('ì›¹ ê²€ìƒ‰:')]

    if web_prices or web_vendors:
        context_str += "[ì›¹ ê²€ìƒ‰ ì •ë³´]\n"
        if web_prices:
            context_str += "\n".join(map(str, web_prices)) + "\n"
        if web_vendors:
            context_str += "\n".join(map(str, web_vendors)) + "\n"
        context_str += "\n"

    # ì‹œìŠ¤í…œ ì§€ì¹¨ êµ¬ì„±
    system_instruction = make_system_instruction(classification)
    
    source_instruction = """
    [ì¶œì²˜ í‘œê¸° ì§€ì¹¨]
    - ë‹µë³€ì— ì •ë³´ë¥¼ í¬í•¨í•  ë•Œ, ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
    - `[ì‹¤ì‹œê°„ì‹œì„¸ ì •ë³´ (API)]`ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì˜ ì¶œì²˜ëŠ” 'https://www.kamis.or.kr/customer/main/main.do'ì„ ëª…ì‹œí•˜ì„¸ìš”.
    - `[íŒë§¤ì²˜ ì •ë³´ (DB)]`ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì˜ ì¶œì²˜ëŠ” 'https://www.data.go.kr/data/15025997/fileData.do'ì„ ëª…ì‹œí•˜ì„¸ìš”.
    - `[ì›¹ ê²€ìƒ‰ ì •ë³´]`ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì˜ ì¶œì²˜ëŠ” ê° í•­ëª© ëì— '(ì¶œì²˜: URL)' í˜•ì‹ìœ¼ë¡œ ì œê³µëœ URLì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    - ë§Œì•½ "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ëŠ” ë‚´ìš©ë§Œ ìˆë‹¤ë©´, í•´ë‹¹ ì •ë³´ì™€ ì¶œì²˜ëŠ” ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    - ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ì ˆëŒ€ ì„ê±°ë‚˜ ì˜ëª» í‘œê¸°í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
    """
    system_instruction += source_instruction

    if state.get("used_web_search"):
        web_search_instruction = f"""
        [ì¶”ê°€ ì§€ì‹œ]
        - ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì¸ '{state['query']}'ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
        """
        # íŒë§¤ì²˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ í–ˆì„ ê²½ìš°, LLMì—ê²Œ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ëª…í™•íˆ ì „ë‹¬
        if "íŒë§¤ì²˜" in state.get("missing_info_types", []):
            web_search_instruction += "- ì‚¬ìš©ìëŠ” ë†ì‘ë¬¼ì„ 'íŒë§¤'í•  ìˆ˜ ìˆëŠ” ì¥ì†Œ(ê³µíŒì¥, ë„ë§¤ì‹œì¥, ë¡œì»¬í‘¸ë“œ ë“±)ë¥¼ ì°¾ê³  ìˆìœ¼ë‹ˆ, ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì¥ì†Œë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n- íŒë§¤ì²˜ ì •ë³´ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í¬í•¨ëœ ì§€ì—­ê³¼ ë‹¤ë¥´ë‹¤ë©´ í•´ë‹¹ ì •ë³´ëŠ” ë¬´ì‹œ\n"
        
        web_search_instruction += "- ë§Œì•½ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ë„ ìœ ìš©í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´, ì •ë³´ê°€ ì—†ë‹¤ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        system_instruction += web_search_instruction

    # ê²€ì¦ í”¼ë“œë°± ì¶”ê°€
    if state.get("validation_details") and state.get("retry_count", 0) > 0:
        issues = state["validation_details"].get("issues", [])
        context_str += f"\n[ì´ì „ ê²€ì¦ ì‹¤íŒ¨ ì •ë³´]\n" + "\n".join([f"â€¢ {issue}" for issue in issues])
        context_str += "\n\nìœ„ì˜ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."

    # LLM í˜¸ì¶œ
    pred_answer = ask_llm_groq(
        prompt=state["query"],
        context=context_str,
        system_instruction=system_instruction
    )
    
    state.update({
        "pred_answer": pred_answer
    })
    return state

def validate_prices(original_context, pred_answer):
    """ê°€ê²© ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    # í•µì‹¬ ê²€ì¦ë§Œ ìˆ˜í–‰
    context_prices = []
    answer_prices = []

    # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ê°’ ì¶”ì¶œ (ì½¤ë§ˆê°€ í¬í•¨ëœ ìˆ«ì + 'ì›' íŒ¨í„´)
    for doc in original_context.get('ì‹¤ì‹œê°„ì‹œì„¸', []):
        # "í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤" ì²´í¬
        if "í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤" in doc:
            return False, ["ì‹œì„¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"]
            
        price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)ì›', doc)
        context_prices.extend(price_matches)
        
        # ì½¤ë§ˆê°€ ì—†ëŠ” ìˆ«ì + 'ì›' íŒ¨í„´ (4ìë¦¬ ì´ìƒë§Œ)
        simple_price_matches = re.findall(r'(\d{4,})ì›', doc)
        context_prices.extend(simple_price_matches)
    
    # ì¤‘ë³µ ì œê±°í•˜ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ ìœ ì§€

    # LLM ë‹µë³€ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ (ë™ì¼í•œ íŒ¨í„´ ì ìš©)
    answer_price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)ì›', pred_answer)
    answer_prices.extend(answer_price_matches)
    
    simple_answer_matches = re.findall(r'(\d{4,})ì›', pred_answer)
    answer_prices.extend(simple_answer_matches)

    # ì¤‘ë³µ ì œê±°í•˜ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ ìœ ì§€

    # ê°€ê²© ë§¤ì¹­ ê²€ì¦ (1:1 ë§¤ì¹­, ìˆœì„œ ê³ ë ¤, ì¤‘ë³µ ì œí•œ)
    
    # ì›ë³¸ ê°€ê²©ì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
    context_price_count = {}
    for price in context_prices:
        context_price_count[price] = context_price_count.get(price, 0) + 1
    
    # 1:1 ë§¤ì¹­ ê²€ì¦ (ìˆœì„œëŒ€ë¡œ, ì •í™•í•œ ë§¤ì¹­ë§Œ, ì¤‘ë³µ ì œí•œ)
    matched_prices = []
    missing_prices = []
    hallucination_prices = []
    used_answer_indices = set()  # ì´ë¯¸ ì‚¬ìš©ëœ ë‹µë³€ ì¸ë±ìŠ¤
    matched_price_count = {}  # ë§¤ì¹­ëœ ê°€ê²©ì˜ íšŸìˆ˜ ì¶”ì 
    
    # ì›ë³¸ ê°€ê²©ì„ ìˆœì„œëŒ€ë¡œ í™•ì¸
    for i, context_price in enumerate(context_prices):
        matched = False
        
        # ë‹µë³€ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°€ê²© ì°¾ê¸°
        for j, answer_price in enumerate(answer_prices):
            if j in used_answer_indices:
                continue
            
            # ì´ë¯¸ í•´ë‹¹ ê°€ê²©ì„ ìµœëŒ€ í—ˆìš© íšŸìˆ˜ë§Œí¼ ë§¤ì¹­í–ˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°
            if context_price in matched_price_count:
                current_count = matched_price_count[context_price]
                max_allowed = context_price_count[context_price]
                if current_count >= max_allowed:
                    continue
            
            # ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš©
            if context_price == answer_price:
                matched_prices.append(context_price)
                used_answer_indices.add(j)
                matched_price_count[context_price] = matched_price_count.get(context_price, 0) + 1
                matched = True
                break
        
        if not matched:
            missing_prices.append(context_price)
    
    # í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ê²©ì´ ìˆëŠ”ì§€ í™•ì¸ (LLM ë‹µë³€ì— ì›ë³¸ì— ì—†ëŠ” ê°€ê²©ì´ ìˆëŠ”ì§€)
    for j, answer_price in enumerate(answer_prices):
        if j not in used_answer_indices:
            # ì›ë³¸ì— ì—†ëŠ” ê°€ê²©ì¸ì§€ í™•ì¸ (ì •í™•í•œ ë§¤ì¹­ë§Œ)
            is_original = False
            for context_price in context_prices:
                if answer_price == context_price:
                    is_original = True
                    break
            
            if not is_original:
                hallucination_prices.append(f"ì›ë³¸ì— ì—†ëŠ” ê°€ê²©: {answer_price}")
    
    # ì¤‘ë³µ ë§¤ì¹­ ë¬¸ì œ í™•ì¸ (LLM ë‹µë³€ì—ì„œ ì›ë³¸ë³´ë‹¤ ë§ì´ ë‚˜ì˜¤ëŠ” ê°€ê²©)
    answer_price_count = {}
    for price in answer_prices:
        answer_price_count[price] = answer_price_count.get(price, 0) + 1
    
    for price, answer_count in answer_price_count.items():
        context_count = context_price_count.get(price, 0)
        if answer_count > context_count:
            hallucination_prices.append(f"ê°€ê²© ì¤‘ë³µ í• ë£¨ì‹œë„¤ì´ì…˜: {price} (ì›ë³¸ {context_count}íšŒ, ë‹µë³€ {answer_count}íšŒ)")
    
    # ê°€ê²© ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (100% ë§¤ì¹­ë˜ì–´ì•¼ë§Œ ì ìˆ˜ ë¶€ì—¬)
    price_match_score = len(matched_prices) / len(context_prices)
    is_perfect_match = price_match_score == 1.0
    
    # ê²€ì¦ ë¡œì§
    issues = []
    
    # 1. ê°€ê²© ì •ë³´ ë§¤ì¹­ - 100% ë§¤ì¹­ë˜ì–´ì•¼ë§Œ í†µê³¼
    if is_perfect_match and not hallucination_prices:  # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì—†ì–´ì•¼ í•¨
        price_valid = True
    else:
        price_valid = False
        
        # í• ë£¨ì‹œë„¤ì´ì…˜ì— ëŒ€í•œ í”¼ë“œë°±
        if hallucination_prices:
            issues.append(f"ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©(ì˜ˆ: '{hallucination_prices[0]}')ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì œê³µëœ ì°¸ê³  ì •ë³´ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")

        # ëˆ„ë½ëœ ê°€ê²©ì— ëŒ€í•œ í”¼ë“œë°±
        if missing_prices:
            context_docs = original_context.get('ì‹¤ì‹œê°„ì‹œì„¸', [])
            missing_info_feedback = []
            
            # ëˆ„ë½ëœ ê°€ê²©ì´ í¬í•¨ëœ ì›ë³¸ ë¬¸ì„œë¥¼ ì°¾ìŒ
            for doc in context_docs:
                doc_prices = re.findall(r'(\d{1,3}(?:,\d{3})*)ì›', doc) + re.findall(r'(\d{4,})ì›', doc)
                if any(price in doc_prices for price in missing_prices):
                    missing_info_feedback.append(doc)

            # êµ¬ì²´ì ì¸ í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±
            if missing_info_feedback:
                unique_feedback_docs = list(dict.fromkeys(missing_info_feedback))
                issues.append('ë‹¤ìŒ ì¤‘ìš” ì •ë³´ë¥¼ ë‹µë³€ì—ì„œ ëˆ„ë½í–ˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”: ' + " | ".join(unique_feedback_docs))
            # ì›ë³¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬
            elif missing_prices:
                issues.append(f'ê°€ê²© ì •ë³´ë¥¼ ì¼ë¶€({len(missing_prices)}ê°œ) ëˆ„ë½í–ˆìŠµë‹ˆë‹¤.')
        
        if missing_prices:
            issues.append(f'ëˆ„ë½ëœ ê°€ê²©: {missing_prices}')
        if hallucination_prices:
            issues.append(f'í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ê²©: {hallucination_prices}')
    
    # ìƒì„¸í•œ ê²€ì¦ ì •ë³´ ì¶œë ¥
    return price_valid, issues

def validate_vendors(original_context, pred_answer):
    """íŒë§¤ì  ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    # í•µì‹¬ ê²€ì¦ë§Œ ìˆ˜í–‰
    context_has_vendors = False
    answer_has_no_vendor = False

    # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'íŒë§¤ì²˜' in original_context:
        vendor_info = original_context['íŒë§¤ì²˜']
        if vendor_info and len(vendor_info) > 0:
            # ì‹¤ì œ íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸ (ë¹ˆ ë¬¸ìì—´ì´ë‚˜ "ì •ë³´ ì—†ìŒ"ì´ ì•„ë‹Œ ê²½ìš°)
            for vendor in vendor_info:
                if vendor and vendor != "í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." and len(vendor.strip()) > 0:
                    context_has_vendors = True
                    break

    # íŒë§¤ì  ì •ë³´ê°€ ì—†ìœ¼ë©´ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ (ì›¹ ê²€ìƒ‰ í•„ìš”)
    if not context_has_vendors:
        return False, ["íŒë§¤ì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"]

    # LLM ë‹µë³€ì— íŒë§¤ì  ì •ë³´ ë¶€ì¡± í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    no_vendor_keywords = [
        'íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì ì´ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤',
        'í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'
    ]
    answer_has_no_vendor = any(keyword in pred_answer for keyword in no_vendor_keywords)

    # í• ë£¨ì‹œë„¤ì´ì…˜ íŒë‹¨
    hallucination_detected = False
    hallucination_issues = []

    if context_has_vendors and answer_has_no_vendor:
        # ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ë° LLMì´ "ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
        hallucination_detected = True
        hallucination_issues.append("íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆìŒì—ë„ 'ì—†ìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
    
    elif not context_has_vendors and not answer_has_no_vendor:
        # ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ì—†ëŠ”ë° LLMì´ "ìˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
        hallucination_detected = True
        hallucination_issues.append("íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ì—†ìŒì—ë„ 'ìˆìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
    
    return not hallucination_detected, hallucination_issues

def node_judge_recommendation_graph(state: GraphState) -> GraphState:
    """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ë° ì¬ìƒì„± ì—¬ë¶€ ê²°ì •"""
    pred_answer = state["pred_answer"]
    original_context = state["context"]
    question_classification = state.get("question_classification", "ì‹œì„¸+íŒë§¤ì²˜")
    retry_count = state.get("retry_count", 0)
    
    print(f"ğŸ” ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì¤‘... (ì§ˆë¬¸ ë¶„ë¥˜: {question_classification}, ì¬ì‹œë„: {retry_count}íšŒ)")
    
    # ê²€ì¦ ì‹¤í–‰
    validations = {}
    
    # ê°€ê²© ê²€ì¦ (ì‹œì„¸ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ)
    if question_classification in ["ì‹œì„¸", "ì‹œì„¸+íŒë§¤ì²˜"]:
        validations['price'] = validate_prices(original_context, pred_answer)
    
    # íŒë§¤ì  ê²€ì¦ (íŒë§¤ì²˜ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ)
    if question_classification in ["íŒë§¤ì²˜", "ì‹œì„¸+íŒë§¤ì²˜"]:
        validations['vendor'] = validate_vendors(original_context, pred_answer)
    
    # ì „ì²´ ê²€ì¦ ê²°ê³¼ - íŠœí”Œì˜ ì²« ë²ˆì§¸ ê°’(is_valid)ë§Œ ì¶”ì¶œí•˜ì—¬ í‰ê°€
    all_valid = all(is_valid for is_valid, _ in validations.values()) if validations else True
    all_issues = [issue for _, issues in validations.values() for issue in issues]
    
    print(f"âœ… ê²€ì¦ ì™„ë£Œ: {'í†µê³¼' if all_valid else 'ì‹¤íŒ¨'}")
    
    # ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ - 1íšŒ ì¬ë¶„ì„ í›„ì—ë§Œ ê³ ë ¤
    needs_web_search = False
    missing_info_types = []
    
    # 1íšŒ ì¬ë¶„ì„ í›„ì—ë„ ê²€ì¦ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ì›¹ ê²€ìƒ‰ ê³ ë ¤
    if retry_count >= 1 and not all_valid:
        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì–´ë–¤ ì •ë³´ê°€ ë¶€ì¡±í•œì§€ ë¶„ì„
        for validation_name, (is_valid, issues) in validations.items():
            if not is_valid:
                if validation_name == 'price':
                    missing_info_types.append("ì‹œì„¸")
                elif validation_name == 'vendor':
                    missing_info_types.append("íŒë§¤ì²˜")
        
        # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ìƒíƒœ ì—…ë°ì´íŠ¸
        if missing_info_types and not state.get("used_web_search"):
            needs_web_search = True
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state.update({
        "is_recommend_ok": all_valid,
        "validation_details": {"validations": validations, "issues": all_issues},
        "needs_web_search": needs_web_search,
        "missing_info_types": missing_info_types
    })

    return state

def node_reanalyze_graph(state: GraphState) -> GraphState:
    state["retry_count"] += 1
    print(f"ğŸ”„ ì¬ë¶„ì„ ì¤‘... (ì‹œë„: {state['retry_count']}íšŒ)")
    # node_collect_info_graphì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
    return node_collect_info_graph(state)

def node_web_search_supplement(state: GraphState) -> GraphState:
    """ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤."""
    if not state.get("needs_web_search"):
        return state
    
    query = state["query"]
    original_context = state["context"]
    missing_info_types = state.get("missing_info_types", [])
    
    print(f"ğŸ” ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ë³´ê°• ì¤‘...")
    
    # ë¶€ì¡±í•œ ì •ë³´ íƒ€ì…ë³„ë¡œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    supplemented_context = original_context.copy()
    
    for info_type in missing_info_types:
        supplemented_context = supplement_missing_info_with_web_search(
            query, info_type, supplemented_context
        )
    
    # ë³´ì™„ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸
    state["context"] = supplemented_context
    state["used_web_search"] = True
    
    return state

def node_output_graph(state: GraphState) -> GraphState:
    # ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë‹µë³€ì„ ë³´ì™„í•œ ê²½ìš°, ê²€ì¦ ê²°ê³¼ì™€ ìƒê´€ì—†ì´ ë³´ì™„ëœ ë‹µë³€ì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
    if state.get("used_web_search"):
        state["final_answer"] = f"{state['pred_answer']}"
        return state

    if state["retry_count"] >= 1 and not state["is_recommend_ok"]:
        # ì›¹ ê²€ìƒ‰ ì—†ì´ 1íšŒ ì¬ë¶„ì„ í›„ ì¢…ë£Œ
        state["final_answer"] = "í•´ë‹¹ ì‘ë¬¼ê³¼ ì§€ì—­ì— ëŒ€í•œ ì‹œì„¸ ë˜ëŠ” íŒë§¤ì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í˜¹ì‹œ ë‹¤ë¥¸ ì‘ë¬¼ì´ë‚˜ ì§€ì—­ì„ ì°¾ì•„ë“œë¦´ê¹Œìš”?"
    else:
        state["final_answer"] = f"{state['pred_answer']}"
    return state

# LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
graph = StateGraph(GraphState)

graph.add_node("input", node_input_graph)
graph.add_node("classify_question", node_classify_question)
graph.add_node("collect_info", node_collect_info_graph)
graph.add_node("llm_summarize", node_llm_summarize_graph)
graph.add_node("judge_recommendation", node_judge_recommendation_graph)
graph.add_node("web_search_supplement", node_web_search_supplement)  # ì›¹ ê²€ìƒ‰ ë…¸ë“œ ì¶”ê°€
graph.add_node("reanalyze", node_reanalyze_graph)
graph.add_node("output", node_output_graph)

graph.add_edge("input", "classify_question")
graph.add_edge("classify_question", "collect_info")
graph.add_edge("collect_info", "llm_summarize")

# ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§
def summarize_branch(state: GraphState) -> str:
    """LLM ìš”ì•½ í›„ ë¶„ê¸°: ì›¹ ê²€ìƒ‰ì„ í–ˆë‹¤ë©´ ê²€ì¦ ì—†ì´ ë°”ë¡œ ì¶œë ¥, ì•„ë‹ˆë©´ ê²€ì¦ìœ¼ë¡œ ì´ë™"""
    if state.get("used_web_search"):
        return "output"
    else:
        return "judge_recommendation"

def judge_branch(state: GraphState) -> str:
    """ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°"""
    if state.get("exit"):
        return END
    
    if state.get("is_recommend_ok"):
        return "output"
    
    # ì¬ë¶„ì„ íšŸìˆ˜ê°€ 1íšŒ ë¯¸ë§Œì¸ ê²½ìš°
    if state["retry_count"] < 1:
        return "reanalyze"
    
    # ì¬ë¶„ì„ íšŸìˆ˜ê°€ 1íšŒ ì´ìƒì¸ ê²½ìš°
    # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•˜ê³  ì•„ì§ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì›¹ ê²€ìƒ‰ ì‹¤í–‰
    if state.get("needs_web_search") and not state.get("used_web_search"):
        return "web_search_supplement"
    
    # ì¬ì‹œë„ê°€ ëª¨ë‘ ì†Œì§„ë˜ì—ˆê±°ë‚˜ ì›¹ ê²€ìƒ‰ì„ ì´ë¯¸ ìˆ˜í–‰í•œ ê²½ìš° ì¢…ë£Œ
    return "output"

graph.add_conditional_edges(
    "llm_summarize",
    summarize_branch,
    {
        "output": "output",
        "judge_recommendation": "judge_recommendation",
    },
)

graph.add_conditional_edges(
    "judge_recommendation", 
    judge_branch,
    {
        "output": "output",
        "web_search_supplement": "web_search_supplement",  # ì›¹ ê²€ìƒ‰ ë¶„ê¸° ì¶”ê°€
        "reanalyze": "reanalyze"
    }
)
graph.add_edge("web_search_supplement", "llm_summarize")  # ì›¹ ê²€ìƒ‰ í›„ LLMìœ¼ë¡œ
graph.add_edge("reanalyze", "llm_summarize")
graph.add_edge("output", END)

graph.set_entry_point("input")

# ì‹¤í–‰ í•¨ìˆ˜
def run(state):
    """
    íŒë§¤ì²˜ ì—ì´ì „íŠ¸ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ ì „ë‹¬ë°›ì€ ìƒíƒœë¥¼ ë°”íƒ•ìœ¼ë¡œ LangGraphë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print("\n\n===== Sales Agent ì‹¤í–‰ ì‹œì‘ =====")
    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¶”ê°€
    try:
        check_collection()
    except Exception as e:
        print(f"âŒ Milvus ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
    
    app = graph.compile()
    
    # LangGraphê°€ TypedDictë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì—, ì¼ë°˜ Dictë¥¼ TypedDictë¡œ ë³€í™˜
    if not isinstance(state, GraphState):
        state = GraphState(**state)
        
    result_state = app.invoke(state)
    return result_state

if __name__ == "__main__":
    # íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰ìš© ì½”ë“œ
    print("=== íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ ===")
    
    # LangGraphë¥¼ ì»´íŒŒì¼í•˜ê³  ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰
    app = graph.compile()

    # íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰, ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "sales_agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    result_state = app.invoke({"query": "ë•ì •ì—ì„œ ì‚¬ê³¼ë¥¼ íŒ”ê³  ì‹¶ì–´"})
    
    print("\n" + "=" * 50)
    if result_state.get('final_answer'):
        print(f"\n[ìµœì¢… ë‹µë³€]")
        print(result_state['final_answer'])