# ì£¼ì˜ ë¬´ì‹œ
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# ì„ë² ë”© ëª¨ë¸
from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer("jhgan/ko-sroberta-multitask")

# Milvus ì—°ê²° ì„¤ì •
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, list_collections, utility
import requests
from dotenv import load_dotenv
import os
import pandas as pd
from konlpy.tag import Okt
import re
from groq import Groq
from sklearn.metrics.pairwise import cosine_similarity
from langgraph.graph import StateGraph, END
from typing import Dict, Any

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("KAMIS_API_KEY")
api_id = os.getenv("KAMIS_ID")
groq_api_key = os.getenv(f"OPENAI_KEY1")
milvus_host = os.getenv("MILVUS_HOST", "localhost")
milvus_port = os.getenv("MILVUS_PORT", "19530")
collection_name = "market_price_docs"


# CSV íŒŒì¼ ì„ë² ë”© ë° Milvusì— ì €ì¥
def embed_and_store_csv(csv_path="sales/info_20240812.csv"):
    df = pd.read_csv(csv_path, encoding="euc-kr")
    df['í’ˆëª©'] = df['í’ˆëª©'].fillna("ì •ë³´ ì—†ìŒ")
    docs = []
    for _, row in df.iterrows():
        doc = f"{row['íŒë§¤ì¥ ì´ë¦„']} ({row['ì£¼ì†Œ']} / ì£¼ìš” í’ˆëª©: {row['í’ˆëª©']})"
        docs.append(doc)
    if docs:
        embeddings = embedder.encode(docs)
        collection.insert([embeddings.tolist(), docs], fields=["embedding", "text"])

def check_collection():
    global collection
    connections.connect("default", host=milvus_host, port=milvus_port)

    if collection_name in utility.list_collections():
        collection = Collection(collection_name)
        collection.load()
        
        # ì‹¤ì œ ì¿¼ë¦¬ë¡œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            print(f"ğŸ” ì»¬ë ‰ì…˜ '{collection_name}' ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì¤‘...")
            
            # ì‹¤ì œ ì¿¼ë¦¬ë¡œ ë°ì´í„° í™•ì¸
            sample_results = collection.query(
                expr="id >= 0",
                output_fields=["id", "text"],
                limit=1
            )
            
            has_data = len(sample_results) > 0
            print(f"ï¿½ï¿½ ì¿¼ë¦¬ ê²°ê³¼: {len(sample_results)}ê°œ")
            print(f"ğŸ” ë°ì´í„° ì¡´ì¬ ì—¬ë¶€: {'ìˆìŒ' if has_data else 'ì—†ìŒ'}")
            
            if has_data:
                print(f"âœ… ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ë³´ì¡´í•©ë‹ˆë‹¤.")
                return collection
            else:
                print(f"âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                utility.drop_collection(collection_name)
                print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì™„ë£Œ")
                
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                fields = [
                    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
                    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512),
                ]
                schema = CollectionSchema(fields, "ì‹œì¥ ê°€ê²© ë¬¸ì„œ ì»¬ë ‰ì…˜")
                collection = Collection(collection_name, schema)
                print(f"ğŸ”„ ì»¬ë ‰ì…˜ '{collection_name}' ì¬ìƒì„± ì™„ë£Œ")
                
                # ë°ì´í„° ì‚½ì…
                embed_and_store_csv()
                print(f"âœ… ë°ì´í„° ì‚½ì… ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ë³´ì¡´í•©ë‹ˆë‹¤.")
            return collection
    else:
        # ì»¬ë ‰ì…˜ì´ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512),
        ]
        schema = CollectionSchema(fields, "ì‹œì¥ ê°€ê²© ë¬¸ì„œ ì»¬ë ‰ì…˜")
        collection = Collection(collection_name, schema)
        print(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        embed_and_store_csv()

    # ì»¬ë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„± (ì„ë² ë”© í•„ë“œì— ëŒ€í•´)
    if not collection.has_index():
        index_params = {
            "metric_type": "IP",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    return collection

# ì§ˆë¬¸ ë¶„ë¥˜ í•¨ìˆ˜
def classify_question_simple(query: str) -> str:
    """í•µì‹¬ ì˜ë„ í‚¤ì›Œë“œë§Œìœ¼ë¡œ ì§ˆë¬¸ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    query_lower = query.lower()
    
    # í•µì‹¬ ì˜ë„ í‚¤ì›Œë“œ (ê°€ì¥ ì¤‘ìš”í•œ ê²ƒë“¤ë§Œ)
    selling_intent = ['íŒ”ê³  ì‹¶ì–´', 'íŒ” ìˆ˜ ìˆ', 'ê±°ë˜', 'íŒë§¤', 'ë§¤ë§¤', 'íŒ”ë˜','íŒ”ê³ ì‹¶ì–´','íŒ”ìˆ˜ ìˆ','íŒ”ìˆ˜ìˆ','íŒ” ìˆ˜ìˆ', 'íŒ”ê¹Œ', 'íŒ”ë©´', 'íŒŒëŠ”ê²Œ', 'íŒŒëŠ” ê²ƒ', 'íŒŒëŠ”ê²ƒ']
    price_intent = ['ê°€ê²©', 'ì‹œì„¸', 'ì–¼ë§ˆ', 'ê°’', 'ì›']
    location_intent = ['íŒŒëŠ” ê³³', 'íŒë§¤ì ', 'ì§ë§¤ì¥', 'ì‹œì¥', 'ì–´ë””', 'íŒŒëŠ”ê³³']
    
    # "ë†ì‘ë¬¼"ì´ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
    if "ë†ì‘ë¬¼" in query_lower:
        if any(keyword in query_lower for keyword in selling_intent):
            return "íŒë§¤ì²˜" # "ë†ì‘ë¬¼ì„ íŒ”ê³  ì‹¶ì–´" â†’ íŒë§¤ì²˜
        elif any(keyword in query_lower for keyword in price_intent):
            return "ì •ë³´ ë¶€ì¡±"  # "ë†ì‘ë¬¼ ê°€ê²©" (ë†ì‘ë¬¼ì€ êµ¬ì²´ì ì´ì§€ ì•ŠìŒ)
        else:
            return "ì •ë³´ ë¶€ì¡±"  # "ë†ì‘ë¬¼"ë§Œ
    
    # ì¼ë°˜ì ì¸ ë¶„ë¥˜ ë¡œì§
    if any(keyword in query_lower for keyword in selling_intent):
        return "ì‹œì„¸+íŒë§¤ì²˜"  # êµ¬ì²´ì ì¸ ì‘ë¬¼ëª… + íŒ”ê³  ì‹¶ë‹¤ëŠ” ì˜ë„
    
    if any(keyword in query_lower for keyword in price_intent):
        if any(keyword in query_lower for keyword in location_intent):
            return "ì‹œì„¸+íŒë§¤ì²˜"  # ê°€ê²© + ìœ„ì¹˜ ëª¨ë‘ ìš”êµ¬
        else:
            return "ì‹œì„¸"  # ê°€ê²©ë§Œ ìš”êµ¬
    
    if any(keyword in query_lower for keyword in location_intent):
        return "íŒë§¤ì²˜"  # ìœ„ì¹˜ë§Œ ìš”êµ¬
    
    return "ê¸°íƒ€"

# api ìš”ì²­
def fetch_api_data(query=None):
    url = "http://www.kamis.or.kr/service/price/xml.do?action=dailySalesList"
    params = {
        "p_cert_key": api_key,
        "p_cert_id": api_id,
        "p_returntype": "json"
    }
    response = requests.get(url, params=params)
    docs = []
    if response.status_code == 200:
        data = response.json()
        items = []
        price = data.get("price", {})
        if isinstance(price, dict):
            items = price.get("item", [])
        elif isinstance(price, list):
            items = price
        if isinstance(items, dict):
            items = [items]

        def safe_val(val):
            if isinstance(val, list):
                return val[0] if val else ""
            return val if val is not None else ""

        # ì¿¼ë¦¬ ê¸°ë°˜ í•„í„°ë§
        keywords = extract_keywords(query)
        filtered_items = []
        for item in items:
            item_name_full = safe_val(item.get('item_name', ''))
            item_name_parts = item_name_full.split('/')
            item_names = [part.strip() for part in item_name_parts]
            match_count = sum([q == name for q in keywords for name in item_names])
            partial_count = sum([q in name for q in keywords for name in item_names])
            included_keywords = [q for q in keywords if any(q in name for name in item_names)]
            score = 0
            if keywords:
                if match_count > 0:
                    score = 3 + len(included_keywords)  # ì™„ì „ ì¼ì¹˜ + í‚¤ì›Œë“œ ê°œìˆ˜
                elif partial_count > 0:
                    score = 2 + len(included_keywords)  # ë¶€ë¶„ ì¼ì¹˜ + í‚¤ì›Œë“œ ê°œìˆ˜
                else:
                    score = len(included_keywords)      # í‚¤ì›Œë“œ ì¼ë¶€ë§Œ í¬í•¨
            else:
                score = 0
            filtered_items.append((score, item))

        filtered_items.sort(key=lambda x: x[0], reverse=True)
        filtered_items = [item for _, item in filtered_items]

        for item in filtered_items:
            category = safe_val(item.get('category_name', ''))
            if category not in ['ìˆ˜ì‚°ë¬¼', 'ì¶•ì‚°ë¬¼'] and safe_val(item.get('product_cls_name', '')) != 'ì†Œë§¤':
                direction_raw = safe_val(item.get('direction', ''))
                value_raw = safe_val(item.get('value', ''))
                dpr1 = safe_val(item.get('dpr1', ''))
                dpr2 = safe_val(item.get('dpr2', ''))
                day3 = safe_val(item.get('day3', ''))
                dpr3 = safe_val(item.get('dpr3', ''))
                day4 = safe_val(item.get('day4', ''))
                dpr4 = safe_val(item.get('dpr4', ''))

                try:
                    dpr1_val = int(str(dpr1).replace(',', '').replace(' ', '') or '0')
                    dpr2_val = int(str(dpr2).replace(',', '').replace(' ', '') or '0')
                    diff = abs(dpr1_val - dpr2_val)
                except (ValueError, TypeError):
                    diff = 0
                
                change_str = "ì™€ ë³€ë™ ì—†ëŠ”"
                if str(direction_raw) == "0":
                    change_str = f"ë³´ë‹¤ {value_raw}%({diff}ì›) ê°ì†Œí•œ"
                elif str(direction_raw) == "1":
                    change_str = f"ë³´ë‹¤ {value_raw}%({diff}ì›) ì¦ê°€í•œ"
                
                doc = (
                    f"{safe_val(item.get('item_name', ''))} ({safe_val(item.get('unit', ''))})ì˜ ê°€ê²©ì€ ì–´ì œ"
                    f"{change_str} {dpr1}ì› ì…ë‹ˆë‹¤."
                )
                if dpr3 and str(dpr3).strip() != "" and str(dpr3).strip() != "ì›":
                    doc += f"{day3}ì—ëŠ” {dpr3}ì›, "
                if dpr4 and str(dpr4).strip() != "" and str(dpr4).strip() != "ì›":
                    doc += f"{day4}ì—ëŠ” {dpr4}ì› ì´ì—ˆìŠµë‹ˆë‹¤."
                docs.append(doc)
    else:
        print("API í˜¸ì¶œ ì‹¤íŒ¨:", response.status_code)
        print(response.text)
    if docs and any(any(k in doc for k in extract_keywords(query)) for doc in docs):
        return docs
    else:
        return ["í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤."]

# Milvusì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
def search_market_docs(query, top_k=3):
    # ì „ì—­ ë³€ìˆ˜ collectionì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë¡œì»¬ì—ì„œ ì²˜ë¦¬
    try:
        from pymilvus import connections, Collection, utility
        
        # Milvus ì—°ê²°
        connections.connect("default", host=milvus_host, port=milvus_port)
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ
        if collection_name in utility.list_collections():
            local_collection = Collection(collection_name)
            local_collection.load()
            print(f"âœ… Milvus ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"âŒ Milvus ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            connections.disconnect("default")
            return ["íŒë§¤ì  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        
        # ì „ì²´ ì¿¼ë¦¬ë¡œ í•œ ë²ˆë§Œ ê²€ìƒ‰
        all_results = []

        query_nouns = extract_keywords(query)

        # ë¯¸ë¦¬ ì •ì˜ëœ ì§€ì—­ëª… ë¦¬ìŠ¤íŠ¸ì™€ ëª…ì‚¬ í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ ì§€ì—­ëª…ë§Œ ì¶”ì¶œ
        predefined_locations = ['í•¨í‰','ì„œì‚°','ëŒ€ì „', 'ì¶˜ì²œ','ê´‘ì£¼', 'ê²½ì‚°', 'ê°•ë™êµ¬', 'íƒœì•ˆ', 'ì„±ì£¼', 'ì°½ì›', 'ìš©ì¸', 'ìš¸ì£¼', 'ìˆœì²œ', 'ê²½ì£¼', 'ì–‘í‰', 'ìš¸ì‚°ê´‘ì—­', 'ì˜ì•”', 'ê¹€ì œ', 'ê³ ì°½', 'ì „ì£¼', 'í•˜ë™', 'ì œì²œ', 'í™ì„±', 'í™”ì„±', 'ì˜ì™•', 'ë‹´ì–‘', 'ì§„ì£¼', 'ì‚¬ì²œ', 'ë‚¨ì–‘ì£¼', 'ì—¬ìˆ˜', 'ìœ ì„±êµ¬', 'ì •ì', 'í™ì²œ', 'ë‚¨ì›', 'ë™êµ¬', 'ë‹¬ì„œêµ¬', 'ë‚¨í•´', 'ì˜ë™', 'ì„œêµ¬', 'ê³„ë£¡', 'ê³ ì„±', 'ê³ ì–‘', 'í‰íƒ', 'ë‚¨êµ¬', 'ìš¸ì§„', 'ë‚˜ì£¼', 'ì „ë¼ë¶ë„', 'ìµì‚°', 'ë¶€ì—¬', 'ì²­ë„', 'í•©ì²œ', 'í¬í•­', 'ë´‰í™”', 'ë¬¸ê²½', 'ê¹€í•´', 'í•¨ì–‘', 'ë¶êµ¬', 'ì² ì›', 'í™”ìˆœ', 'ìƒì£¼', 'ê²½ë¶ë„', 'ì•ˆì‚°', 'ì²­ì–‘', 'ì¶©ì£¼', 'ê¹€ì²œ', 'ì˜ê´‘', 'ì„±ë‚¨', 'ì „ë¼ë‚¨ë„', 'ë‹¬ì„±', 'ì¸ì œ', 'ì²œì•ˆ', 'ì œì£¼', 'ì›ì£¼', 'ê°€í‰', 'ì™„ì£¼', 'ì œì²œì‹œ', 'ì„±ì£¼êµ°', 'ê³ ì„±êµ°', 'ì§„ì²œ', 'ê±°ì°½', 'ì²­ì£¼', 'ê¹€í¬', 'í™”ì„±ì‹œ', 'ì™„ë„', 'í•¨ì•ˆ', 'ì˜¥ì²œ', 'ê¹€í•´ì‹œ', 'í•´ë‚¨', 'ë¬´ì•ˆ', 'ì˜ˆì‚°', 'ê¸ˆì‚°', 'ê°•ì„œêµ¬', 'ìƒë‹¹êµ¬', 'ì†¡íŒŒêµ¬', 'ê³µë„ì', 'ê³¡ì„±', 'ìš¸ë¦‰êµ°', 'ì„œê·€í¬', 'ì •ì„ ', 'í‰ì°½', 'ì–‘ì£¼', 'í¬ì²œ', 'ì§„ì•ˆ', 'ì„¸ì¢…']
        locations = [kw for kw in query_nouns if kw in predefined_locations or any(suffix in kw for suffix in ['ì‹œ', 'êµ°', 'êµ¬', 'ë„'])]

        # 1. ì§€ì—­ í‚¤ì›Œë“œ ì„ë² ë”© ê²€ìƒ‰
        if locations:
            region_query = " ".join(locations)
            region_vec = embedder.encode([region_query])[0]
            region_results = local_collection.search(
                data=[region_vec],
                anns_field="embedding",
                param={"metric_type": "IP", "params": {"nprobe": 20}},
                limit=200,
                output_fields=["text"],
            )
            if region_results and region_results[0]:
                all_results.extend([hit.entity.get("text") for hit in region_results[0]])

        # 2. ì „ì²´ ì¿¼ë¦¬ ì„ë² ë”© ê²€ìƒ‰
        query_vec = embedder.encode([query])[0]
        query_results = local_collection.search(
            data=[query_vec],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"nprobe": 20}},
            limit=200,
            output_fields=["text"],
        )
        if query_results and query_results[0]:
            all_results.extend([hit.entity.get("text") for hit in query_results[0]])

        # ì¤‘ë³µ ì œê±°
        all_results = list(dict.fromkeys(all_results))

        found_results = []
        for result_text in all_results:
            if any(loc in result_text for loc in locations):
                found_results.append(result_text)

        if not found_results:
            connections.disconnect("default")
            return ["í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."]
        else:
            def overlap_score(result_text):
                item_part = result_text.split('(')[0].strip() if 'ì£¼ìš” í’ˆëª©:' in result_text else result_text
                item_names = [x.strip() for x in re.split(r'[/,]', item_part)]
                query_strip = query.strip()
                query_nouns_set = set(extract_keywords(query_strip))
                current_score = 0
                if query_strip in item_names:
                    current_score += 10000
                for name in item_names:
                    name_nouns_set = set(extract_keywords(name))
                    if query_nouns_set.issubset(name_nouns_set):
                        current_score += 1000
                    current_score += len(query_nouns_set.intersection(name_nouns_set)) * 100
                for name in item_names:
                    if any(qn in name for qn in query_nouns_set):
                        if not any(qn in extract_keywords(name) for qn in query_nouns_set):
                            current_score += 1
                return current_score
    
            found_results.sort(key=overlap_score, reverse=True)
            connections.disconnect("default")
            return found_results[:top_k]
            
    except Exception as e:
        print(f"âŒ Milvus ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        try:
            connections.disconnect("default")
        except:
            pass
        return ["íŒë§¤ì  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

def web_search_tool(query: str) -> str:
    """Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        from langchain_community.tools.tavily_search import TavilySearchResults
        from dotenv import load_dotenv
        import os
        
        load_dotenv()
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        
        if not tavily_api_key:
            return "Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        tavily_tool = TavilySearchResults(max_results=3, api_key=tavily_api_key)
        search_results = tavily_tool.invoke({"query": query})
        
        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        summary_prompt = f"""
        ë‹¤ìŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
        
        ì§ˆë¬¸: {query}
        ê²€ìƒ‰ ê²°ê³¼: {search_results}
        
        ê·œì¹™:
        - ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©
        - í•œê¸€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
        - êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨
        """
        
        llm = GroqLLM(model="openai/gpt-oss-20b", api_key=groq_api_key)
        answer = llm.invoke(summary_prompt)
        return answer
        
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(query):
    okt = Okt()
    return okt.nouns(query)

def execute_milvus_search(query: str) -> list[str]:
    """Milvus ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    try:
        connections.connect("default", host=milvus_host, port=milvus_port)
        collection = Collection(collection_name)
        collection.load()
        results = search_market_docs(query, top_k=3)
        connections.disconnect("default")
        return results
    except Exception as e:
        print(f"âŒ Milvus ì—°ê²° ì˜¤ë¥˜: {e}")
        return ["íŒë§¤ì  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

# Groq LLM
class GroqLLM:
    def __init__(self, model="openai/gpt-oss-20b", api_key=None):
        self.model = model
        self.api_key = groq_api_key
        self.client = None

    def invoke(self, prompt: str, context: str = None, system_instruction: str = None):
        messages = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì»¨í…ìŠ¤íŠ¸ì™€ ì§€ì‹œì‚¬í•­ ì „ë‹¬
        if context or system_instruction:
            system_content = ""
            if context:
                system_content += f"[ì°¸ê³  ì •ë³´]\n{context}\n\n"
            if system_instruction:
                system_content += system_instruction
            
            messages.append({
                "role": "system", 
                "content": system_content
            })
        
        # ì‚¬ìš©ì ì§ˆë¬¸
        messages.append({
            "role": "user", 
            "content": prompt
        })
        
        try:
            self.client = Groq(api_key=self.api_key)
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.8,
                max_completion_tokens=512,
                top_p=0.8,
                reasoning_effort="low",
                stream=True,
                stop=None
            )
            result = "".join(chunk.choices[0].delta.content or "" for chunk in completion)
            return result.strip()
        except Exception as e:
            print(f"API key ì‹¤íŒ¨: {e}")
            return f"LLM í˜¸ì¶œ ì‹¤íŒ¨"

# í”„ë¡¬í”„íŠ¸ ìƒì„±
def make_system_instruction(classification="ì‹œì„¸+íŒë§¤ì²˜"):
    """ì§ˆë¬¸ ë¶„ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    templates = {
        "ì‹œì„¸": {
            "order": "í’ˆëª©/ë“±ë½ìœ¨ â†’ ê°€ê²©ì •ë³´(ì—†ìœ¼ë©´ ìƒëµ) â†’ ì¶œì²˜",
            "exclude": "íŒë§¤ì²˜ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì‹œì„¸ ì§ˆë¬¸ì´ë¯€ë¡œ)",
            "example": "ê°ì(20kg)ì˜ ê°€ê²©ì€ ì–´ì œë³´ë‹¤ 2.8%(1,060ì›) ì¦ê°€í•œ 39,660ì›ì…ë‹ˆë‹¤.\n1ê°œì›”ì „ì—ëŠ” 33,260ì›, 1ë…„ì „ì—ëŠ” 31,576ì›ì´ì—ˆìŠµë‹ˆë‹¤.\n\nì‹œì„¸ ì •ë³´ ì¶œì²˜: https://www.kamis.or.kr/customer/main/main.do"
        },
        "íŒë§¤ì²˜": {
            "order": "íŒë§¤ì²˜ ì •ë³´ â†’ ì¶œì²˜",
            "exclude": "ì‹œì„¸ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (íŒë§¤ì²˜ ì§ˆë¬¸ì´ë¯€ë¡œ)",
            "example": "í•´ë‹¹ ì§€ì—­ì˜ íŒë§¤ì²˜ëŠ” ì¶©ë‚¨ íƒœì•ˆêµ° íƒœì•ˆ ë¡œì»¬í‘¸ë“œ íŒë§¤ì¥(ì¶©ë‚¨ íƒœì•ˆêµ° ë‚¨ë©´ ì•ˆë©´ëŒ€ë¡œ 1641 / ì£¼ìš” í’ˆëª©: ì±„ì†Œ, ê³¼ì¼, ì„œë¥˜) ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n\níŒë§¤ì²˜ ì •ë³´ ì¶œì²˜: https://www.data.go.kr/data/15025997/fileData.do"
        },
        "ì‹œì„¸+íŒë§¤ì²˜": {
            "order": "í’ˆëª©/ë“±ë½ìœ¨ â†’ ê°€ê²©ì •ë³´(ì—†ìœ¼ë©´ ìƒëµ) â†’ íŒë§¤ì²˜ â†’ ì¶œì²˜",
            "exclude": "",
            "example": "ê°ì(20kg)ì˜ ê°€ê²©ì€ ì–´ì œë³´ë‹¤ 2.8%(1,060ì›) ì¦ê°€í•œ 39,660ì›ì…ë‹ˆë‹¤.\n1ê°œì›”ì „ì—ëŠ” 33,260ì›, 1ë…„ì „ì—ëŠ” 31,576ì›ì´ì—ˆìŠµë‹ˆë‹¤.\n\ní•´ë‹¹ ì§€ì—­ì˜ íŒë§¤ì²˜ëŠ” ì¶©ë‚¨ íƒœì•ˆêµ° íƒœì•ˆ ë¡œì»¬í‘¸ë“œ íŒë§¤ì¥(ì¶©ë‚¨ íƒœì•ˆêµ° ë‚¨ë©´ ì•ˆë©´ëŒ€ë¡œ 1641 / ì£¼ìš” í’ˆëª©: ì±„ì†Œ, ê³¼ì¼, ì„œë¥˜) ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n\nì‹œì„¸ ì •ë³´ ì¶œì²˜: https://www.kamis.or.kr/customer/main/main.do\níŒë§¤ì²˜ ì •ë³´ ì¶œì²˜: https://www.data.go.kr/data/15025997/fileData.do"
        }
    }
    
    template = templates.get(classification, templates["ì‹œì„¸+íŒë§¤ì²˜"])
    
    return f"""
    [ì§€ì‹œ]
    - [ì°¸ê³  ì •ë³´]ì˜ ê°€ê²©ê³¼ ë‹¨ìœ„ë¥¼ ì •í™•íˆ ì‚¬ìš©
    - ì—†ëŠ” ì •ë³´ëŠ” ì—†ë‹¤ê³  ì•ˆë‚´
    - ìˆœì„œ: {template['order']}
    {f"- {template['exclude']}" if template['exclude'] else ""}

    [ì˜ˆì‹œ]
    {template['example']}
    """

# LLM í˜¸ì¶œ
def ask_llm_groq(prompt, context="", system_instruction=None, model="openai/gpt-oss-20b"):
    if system_instruction is None:
        system_instruction = make_system_instruction()
    
    llm = GroqLLM(model=model, api_key=groq_api_key)
    return llm.invoke(prompt, context, system_instruction)

# === Agent Node ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ===
# ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
class GraphState(dict):
    query: str = ""
    question_classification: str = ""
    context: Dict[str, Any] = {}
    context_str_for_judge: str = ""
    pred_answer: str = ""
    is_recommend_ok: bool = False
    exit: bool = False
    retry_count: int = 0
    final_answer: str = ""
    skip_llm: bool = False

# LangGraph ë…¸ë“œ í•¨ìˆ˜
def node_input_graph(state: GraphState) -> GraphState:
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ state["query"]ê°€ ì´ë¯¸ ì „ë‹¬ëœ ê²½ìš°, ì¶”ê°€ ì…ë ¥ ì—†ì´ ë°”ë¡œ ì‚¬ìš©
    if state.get("query"):
        state["retry_count"] = 0  # ìƒˆë¡œìš´ ì…ë ¥ ì‹œ ì¬ë¶„ì„ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        return state
    query = input("ì‘ë¬¼ ë° ì§€ì—­ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if query.strip().lower() == "exit":
        state["exit"] = True
    else:
        state["query"] = query
        state["retry_count"] = 0
    return state

# ì§ˆë¬¸ ë¶„ë¥˜ í•¨ìˆ˜
def node_classify_question(state: GraphState) -> GraphState:
    """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    query = state["query"]
    print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘: {query}")
    
    classification = classify_question_simple(query)
    state["question_classification"] = classification
    
    print(f"âœ… ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ: {classification}")
    return state

def node_collect_info_graph(state: GraphState) -> GraphState:
    """ì§ˆë¬¸ ë¶„ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    query = state["query"]
    classification = state.get("question_classification", "ì‹œì„¸+íŒë§¤ì²˜")
    
    print(f"ğŸ› ï¸ ë„êµ¬ ì„ íƒ ë° ì •ë³´ ìˆ˜ì§‘: {classification}")
    
    if classification == "ì •ë³´ ë¶€ì¡±":
        print("âš ï¸ ë„ˆë¬´ ì¼ë°˜ì ì¸ ì§ˆë¬¸ - LLM í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ì•ˆë‚´ ë©”ì‹œì§€ ì œê³µ")
        state["context"] = {"ì •ë³´ ë¶€ì¡±": True}
        state["pred_answer"] = "ì›í•˜ëŠ” ì •ë³´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ:\n- 'ì§€ì—­ëª…'ì—ì„œ 'ì‘ë¬¼ëª…'ì„ íŒ”ê³  ì‹¶ì–´\n- 'ì‘ë¬¼ëª…' ê°€ê²©ì´ ì–¼ë§ˆì¸ê°€ìš”?"
        state["is_recommend_ok"] = True
        state["final_answer"] = state["pred_answer"]  # ìµœì¢… ë‹µë³€ë„ ë°”ë¡œ ì„¤ì •
        state["skip_llm"] = True  # LLM í˜¸ì¶œ ê±´ë„ˆë›°ê¸° í”Œë˜ê·¸
        return state

    # ë¶„ë¥˜ì— ë”°ë¼ ì§ì ‘ ë„êµ¬ ì‹¤í–‰
    results = {
        "ì‹¤ì‹œê°„ì‹œì„¸": [],
        "íŒë§¤ì²˜": [],
        "ì›¹ê²€ìƒ‰": []
    }
    
    if classification == "ì‹œì„¸":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = fetch_api_data(query)[:1]
        results["íŒë§¤ì²˜"] = ["í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."]
    elif classification == "íŒë§¤ì²˜":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = ["í•´ë‹¹ ì‘ë¬¼ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì—†ìŠµë‹ˆë‹¤."]
        results["íŒë§¤ì²˜"] = execute_milvus_search(query)
    elif classification == "ì‹œì„¸+íŒë§¤ì²˜":
        results["ì‹¤ì‹œê°„ì‹œì„¸"] = fetch_api_data(query)[:1]
        results["íŒë§¤ì²˜"] = execute_milvus_search(query)
    elif classification == "ê¸°íƒ€":
        results["ì›¹ê²€ìƒ‰"] = [web_search_tool(query)]
    
    state["context"] = results
    
    # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ê¸°ë¡
    tools_used = []
    if results.get("ì‹¤ì‹œê°„ì‹œì„¸"):
        tools_used.append("ì‹œì„¸ API")
    if results.get("íŒë§¤ì²˜"):
        tools_used.append("íŒë§¤ì²˜ ì •ë³´")
    if results.get("ì›¹ê²€ìƒ‰"):
        tools_used.append("ì›¹ ê²€ìƒ‰")
    
    print(f" ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tools_used)}")
    
    return state

def node_llm_summarize_graph(state: GraphState) -> GraphState:
    context = state["context"]
    classification = state["question_classification"]

    # LLM í˜¸ì¶œì„ ê±´ë„ˆë›°ì–´ì•¼ í•˜ëŠ” ê²½ìš°
    if state.get("skip_llm", False):
        print("â­ï¸ LLM í˜¸ì¶œ ê±´ë„ˆë›°ê¸° - ì´ë¯¸ ë‹µë³€ ìƒì„±ë¨")
        return state
    
    # ë¶„ë¥˜ë³„ ì»¨í…ìŠ¤íŠ¸ ë§¤í•‘
    context_mapping = {
        "ì‹œì„¸": ["ì‹¤ì‹œê°„ì‹œì„¸"],
        "íŒë§¤ì²˜": ["íŒë§¤ì²˜"],
        "ì‹œì„¸+íŒë§¤ì²˜": ["ì‹¤ì‹œê°„ì‹œì„¸", "íŒë§¤ì²˜"],
        "ê¸°íƒ€": ["ì›¹ê²€ìƒ‰"]
    }
    
    # í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì„ íƒ
    selected_keys = context_mapping.get(classification, [])
    context_parts = [
        f"{key} ì •ë³´: {context[key]}" 
        for key in selected_keys 
        if context.get(key)
    ]
    
    context_str = "\n".join(context_parts)
    
    # ê²€ì¦ í”¼ë“œë°± ì¶”ê°€
    if state.get("validation_details") and state.get("retry_count", 0) > 0:
        issues = state["validation_details"].get("issues", [])
        context_str += f"\n[ì´ì „ ê²€ì¦ ì‹¤íŒ¨ ì •ë³´]\n" + "\n".join([f"â€¢ {issue}" for issue in issues])
        context_str += "\n\nìœ„ì˜ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."

    # LLM í˜¸ì¶œ
    pred_answer = ask_llm_groq(
        prompt=state["query"],
        context=context_str,
        system_instruction=make_system_instruction(classification)
    )
    
    state.update({
        "pred_answer": pred_answer,
        "context_str_for_judge": context_str
    })
    return state

def validate_prices(original_context, pred_answer):
    """ê°€ê²© ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    # í•µì‹¬ ê²€ì¦ë§Œ ìˆ˜í–‰
    context_prices = []
    answer_prices = []

    # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ê°’ ì¶”ì¶œ (ì½¤ë§ˆê°€ í¬í•¨ëœ ìˆ«ì + 'ì›' íŒ¨í„´)
    for doc in original_context.get('ì‹¤ì‹œê°„ì‹œì„¸', []):
        price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)ì›', doc)
        context_prices.extend(price_matches)
        
        # ì½¤ë§ˆê°€ ì—†ëŠ” ìˆ«ì + 'ì›' íŒ¨í„´ (4ìë¦¬ ì´ìƒë§Œ)
        simple_price_matches = re.findall(r'(\d{4,})ì›', doc)
        context_prices.extend(simple_price_matches)
    
    # ì¤‘ë³µ ì œê±°í•˜ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ ìœ ì§€
    print(f"ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ê°€ê²© (ìˆœì„œ ìœ ì§€): {context_prices}")
    
    # LLM ë‹µë³€ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ (ë™ì¼í•œ íŒ¨í„´ ì ìš©)
    answer_price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)ì›', pred_answer)
    answer_prices.extend(answer_price_matches)
    
    simple_answer_matches = re.findall(r'(\d{4,})ì›', pred_answer)
    answer_prices.extend(simple_answer_matches)

    # ì¤‘ë³µ ì œê±°í•˜ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ ìœ ì§€
    print(f"LLM ë‹µë³€ì—ì„œ ì¶”ì¶œëœ ê°€ê²© (ìˆœì„œ ìœ ì§€): {answer_prices}")

    # ê°€ê²© ë§¤ì¹­ ê²€ì¦ (1:1 ë§¤ì¹­, ìˆœì„œ ê³ ë ¤, ì¤‘ë³µ ì œí•œ)
    if not context_prices:
        print("ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì— ê°€ê²© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return True, []  # ê°€ê²© ì •ë³´ê°€ ì—†ìœ¼ë©´ ê²€ì¦ í†µê³¼
    
    # ì›ë³¸ ê°€ê²©ì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
    context_price_count = {}
    for price in context_prices:
        context_price_count[price] = context_price_count.get(price, 0) + 1
    
    print(f"ì›ë³¸ ê°€ê²© ì¶œí˜„ íšŸìˆ˜: {context_price_count}")
    
    # 1:1 ë§¤ì¹­ ê²€ì¦ (ìˆœì„œëŒ€ë¡œ, ì •í™•í•œ ë§¤ì¹­ë§Œ, ì¤‘ë³µ ì œí•œ)
    matched_prices = []
    missing_prices = []
    hallucination_prices = []
    used_answer_indices = set()  # ì´ë¯¸ ì‚¬ìš©ëœ ë‹µë³€ ì¸ë±ìŠ¤
    matched_price_count = {}  # ë§¤ì¹­ëœ ê°€ê²©ì˜ íšŸìˆ˜ ì¶”ì 
    
    # ì›ë³¸ ê°€ê²©ì„ ìˆœì„œëŒ€ë¡œ í™•ì¸
    for i, context_price in enumerate(context_prices):
        matched = False
        
        # ë‹µë³€ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°€ê²© ì°¾ê¸°
        for j, answer_price in enumerate(answer_prices):
            if j in used_answer_indices:
                continue
            
            # ì´ë¯¸ í•´ë‹¹ ê°€ê²©ì„ ìµœëŒ€ í—ˆìš© íšŸìˆ˜ë§Œí¼ ë§¤ì¹­í–ˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°
            if context_price in matched_price_count:
                current_count = matched_price_count[context_price]
                max_allowed = context_price_count[context_price]
                print(f"  ì¤‘ë³µ ì²´í¬: {context_price} (í˜„ì¬ {current_count}/{max_allowed})")
                if current_count >= max_allowed:
                    print(f"  ì¤‘ë³µ ì œí•œìœ¼ë¡œ ê±´ë„ˆë›°ê¸°: {context_price}")
                    continue
            
            # ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš©
            if context_price == answer_price:
                matched_prices.append(context_price)
                used_answer_indices.add(j)
                matched_price_count[context_price] = matched_price_count.get(context_price, 0) + 1
                matched = True
                print(f"ì •í™•í•œ ë§¤ì¹­: {context_price} â† {answer_price} (ë§¤ì¹­ íšŸìˆ˜: {matched_price_count[context_price]})")
                break
        
        if not matched:
            missing_prices.append(context_price)
    
    # í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ê²©ì´ ìˆëŠ”ì§€ í™•ì¸ (LLM ë‹µë³€ì— ì›ë³¸ì— ì—†ëŠ” ê°€ê²©ì´ ìˆëŠ”ì§€)
    for j, answer_price in enumerate(answer_prices):
        if j not in used_answer_indices:
            # ì›ë³¸ì— ì—†ëŠ” ê°€ê²©ì¸ì§€ í™•ì¸ (ì •í™•í•œ ë§¤ì¹­ë§Œ)
            is_original = False
            for context_price in context_prices:
                if answer_price == context_price:
                    is_original = True
                    break
            
            if not is_original:
                hallucination_prices.append(f"ì›ë³¸ì— ì—†ëŠ” ê°€ê²©: {answer_price}")
    
    # ì¤‘ë³µ ë§¤ì¹­ ë¬¸ì œ í™•ì¸ (LLM ë‹µë³€ì—ì„œ ì›ë³¸ë³´ë‹¤ ë§ì´ ë‚˜ì˜¤ëŠ” ê°€ê²©)
    answer_price_count = {}
    for price in answer_prices:
        answer_price_count[price] = answer_price_count.get(price, 0) + 1
    
    print(f"LLM ë‹µë³€ ê°€ê²© ì¶œí˜„ íšŸìˆ˜: {answer_price_count}")
    
    for price, answer_count in answer_price_count.items():
        context_count = context_price_count.get(price, 0)
        if answer_count > context_count:
            hallucination_prices.append(f"ê°€ê²© ì¤‘ë³µ í• ë£¨ì‹œë„¤ì´ì…˜: {price} (ì›ë³¸ {context_count}íšŒ, ë‹µë³€ {answer_count}íšŒ)")
    
    # ê°€ê²© ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (100% ë§¤ì¹­ë˜ì–´ì•¼ë§Œ ì ìˆ˜ ë¶€ì—¬)
    price_match_score = len(matched_prices) / len(context_prices)
    is_perfect_match = price_match_score == 1.0
    
    print(f"ê°€ê²© ë§¤ì¹­ ì ìˆ˜: {len(matched_prices)}/{len(context_prices)} = {price_match_score:.2f}")
    print(f"ì™„ë²½í•œ ë§¤ì¹­: {'âœ…' if is_perfect_match else 'âŒ'}")
    print(f"ë§¤ì¹­ëœ ê°€ê²© íšŸìˆ˜: {matched_price_count}")
    
    # ê²€ì¦ ë¡œì§
    issues = []
    
    # 1. ê°€ê²© ì •ë³´ ë§¤ì¹­ - 100% ë§¤ì¹­ë˜ì–´ì•¼ë§Œ í†µê³¼
    if is_perfect_match and not hallucination_prices:  # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì—†ì–´ì•¼ í•¨
        print("âœ… ê°€ê²© ì •ë³´ ì™„ë²½ ë§¤ì¹­ - í†µê³¼")
        price_valid = True
    else:
        print("âŒ ê°€ê²© ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨ - ë¶ˆí†µê³¼")
        price_valid = False
        
        if hallucination_prices:
            issues.append(f'í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ë¡œ ì¸í•œ ê°€ê²© ë§¤ì¹­ ì‹¤íŒ¨')
        else:
            issues.append(f'ê°€ê²© ì •ë³´ ë¶ˆì™„ì „ ë§¤ì¹­: {len(matched_prices)}/{len(context_prices)}')
        
        if missing_prices:
            issues.append(f'ëˆ„ë½ëœ ê°€ê²©: {missing_prices}')
        if hallucination_prices:
            issues.append(f'í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ê²©: {hallucination_prices}')
    
    # ìƒì„¸í•œ ê²€ì¦ ì •ë³´ ì¶œë ¥
    print(f"\n=== ìƒì„¸ ê²€ì¦ ê²°ê³¼ ===")
    if matched_prices:
        print(f"âœ… ë§¤ì¹­ëœ ê°€ê²©: {matched_prices}")
    if missing_prices:
        print(f"âŒ ëˆ„ë½ëœ ê°€ê²©: {missing_prices}")
    if hallucination_prices:
        print(f" í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ê²©: {hallucination_prices}")
    
    return price_valid, issues

def validate_vendors(original_context, pred_answer):
    """íŒë§¤ì  ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    # í•µì‹¬ ê²€ì¦ë§Œ ìˆ˜í–‰
    context_has_vendors = False
    answer_has_no_vendor = False

    # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'íŒë§¤ì²˜' in original_context:
        vendor_info = original_context['íŒë§¤ì²˜']
        if vendor_info and len(vendor_info) > 0:
            # ì‹¤ì œ íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸ (ë¹ˆ ë¬¸ìì—´ì´ë‚˜ "ì •ë³´ ì—†ìŒ"ì´ ì•„ë‹Œ ê²½ìš°)
            for vendor in vendor_info:
                if vendor and vendor != "í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." and len(vendor.strip()) > 0:
                    context_has_vendors = True
                    break

    # LLM ë‹µë³€ì— íŒë§¤ì  ì •ë³´ ë¶€ì¡± í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    no_vendor_keywords = [
        'íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì ì´ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤',
        'í•´ë‹¹ ì§€ì—­ì— ìœ„ì¹˜í•œ íŒë§¤ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
        'íŒë§¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'
    ]
    answer_has_no_vendor = any(keyword in pred_answer for keyword in no_vendor_keywords)

    # í• ë£¨ì‹œë„¤ì´ì…˜ íŒë‹¨
    hallucination_detected = False
    hallucination_issues = []

    if context_has_vendors and answer_has_no_vendor:
        # ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆëŠ”ë° LLMì´ "ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
        hallucination_detected = True
        hallucination_issues.append("íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆìŒì—ë„ 'ì—†ìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
        print("âŒ íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ìˆìŒì—ë„ 'ì—†ìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
    
    elif not context_has_vendors and not answer_has_no_vendor:
        # ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ì—†ëŠ”ë° LLMì´ "ìˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
        hallucination_detected = True
        hallucination_issues.append("íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ì—†ìŒì—ë„ 'ìˆìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
        print("âŒ íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€: ì›ë³¸ì— íŒë§¤ì  ì •ë³´ê°€ ì—†ìŒì—ë„ 'ìˆìŠµë‹ˆë‹¤'ë¼ê³  í‘œì‹œ")
    
    else:
        print("âœ… íŒë§¤ì  ì •ë³´ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ í†µê³¼")
    
    return not hallucination_detected, hallucination_issues

def node_judge_recommendation_graph(state: GraphState) -> GraphState:
    """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ë° ì¬ìƒì„± ì—¬ë¶€ ê²°ì •"""
    pred_answer = state["pred_answer"]
    original_context = state["context"]
    question_classification = state.get("question_classification", "ì‹œì„¸+íŒë§¤ì²˜")
    
    print(f"ğŸ” ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì¤‘... (ì§ˆë¬¸ ë¶„ë¥˜: {question_classification})")
    
    # ê²€ì¦ ì‹¤í–‰
    validations = {}
    
    # ê°€ê²© ê²€ì¦ (ì‹œì„¸ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ)
    if question_classification in ["ì‹œì„¸", "ì‹œì„¸+íŒë§¤ì²˜"]:
        validations['price'] = validate_prices(original_context, pred_answer)
    
    # íŒë§¤ì  ê²€ì¦ (íŒë§¤ì²˜ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ)
    if question_classification in ["íŒë§¤ì²˜", "ì‹œì„¸+íŒë§¤ì²˜"]:
        validations['vendor'] = validate_vendors(original_context, pred_answer)
    
    # ì „ì²´ ê²€ì¦ ê²°ê³¼
    all_valid = all(validations.values()) if validations else True
    all_issues = [issue for validation in validations.values() for issue in validation[1]]
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state.update({
        "is_recommend_ok": all_valid,
        "validation_details": {"validations": validations, "issues": all_issues},
        "needs_web_search": not all_valid
    })

    return state

def node_reanalyze_graph(state: GraphState) -> GraphState:
    state["retry_count"] += 1
    query = state["query"]

    # ì´ì „ ê²€ì¦ ì‹¤íŒ¨ ì •ë³´ ì¶œë ¥
    if state.get("validation_details"):
        print(f"ì¬ë¶„ì„ {state['retry_count']}íšŒì°¨: ì´ì „ ê²€ì¦ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì :")
        for issue in state.get("validation_details", {}).get("issues", []):
            print(f"  - {issue}")
        print("ìœ„ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ì—¬ ì¬ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # node_collect_info_graphì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
    return node_collect_info_graph(state)

def node_output_graph(state: GraphState) -> GraphState:
    if state["retry_count"] >= 2 and not state["is_recommend_ok"]:
        state["final_answer"] = "í•´ë‹¹ ì‘ë¬¼ê³¼ ì§€ì—­ì— ëŒ€í•œ ì‹œì„¸ ë˜ëŠ” íŒë§¤ì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í˜¹ì‹œ ë‹¤ë¥¸ ì‘ë¬¼ì´ë‚˜ ì§€ì—­ì„ ì°¾ì•„ë“œë¦´ê¹Œìš”?"
    else:
        state["final_answer"] = f"{state['pred_answer']}"
    return state

# LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
graph = StateGraph(GraphState)

graph.add_node("input", node_input_graph)
graph.add_node("classify_question", node_classify_question)
graph.add_node("collect_info", node_collect_info_graph)
graph.add_node("llm_summarize", node_llm_summarize_graph)
graph.add_node("judge_recommendation", node_judge_recommendation_graph)
graph.add_node("reanalyze", node_reanalyze_graph)
graph.add_node("output", node_output_graph)

graph.add_edge("input", "classify_question")
graph.add_edge("classify_question", "collect_info")
graph.add_edge("collect_info", "llm_summarize")
graph.add_edge("llm_summarize", "judge_recommendation")

# ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§
def judge_branch(state: GraphState) -> str:
    if state.get("exit"):
        return END
    
    if state.get("is_recommend_ok"):
        return "output"
    
    # 2íšŒ ì¬ë¶„ì„ í›„ì—ë„ ì ì ˆí•˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
    if state["retry_count"] >= 2:
        return "output"
    else:
        return "reanalyze"

graph.add_conditional_edges(
    "judge_recommendation", 
    judge_branch,
    {
        "output": "output",
        "reanalyze": "reanalyze"
    }
)
graph.add_edge("reanalyze", "llm_summarize")
graph.add_edge("output", END)

graph.set_entry_point("input")

# ì‹¤í–‰ í•¨ìˆ˜
def run(state):
    """
    íŒë§¤ì²˜ ì—ì´ì „íŠ¸ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ ì „ë‹¬ë°›ì€ ìƒíƒœë¥¼ ë°”íƒ•ìœ¼ë¡œ LangGraphë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¶”ê°€
    try:
        check_collection()
        print("âœ… Milvus ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ Milvus ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
    
    app = graph.compile()
    
    # LangGraphê°€ TypedDictë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì—, ì¼ë°˜ Dictë¥¼ TypedDictë¡œ ë³€í™˜
    if not isinstance(state, GraphState):
        state = GraphState(**state)
        
    result_state = app.invoke(state)
    return result_state

if __name__ == "__main__":
    # íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰ìš© ì½”ë“œ
    print("=== íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ ===")
    
    # LangGraphë¥¼ ì»´íŒŒì¼í•˜ê³  ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰
    app = graph.compile()

    # íŒë§¤ì²˜ ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰# ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "sales_agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    result_state = app.invoke({"query": ""})
    
    print("\n" + "=" * 50)
    if result_state.get('final_answer'):
        print(f"\n[ìµœì¢… ë‹µë³€]")
        print(result_state['final_answer'])