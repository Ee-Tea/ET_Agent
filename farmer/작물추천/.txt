cd farmer\ì‘ë¬¼ì¶”ì²œ -> CMD ê²½ë¡œ ë“¤ì–´ê°€ì„œ Crop65pdfllm.py íŒŒì¼ ì‹¤í–‰

cd farmer\ì‘ë¬¼ì¶”ì²œ\Goldenset_test -> Goldenset_test.py íŒŒì¼ ì‹¤í–‰ <- ê³¨ë“ ì…‹ í‰ê°€ ì‹¤í–‰ log ê¸°ë¡

cd farmer\ì‘ë¬¼ì¶”ì²œ\csv_change_need> -> pdf ë³€ê²½


vectordb ìœ„ì¹˜ 
C:/Rookies_project/ET_Agent/Crop_Recommendations_DB/faiss_pdf_db


-------------------------

import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
import math
import re  # ì§€ì—­ ì¶”ì¶œ/í’ˆì§ˆ ê²€ì‚¬

from langchain_core.runnables.graph import MermaidDrawMethod
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(find_dotenv())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ê²½ë¡œ ì„¤ì • (__file__ ì—†ëŠ” í™˜ê²½ ëŒ€ë¹„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    BASE_DIR = Path(__file__).resolve().parent
except NameError:
    BASE_DIR = Path.cwd()

VECTOR_DB_PATH = Path("faiss_pdf_db")
CHAT_HISTORY_PATH = Path("chat_history.json")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API í‚¤ ë° ê²Œì´íŠ¸/í’ˆì§ˆ íŒŒë¼ë¯¸í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))
os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
os.environ["COHERE_API_KEY"] = COHERE_API_KEY

# ìœ ì‚¬ë„ ê²Œì´íŠ¸ & í’ˆì§ˆ ê·œì¹™ (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì ˆ ê°€ëŠ¥)
ANSWER_CONTEXT_SIM_THRESHOLD = float(os.getenv("ANSWER_CONTEXT_SIM_THRESHOLD", "0.8"))
QUALITY_MIN_CHARS = int(os.getenv("QUALITY_MIN_CHARS", "80"))
QUALITY_REQUIRE_KOREAN = bool(int(os.getenv("QUALITY_REQUIRE_KOREAN", "1")))
QUALITY_MIN_OVERLAP = float(os.getenv("QUALITY_MIN_OVERLAP", "0.03"))
QUALITY_MIN_LINES_IF_CONTEXT_LONG = int(os.getenv("QUALITY_MIN_LINES_IF_CONTEXT_LONG", "2"))
CONTEXT_LONG_CHARS = int(os.getenv("CONTEXT_LONG_CHARS", "400"))

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
if not os.getenv("TAVILY_API_KEY"):
    raise ValueError("TAVILY_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

print(f"ğŸ”§ GROQ_MODEL={GROQ_MODEL}, TEMPERATURE={TEMPERATURE}")
try:
    print(f"ğŸ“¦ VECTOR_DB_PATH={VECTOR_DB_PATH.resolve()}")
except Exception:
    print(f"ğŸ“¦ VECTOR_DB_PATH={VECTOR_DB_PATH}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ê³¼ 'ëŒ€í™” ê¸°ë¡'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- **ë§Œì•½ ì£¼ì–´ì§„ ë¬¸ë§¥ì— ë‹µë³€í•  ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´, ë°˜ë“œì‹œ 'ì •ë³´ ë¶€ì¡±'ì´ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ë¶€ê°€ì ì¸ ì„¤ëª…ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.**

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_SEARCH_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ê´€ë ¨ ì§€ì‹ê³¼ ì¼ë°˜ ìƒì‹ì— í•´ë°•í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ëŒ€í™” ê¸°ë¡'ê³¼ 'ê²€ìƒ‰ ê²°ê³¼'ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì§ˆë¬¸'ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„ì¹˜ ì•Šê±°ë‚˜ ì—†ëŠ” ê²½ìš° 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´ ë‹µë³€ë§Œ ì œê³µ.
- ë§¤ ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— í•­ìƒ ì‚¬ìš©ìì˜ ë‹¤ìŒ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ê¸ì •ì ì´ê³  ê°œë°©ì ì¸ ì§ˆë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

[ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
web_search_prompt = PromptTemplate.from_template(WEB_SEARCH_PROMPT_TMPL)

QUERY_EXPANSION_PROMPT_TMPL = """
ì•„ë˜ 'ëŒ€í™” ê¸°ë¡'ê³¼ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ,
ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ ê²€ìƒ‰ ì§ˆì˜(query)ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

ì§ˆë¬¸: {question}

ìƒì„±í•  ê²€ìƒ‰ ì§ˆì˜ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ, 30ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
query_expansion_prompt = ChatPromptTemplate.from_template(QUERY_EXPANSION_PROMPT_TMPL)

QUESTION_TYPE_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
'ëŒ€í™” ê¸°ë¡'ê³¼ 'ì§ˆë¬¸'ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¥ ì í•©í•œ ì§ˆë¬¸ ìœ í˜•ì„ ì•„ë˜ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
'context', 'crop', 'general' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ê°€ì ì¸ ì„¤ëª…ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.

- **context**: í˜„ì¬ ì§ˆë¬¸ì´ **ì§ì „ ëŒ€í™”ì˜ ì£¼ì œ**ì— ëŒ€í•´ ë” ìì„¸í•œ ì •ë³´ë‚˜ ì¶”ê°€ì ì¸ ë¹„êµë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°.
  (ì˜ˆ: "ê·¸ëŸ¬ë©´ ê³¼ì¼ì´ë‘ë„ ë¹„êµí•´ ì¤˜", "ë” ìì„¸í•œ ì •ë³´ ì•Œë ¤ì¤˜", "ê·¸ê±° ë§ê³  ë‹¤ë¥¸ ê±´ ì—†ì–´?")
- **crop**: ì§ˆë¬¸ì´ **ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²•**ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°. ì´ëŠ” ëŒ€í™”ì˜ ì²« ì§ˆë¬¸ì¼ ìˆ˜ë„ ìˆê³ , ëŒ€í™” ì¤‘ ì£¼ì œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë„ í•´ë‹¹í•©ë‹ˆë‹¤.
  (ì˜ˆ: "ê°€ì§€ ì¬ë°° ë°©ë²• ì•Œë ¤ì¤˜", "ë†ì•½ì€ ì–´ë–»ê²Œ ì¨?", "ê³ ì¶” í‚¤ìš°ëŠ” ë²•ì€?")
- **general**: ìœ„ ë‘ ê°€ì§€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ”, **ë†ì—…ê³¼ ê´€ë ¨ ì—†ëŠ”** ì™„ì „íˆ ìƒˆë¡œìš´ ì£¼ì œì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸.
  (ì˜ˆ: "24ì‹œê°„ì´ ëª‡ ì´ˆì•¼?", "ì„œìš¸ì˜ ë‚ ì”¨ëŠ”?")

[ëŒ€í™” ê¸°ë¡]
{chat_history}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
question_type_prompt = ChatPromptTemplate.from_template(QUESTION_TYPE_PROMPT_TMPL)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒíƒœ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GraphState(TypedDict):
    question: Optional[str]
    chat_history: List[BaseMessage]
    vectorstore: Optional[Any]
    context: Optional[str]
    answer: Optional[str]
    question_type: Optional[str]
    rewritten_query: Optional[str]
    previous_question_type: Optional[str]
    # ì¶”ê°€
    awaiting_region: Optional[bool]
    user_region: Optional[str]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_vectorstore(db_path: str) -> Any:
    try:
        return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

def format_chat_history(history: List[BaseMessage]) -> str:
    formatted_history = ""
    for message in history:
        if isinstance(message, HumanMessage):
            formatted_history += f"ì‚¬ìš©ì: {message.content}\n"
        elif isinstance(message, AIMessage):
            formatted_history += f"AI: {message.content}\n"
    return formatted_history

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ì¶”ê°€) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ & í’ˆì§ˆ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cosine(u: List[float], v: List[float]) -> float:
    dot = sum(ux * vx for ux, vx in zip(u, v))
    nu = math.sqrt(sum(ux * ux for ux in u))
    nv = math.sqrt(sum(vx * vx for vx in v))
    if nu == 0.0 or nv == 0.0:
        return 0.0
    return dot / (nu * nv)

def print_cosine_similarities(query_text: str, docs: List[Any], max_docs: int = 5):
    if not docs:
        return
    print("\nğŸ§® ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì •ì˜: cos(u, v) = (uÂ·v) / (||u||Â·||v||)")
    try:
        q_vec = embeddings.embed_query(query_text)
        doc_texts = [d.page_content for d in docs[:max_docs]]
        doc_vecs = embeddings.embed_documents(doc_texts)
        print("ğŸ“ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì§ˆë¬¸ vs. ë¬¸ì„œ ìƒìœ„ {}ê°œ):".format(len(doc_vecs)))
        for doc, dv in zip(docs[:max_docs], doc_vecs):
            cs = cosine(q_vec, dv)
            src = doc.metadata.get("source")
            print(f"  - cos_sim={cs:.4f} | source:{src} | {doc.page_content[:100]}...")
    except Exception as e:
        print(f"âš ï¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")

def compute_answer_context_similarity_texts(context_text: str, answer_text: str) -> float:
    try:
        ctx = context_text[:1500]
        ans = answer_text[:1500]
        v_ctx = embeddings.embed_query(ctx)
        v_ans = embeddings.embed_query(ans)
        sim = cosine(v_ctx, v_ans)
        print(f"ğŸ§® context-ë‹µë³€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = {sim:.4f}  (threshold={ANSWER_CONTEXT_SIM_THRESHOLD})")
        return sim
    except Exception as e:
        print(f"âš ï¸ context-ë‹µë³€ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return 0.0

_token_re = re.compile(r"[ê°€-í£A-Za-z0-9]+")
REGION_RE = re.compile(
    r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼|"
    r"[ê°€-í£]{2,10}(ì‹œ|êµ°|êµ¬)|[ê°€-í£]{2,10}(ì|ë©´|ë™))"
)

def _simple_tokens(text: str) -> List[str]:
    return [t for t in _token_re.findall(text or "") if len(t) > 1]

def extract_region_from_text(text: str) -> Optional[str]:
    if not text:
        return None
    m = REGION_RE.search(text)
    return m.group(0) if m else None

def question_needs_region(text: str) -> bool:
    if not text:
        return False
    t = text.replace(" ", "")
    keys = ["ì–´ë””ì„œ", "ì–´ë””ì—", "íŒë§¤", "íŒ”ìˆ˜", "ì§íŒ", "ë‚©í’ˆ", "ë„ë§¤ì‹œì¥", "ë¡œì»¬í‘¸ë“œ"]
    return any(k in t for k in keys)

def assess_answer_quality(answer: str, context: str, question: str) -> Dict[str, Any]:
    ans = (answer or "").strip()
    ctx = (context or "").strip()
    reasons = []
    metrics: Dict[str, Any] = {}

    metrics["length"] = len(ans)
    if len(ans) < QUALITY_MIN_CHARS:
        reasons.append(f"too_short(<{QUALITY_MIN_CHARS})")

    has_korean = bool(re.search(r"[ê°€-í£]", ans))
    metrics["has_korean"] = has_korean
    if QUALITY_REQUIRE_KOREAN and not has_korean:
        reasons.append("no_korean")

    ans_tokens = set(_simple_tokens(ans))
    ctx_tokens = set(_simple_tokens(ctx))
    overlap = 0.0
    if ans_tokens:
        overlap = len(ans_tokens & ctx_tokens) / max(1, len(ans_tokens))
    metrics["overlap_ratio"] = round(overlap, 4)
    if ctx and overlap < QUALITY_MIN_OVERLAP:
        reasons.append(f"low_term_overlap(<{QUALITY_MIN_OVERLAP})")

    line_count = len([ln for ln in ans.splitlines() if ln.strip()])
    metrics["line_count"] = line_count
    if len(ctx) >= CONTEXT_LONG_CHARS and line_count < QUALITY_MIN_LINES_IF_CONTEXT_LONG:
        reasons.append(f"too_few_lines(<{QUALITY_MIN_LINES_IF_CONTEXT_LONG})")

    ok = len(reasons) == 0
    print(f"ğŸ§ª í’ˆì§ˆ í‰ê°€: ok={ok} | metrics={metrics} | reasons={reasons}")
    return {"ok": ok, "reasons": reasons, "metrics": metrics}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LangGraph ë…¸ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_vs_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ")
    vs = load_vectorstore(VECTOR_DB_PATH.as_posix())
    return {**state, "vectorstore": vs}

def classify_question_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    formatted_chat_history = format_chat_history(chat_history)

    prev_type = state.get("question_type")

    chain = question_type_prompt | make_llm() | StrOutputParser()
    try:
        result = chain.invoke({"question": question, "chat_history": formatted_chat_history}).strip().lower()
        # íŒë§¤/ì–´ë””ì„œ ê³„ì—´ì€ ì›¹ê²€ìƒ‰ ì¹œí™” â†’ generalë¡œ ë³´ì •
        if question_needs_region(question):
            result = "general"
        if result not in ['crop', 'general', 'context']:
            result = 'general'
        print(f"LLMì´ íŒë‹¨í•œ ì§ˆë¬¸ ìœ í˜•: {result}")
    except Exception as e:
        print(f"âŒ LLM ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’(general)ìœ¼ë¡œ ì„¤ì •: {e}")
        result = 'general'
    return {**state, "previous_question_type": prev_type, "question_type": result}

def check_location_node(state: GraphState) -> Dict[str, Any]:
    """ì§€ì—­ì´ í•„ìš”í•œë° ì—†ìœ¼ë©´ ë¨¼ì € ì§€ì—­ì„ ë¬¼ì–´ë³¸ë‹¤."""
    print("ğŸ§© ë…¸ë“œ: ì§€ì—­ í•„ìš” ì—¬ë¶€ í™•ì¸")
    q = state["question"] or ""
    chat_history = state.get("chat_history", [])
    hist_text = format_chat_history(chat_history)
    region = extract_region_from_text(q) or extract_region_from_text(hist_text)

    needs = question_needs_region(q)
    print(f"ì§€ì—­ ì¶”ì¶œ: region={region} | needs_region={needs}")

    if needs and not region:
        ask = (
            "íŒë§¤ ì±„ë„ì€ ì§€ì—­ì— ë”°ë¼ ë‹¬ë¼ìš”.\n"
            "ì–´ëŠ ì§€ì—­(ì‹œ/êµ°/êµ¬)ì—ì„œ íŒë§¤í•˜ì‹œë‚˜ìš”?\n"
            "ì˜ˆ: 'ê²½ê¸° ìš©ì¸ì‹œ ìˆ˜ì§€êµ¬', 'ê´‘ì£¼ ë¶êµ¬', 'ê²½ë‚¨ ê¹€í•´ì‹œ'"
        )
        return {**state, "answer": ask, "awaiting_region": True, "user_region": None}
    return {**state, "awaiting_region": False, "user_region": region}

def retrieve_and_generate_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: Auto-RAG ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±")
    vs = state.get("vectorstore")
    if not vs:
        return {**state, "answer": "ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    q = state["question"] or ""
    # ì§€ì—­ì´ ìˆë‹¤ë©´ ì§ˆë¬¸ì— ì£¼ì…í•´ ê²€ìƒ‰ íŒíŠ¸ ê°•í™”
    if state.get("user_region"):
        q = f"{q} (íŒë§¤ ì§€ì—­: {state['user_region']})"

    chat_history = state.get("chat_history", [])
    formatted_chat_history = format_chat_history(chat_history)

    base_retriever = vs.as_retriever(search_kwargs={"k": 12})
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=make_llm()
    )

    try:
        from langchain_cohere import CohereRerank
        compressor = CohereRerank(model="rerank-multilingual-v3.0")
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=multi_query_retriever
        )

        retrieval_query = q if state["question_type"] != 'context' else (formatted_chat_history + "\n" + q)
        print("ğŸ” ë©€í‹° ì¿¼ë¦¬ + Rerank ê²€ìƒ‰...")
        retrieved_docs = compression_retriever.invoke(retrieval_query)

        SCORE_THRESHOLD = 0.6
        filtered_docs = [d for d in retrieved_docs if d.metadata.get('relevance_score', 0) >= SCORE_THRESHOLD]
        if not filtered_docs and retrieved_docs:
            print("âš ï¸ ìƒí–¥ ì™„í™”: í•„í„° ì‹¤íŒ¨ â†’ ìƒìœ„ 5ê°œ ì‚¬ìš©")
            filtered_docs = retrieved_docs[:5]

        seen = set()
        unique_docs = []
        for d in filtered_docs:
            src = d.metadata.get("source")
            if src in seen:
                continue
            seen.add(src)
            unique_docs.append(d)
        filtered_docs = unique_docs[:5]

        if not filtered_docs:
            print("âŒ í•„í„°ë§ ë¬¸ì„œ ì—†ìŒ â†’ 'ì •ë³´ ë¶€ì¡±'")
            return {**state, "context": "", "answer": "ì •ë³´ ë¶€ì¡±"}

        print("\nğŸ’¡ ì°¸ê³ í•œ ë¬¸ì„œì™€ ì ìˆ˜:")
        for doc in filtered_docs:
            score = doc.metadata.get('relevance_score', 0)
            source = doc.metadata.get('source')
            page = doc.metadata.get('page')
            print(f"  - ì ìˆ˜:{score:.4f} | source:{source} | page:{page} | ë‚´ìš©:{doc.page_content[:100]}...")
        print("-----------------------------------")

        print_cosine_similarities(retrieval_query, filtered_docs)

        ctx = "\n\n".join([doc.page_content for doc in filtered_docs])
        if not ctx.strip():
            return {**state, "context": "", "answer": "ì •ë³´ ë¶€ì¡±"}

        chain = rag_prompt | make_llm() | StrOutputParser()
        ans = chain.invoke({"context": ctx, "question": q, "chat_history": formatted_chat_history})
        return {**state, "context": ctx, "answer": ans}

    except ImportError:
        print("âš ï¸ CohereRerank ë¯¸ì‚¬ìš©: ê¸°ë³¸ ê²€ìƒ‰")
        retrieved_docs = multi_query_retriever.invoke(q)
        if not retrieved_docs:
            return {**state, "context": "", "answer": "ì •ë³´ ë¶€ì¡±"}

        print_cosine_similarities(q, retrieved_docs)

        ctx = "\n\n".join([d.page_content for d in retrieved_docs])
        if not ctx.strip():
            return {**state, "context": "", "answer": "ì •ë³´ ë¶€ì¡±"}
        chain = rag_prompt | make_llm() | StrOutputParser()
        ans = chain.invoke({"context": ctx, "question": q, "chat_history": formatted_chat_history})
        return {**state, "context": ctx, "answer": ans}

def rewrite_query_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì§ˆë¬¸ í™•ì¥")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    region = state.get("user_region")

    if state["question_type"] == 'general':
        formatted_chat_history = ""
    else:
        formatted_chat_history = format_chat_history(chat_history)

    q_for_expand = question
    if region and region not in q_for_expand:
        q_for_expand = f"{q_for_expand} (íŒë§¤ ì§€ì—­: {region})"
    print("ğŸ” ëŒ€í™” ê¸°ë¡ ì°¸ê³  ì—¬ë¶€:", "ì—†ìŒ" if state["question_type"] == 'general' else "ìˆìŒ")

    chain = query_expansion_prompt | make_llm() | StrOutputParser()
    try:
        rewritten_query = chain.invoke({"question": q_for_expand, "chat_history": formatted_chat_history}).strip()
        if len(rewritten_query) < 10:
            rewritten_query = q_for_expand
        print(f"âœ… í™•ì¥ëœ ê²€ìƒ‰ ì§ˆì˜: {rewritten_query}")
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ í™•ì¥ ì‹¤íŒ¨: {e}")
        rewritten_query = q_for_expand

    return {**state, "rewritten_query": rewritten_query}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì›¹ ê²€ìƒ‰")
    query = state.get("rewritten_query", state["question"])
    region = state.get("user_region")
    if region and region not in query:
        query = f"{query} (íŒë§¤ ì§€ì—­: {region})"

    chat_history = state.get("chat_history", [])
    tavily_tool = TavilySearchResults(max_results=5)

    formatted_chat_history = "" if state["question_type"] == 'general' else format_chat_history(chat_history)

    print(f"ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì§ˆì˜: {query})")
    try:
        search_results = tavily_tool.invoke({"query": query, "search_depth": "advanced"})
        if not search_results:
            print("âš ï¸ 1ì°¨ ê²°ê³¼ ì—†ìŒ â†’ basicë¡œ ì¬ì‹œë„")
            search_results = tavily_tool.invoke({"query": query})

        if not search_results:
            print("âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {**state, "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}

        search_results_str = "\n".join([str(res) for res in search_results])

    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {**state, "answer": "ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    print("ğŸ’¡ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    chain = web_search_prompt | make_llm() | StrOutputParser()
    web_answer = chain.invoke({
        "search_results": search_results_str,
        "question": query,
        "chat_history": formatted_chat_history
    })
    return {**state, "answer": web_answer}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¡°ê±´ë¶€ ë¼ìš°íŒ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def decide_route_from_type(state: GraphState) -> str:
    qtype = state.get("question_type")
    print("ğŸ§© ë…¸ë“œ: ê²½ë¡œ ê²°ì • (ì§ˆë¬¸ ìœ í˜•) =", qtype)
    if qtype == "general":
        return "rewrite_query"
    return "retrieve_and_generate"

def decide_after_location(state: GraphState) -> str:
    """ì§€ì—­ í™•ì¸ ì´í›„ ë¶„ê¸°: ì§€ì—­ ëŒ€ê¸°ë©´ ask_region, ì•„ë‹ˆë©´ ìœ í˜•ëŒ€ë¡œ ì§„í–‰"""
    if state.get("awaiting_region"):
        return "ask_region"
    return decide_route_from_type(state)

def decide_route_from_retrieve(state: GraphState) -> str:
    print("ğŸ§© ë…¸ë“œ: ê²½ë¡œ ê²°ì • (DB ê²€ìƒ‰ ê²°ê³¼)")
    answer = (state.get("answer") or "").strip()
    ctx = (state.get("context") or "").strip()

    if ("ì •ë³´ ë¶€ì¡±" in answer) or ("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer) or (not ctx):
        print("ê²°ì •: ë‚´ë¶€ DB ë¶€ì¡±/ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ì§ˆë¬¸ í™•ì¥ í›„ ì›¹ê²€ìƒ‰ ì§„í–‰")
        return "rewrite_query"

    try:
        sim = compute_answer_context_similarity_texts(ctx, answer)
        if sim < ANSWER_CONTEXT_SIM_THRESHOLD:
            print(f"ê²°ì •: context-ë‹µë³€ ìœ ì‚¬ë„ ë¯¸ë‹¬ ({sim:.4f} < {ANSWER_CONTEXT_SIM_THRESHOLD}) â†’ ì›¹ê²€ìƒ‰ ê²½ë¡œ")
            return "rewrite_query"
    except Exception as e:
        print(f"âš ï¸ ìœ ì‚¬ë„ ê²Œì´íŠ¸ ì˜¤ë¥˜: {e} â†’ ì•ˆì „í•˜ê²Œ ì›¹ê²€ìƒ‰ ê²½ë¡œ")
        return "rewrite_query"

    qres = assess_answer_quality(answer, ctx, state.get("question", ""))
    if not qres["ok"]:
        print(f"ê²°ì •: í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬({qres['reasons']}) â†’ ì›¹ê²€ìƒ‰ ê²½ë¡œ")
        return "rewrite_query"

    print("ê²°ì •: ìœ ì‚¬ë„/í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ â†’ ì¢…ë£Œ")
    return "end"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê·¸ë˜í”„ ë¹Œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_vs", load_vs_node)
    g.add_node("classify_question", classify_question_node)
    g.add_node("check_location", check_location_node)
    g.add_node("retrieve_and_generate", retrieve_and_generate_node)
    g.add_node("rewrite_query", rewrite_query_node)
    g.add_node("web_search", web_search_node)

    g.set_entry_point("load_vs")
    g.add_edge("load_vs", "classify_question")
    g.add_edge("classify_question", "check_location")

    # ì§€ì—­ í™•ì¸ í›„: ask_region â†’ END / ì•„ë‹ˆë©´ ì§ˆë¬¸ ìœ í˜•ì— ë§ê²Œ
    g.add_conditional_edges(
        "check_location",
        decide_after_location,
        {
            "ask_region": END,
            "retrieve_and_generate": "retrieve_and_generate",
            "rewrite_query": "rewrite_query",
        },
    )

    g.add_conditional_edges(
        "retrieve_and_generate",
        decide_route_from_retrieve,
        {"end": END, "rewrite_query": "rewrite_query"}
    )

    g.add_edge("rewrite_query", "web_search")
    g.add_edge("web_search", END)

    return g.compile()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# JSON íŒŒì¼ ê´€ë ¨ (ëŒ€í™” ê¸°ë¡)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_chat_history() -> List[BaseMessage]:
    if CHAT_HISTORY_PATH.exists():
        try:
            with open(CHAT_HISTORY_PATH, "r", encoding="utf-8") as f:
                history_data = json.load(f)
            chat_history = []
            for item in history_data:
                if item["role"] == "user":
                    chat_history.append(HumanMessage(content=item["content"]))
                elif item["role"] == "assistant":
                    chat_history.append(AIMessage(content=item["content"]))
            print(f"âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ({len(chat_history)}ê°œ)ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return chat_history
        except (json.JSONDecodeError, FileNotFoundError) as e:
            print(f"âŒ ëŒ€í™” ê¸°ë¡ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    return []

def save_chat_history(history: List[BaseMessage]):
    try:
        history_data = []
        for message in history:
            if isinstance(message, HumanMessage):
                history_data.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                history_data.append({"role": "assistant", "content": message.content})
        with open(CHAT_HISTORY_PATH, "w", encoding="utf-8") as f:
            json.dump(history_data, f, ensure_ascii=False, indent=4)
        print("âœ… ëŒ€í™” ê¸°ë¡ì„ 'chat_history.json' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸš€ LangGraph ê¸°ë°˜ì˜ Auto-RAG ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    app = build_graph()
    chat_history: List[BaseMessage] = load_chat_history()

    # ê·¸ë˜í”„ ì‹œê°í™” (PNG ì‹¤íŒ¨ ì‹œ ASCII + Mermaid ì†ŒìŠ¤ ì €ì¥)
    try:
        graph_image_path = BASE_DIR / "agent_workflow_llm.png"
        png_bytes = app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)
        with open(graph_image_path, "wb") as f:
            f.write(png_bytes)
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            ascii_map = app.get_graph().draw_ascii()
            print("\n[ASCII Graph]")
            print(ascii_map)
            mermaid_src = app.get_graph().draw_mermaid()
            mmd_path = BASE_DIR / "agent_workflow.mmd"
            with open(mmd_path, "w", encoding="utf-8") as f:
                f.write(mermaid_src)
            print(f"ğŸ“ Mermaid ì†ŒìŠ¤ë¥¼ '{mmd_path}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (mermaid.live ë“±ì—ì„œ ë Œë” ê°€ëŠ¥)")
        except Exception as e2:
            print(f"ì¶”ê°€ ë°±ì—…ë„ ì‹¤íŒ¨: {e2}")

    # ì§€ì—­ ì…ë ¥ ëŒ€ê¸° ìƒíƒœ ê´€ë¦¬ (ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ë³€ìˆ˜)
    pending_need_region = False
    pending_base_question = ""

    while True:
        print("\n" + "="*50)
        q = input("ì§ˆë¬¸ > ").strip()
        print("="*50 + "\n")

        if q.lower() in ("exit", "quit"):
            save_chat_history(chat_history)
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        if not q:
            continue

        try:
            # ì§ì „ í„´ì—ì„œ ì§€ì—­ì„ ë¬¼ì—ˆë‹¤ë©´, ì´ë²ˆ ì…ë ¥ì„ ì§€ì—­ìœ¼ë¡œ ê°„ì£¼í•´ ì¬ì¡°í•©
            if pending_need_region:
                region = q
                # ì´ì „ ì§ˆë¬¸ì— ì§€ì—­ì„ ì£¼ì…í•´ ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ ì¬ì§ˆë¬¸
                combined_q = (
                    f"{pending_base_question} (íŒë§¤ ì§€ì—­: {region}) "
                    f"í•´ë‹¹ ì§€ì—­ ê¸°ì¤€ìœ¼ë¡œ ê³µì˜ë„ë§¤ì‹œì¥, ë¡œì»¬í‘¸ë“œ ì§ë§¤ì¥, ë†í˜‘/ì¶•í˜‘ íŒë§¤ì²˜, "
                    f"ì‚°ì§€ìœ í†µì„¼í„°(APC), ì˜¨ë¼ì¸/íƒë°° ì±„ë„ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜."
                )
                print(f"ğŸ” ì§€ì—­ ì…ë ¥ í™•ì¸ â†’ ì¬ì§ˆë¬¸: {combined_q}")
                q = combined_q
                pending_need_region = False
                pending_base_question = ""

            state: Dict[str, Any] = {"question": q, "chat_history": chat_history}
            final_state = app.invoke(state)
            answer = final_state["answer"]

            # ì§€ì—­ ëŒ€ê¸° ì‹ í˜¸ê°€ ì˜¤ë©´ ë‹¤ìŒ í„´ì— ì§€ì—­ ì…ë ¥ ëŒ€ê¸°
            if final_state.get("awaiting_region"):
                pending_need_region = True
                pending_base_question = q  # ì› ì§ˆë¬¸ ì €ì¥
                print("\n--- ë‹µë³€(ì§€ì—­ ìš”ì²­) ---")
                print(answer)
                print("------------\n")
            else:
                # ì§ˆë¬¸ ìœ í˜• ì „í™˜ì— ë”°ë¥¸ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ì˜ˆ: general â†’ crop)
                if final_state.get('question_type') == 'crop' and final_state.get('previous_question_type') == 'general':
                    chat_history = []

                print("\n--- ë‹µë³€ ---")
                print(answer)
                print("------------\n")

            # ëŒ€í™”ê¸°ë¡ ë°˜ì˜
            chat_history.append(HumanMessage(content=q if not final_state.get("awaiting_region") else final_state.get("question","")))
            chat_history.append(AIMessage(content=answer))

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n")

if __name__ == "__main__":
    main()
