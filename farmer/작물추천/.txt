cd farmer\ì‘ë¬¼ì¶”ì²œ -> CMD ê²½ë¡œ ë“¤ì–´ê°€ì„œ Crop65pdfllm.py íŒŒì¼ ì‹¤í–‰

cd farmer\ì‘ë¬¼ì¶”ì²œ\Goldenset_test -> Goldenset_test.py íŒŒì¼ ì‹¤í–‰ <- ê³¨ë“ ì…‹ í‰ê°€ ì‹¤í–‰ log ê¸°ë¡

cd farmer\ì‘ë¬¼ì¶”ì²œ\csv_change_need> -> pdf ë³€ê²½


vectordb ìœ„ì¹˜ 
C:/Rookies_project/ET_Agent/Crop_Recommendations_DB/faiss_pdf_db


-------------------------

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
# pip install langchain-huggingface langchain_community langchain-core langchain-groq langgraph pymilvus python-dotenv tavily-python ragas datasets

print("â–¶ [ì´ˆê¸°í™”] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹œì‘...")
import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re

# --- RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
print("â–¶ [ì´ˆê¸°í™”] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ...")
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-8b-8192")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from pymilvus import connections
print("â–¶ [ì´ˆê¸°í™”] ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ìœ¼ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€ì„ ì¢…í•©í•˜ê³  ì •ë¦¬í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ë˜, ì§ì ‘ì ì¸ ë‚´ìš©ì´ ì—†ë”ë¼ë„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœëŒ€í•œ ìœ ìš©í•œ ë‹µë³€ì„ ë§Œë“œì„¸ìš”.
- ë‚´ìš©ì€ ëª…í™•í•˜ê²Œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ê²€ìƒ‰ ê²°ê³¼ë¡œ ì •ë§ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•  ë•Œë§Œ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- [ì¤‘ìš”] ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- í•©ì„± ë ˆí¼ëŸ°ìŠ¤(ì •ë‹µ) ìƒì„± í”„ë¡¬í”„íŠ¸ ---
SYNTHETIC_REF_PROMPT = ChatPromptTemplate.from_template("""
ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•œ 'ì •ë‹µ' í•œ ë‹¨ë½ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
ì´ ì •ë‹µì€ ë‚˜ì¤‘ì— LLMì˜ ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.
**ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
ì§ˆë¬¸: {question}
ì»¨í…ìŠ¤íŠ¸: {context}
ì •ë‹µ:
""")


# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]
    vectorstore: Optional[Milvus]
    context: Optional[str]
    answer: Optional[str]
    web_search_results: Optional[str]
    log_file: Optional[str]
    answer_source: Optional[str]
    ragas_score: Optional[float]
    rag_retry_count: Optional[int]
    original_docs: List[str] # RAGAS context ì§€í‘œë¥¼ ìœ„í•´ ì¶”ê°€
    web_contexts: List[str]  # ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸(ë¦¬ìŠ¤íŠ¸)
    reference: Optional[str] # í•©ì„± ì •ë‹µ(reference) ì¶”ê°€

# --- Embeddings ë° LLM ---
print("â–¶ [ì´ˆê¸°í™”] ì„ë² ë”© ë° LLM ëª¨ë¸ ê°ì²´ ìƒì„± ì‹œì‘...")
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)
llm = ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)
print("â–¶ [ì´ˆê¸°í™”] ì„ë² ë”© ë° LLM ëª¨ë¸ ê°ì²´ ìƒì„± ì™„ë£Œ.")

# --- ìœ í‹¸: ëŒ€í™” ë¡œê·¸ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, source: str, filename: str):
    sentences = re.split(r'(?<=[.!?])\s+', answer.strip())
    sentences = [s for s in sentences if s]
    data = {"timestamp": datetime.now().isoformat(), "question": question, "answer": sentences, "source": source}
    if filename:
        try:
            hist = []
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                with open(filename, "r", encoding="utf-8") as f: hist = json.load(f)
            hist.append(data)
            with open(filename, "w", encoding="utf-8") as f: json.dump(hist, f, ensure_ascii=False, indent=4)
            print(f" Â  Â  Â  ğŸ’¾ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f" Â  Â  Â  âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")


# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("\n--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
    try:
        vs = Milvus(embedding_model, collection_name=MILVUS_COLLECTION, connection_args={"uri": MILVUS_URI, "token": MILVUS_TOKEN})
        print(f" Â  Â  Â  âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f" Â  Â  Â  âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")
    if not question or not vectorstore: raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f" Â  Â  Â  ğŸ“¥ ê²€ìƒ‰ ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=2)

    context = ""
    print(f" Â  Â  Â  ğŸ“„ {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")
    for i, (doc, score) in enumerate(docs_with_scores):
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f" Â  Â  Â  Â  â–¶ ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")
        context += f"\n\n{doc.page_content}"
    print(f" Â  Â  Â  ğŸ“ ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ì")
    
    original_docs = [doc.page_content for doc, score in docs_with_scores]
    return {**state, "context": context, "rag_retry_count": 0, "original_docs": original_docs}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    rag_retries = state.get("rag_retry_count", 0) + 1
    print(f" Â  Â  Â  ğŸ”„ RAG ìƒì„± ì‹œë„: {rag_retries}ë²ˆì§¸")

    context = state.get("context", "")
    question = state.get("question")
    if not question: raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f" Â  Â  Â  â–¶ ì…ë ¥ ì»¨í…ìŠ¤íŠ¸: '{context[:100].replace('\n', ' ')}...'")
    chain = (rag_prompt | llm | StrOutputParser())
    ans = chain.invoke({"context": context, "question": question})
    print(f" Â  Â  Â  ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    return {**state, "answer": ans, "answer_source": "ë‚´ë¶€ DB", "rag_retry_count": rag_retries}

def ragas_eval_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAGAS ë‹µë³€ í‰ê°€ ---")
    question = state.get("question")
    answer = state.get("answer", "")
    source = state.get("answer_source", "N/A")

    # ì»¨í…ìŠ¤íŠ¸ í™•ë³´ (List[str])
    if source == "ì›¹ ê²€ìƒ‰":
        eval_context = state.get("web_search_results", "")
        context_source_name = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
        contexts_for_ragas = state.get("web_contexts", [])
    else:
        eval_context = state.get("context", "")
        context_source_name = "ë‚´ë¶€ DB ë¬¸ì„œ"
        contexts_for_ragas = state.get("original_docs", [])
        if not contexts_for_ragas and eval_context:
            contexts_for_ragas = [eval_context]

    print(f" Â  Â  Â  â–¶ í‰ê°€ ëŒ€ìƒ ({source}): '{answer[:100].replace('\n', ' ')}...'")
    head = (eval_context[:100] if isinstance(eval_context, str) else str(eval_context)[:100]).replace('\n', ' ')
    print(f" Â  Â  Â  â–¶ í‰ê°€ ê¸°ì¤€ ({context_source_name}): '{head}...'")

    if not question or not answer or not contexts_for_ragas:
        print(" Â  Â  Â  âš ï¸ í‰ê°€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "ragas_score": 0.0}

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ’¡ LLMì„ ì‚¬ìš©í•˜ì—¬ í•©ì„±(Synthetic) ì •ë‹µ ìƒì„±
    try:
        print(" Â  Â  Â  âœï¸ LLMìœ¼ë¡œ 'í•©ì„± ì •ë‹µ(reference)' ìƒì„± ì¤‘...")
        chain = (SYNTHETIC_REF_PROMPT | llm | StrOutputParser())
        reference = chain.invoke({"question": question, "context": "\n\n".join(contexts_for_ragas)[:4000]}).strip()
        state["reference"] = reference # ìƒíƒœì— ì €ì¥
        print(f" Â  Â  Â  âœ… í•©ì„± ì •ë‹µ ìƒì„± ì™„ë£Œ: '{reference[:50].replace('\n', ' ')}...'")
    except Exception as e:
        print(f" Â  Â  Â  âŒ í•©ì„± ì •ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        reference = None
        state["reference"] = None
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # ë°ì´í„°ì…‹ êµ¬ì„± (referenceê°€ ìˆìœ¼ë©´ í¬í•¨)
    dataset_dict = {
        "question": [question],
        "answer": [answer],
        "contexts": [contexts_for_ragas],
    }
    if reference:
        dataset_dict["reference"] = [reference]

    dataset = Dataset.from_dict(dataset_dict)
    
    def to_df(result):
        if hasattr(result, "to_pandas"):
            return result.to_pandas()
        import pandas as pd
        return pd.DataFrame(result)

    def harmonic_mean(vals: List[float]) -> float:
        eps = 1e-8
        vals = [max(float(v), eps) for v in vals if v is not None]
        if not vals:
            return 0.0
        return len(vals) / sum(1.0 / v for v in vals)

    # 1) 4ì§€í‘œ ì‹œë„
    metrics_all = [faithfulness, answer_relevancy, context_recall, context_precision]
    try:
        result = evaluate(
            dataset,
            metrics=metrics_all,
            llm=llm,
            embeddings=embedding_model,
            raise_exceptions=True,
        )
        df = to_df(result)
        f  = float(df.get("faithfulness", [0.0])[0])
        ar = float(df.get("answer_relevancy", [0.0])[0])
        cr = float(df.get("context_recall", [0.0])[0])
        cp = float(df.get("context_precision", [0.0])[0])

        print("\n--- ğŸ“Š RAGAS í‰ê°€ ìƒì„¸ ì ìˆ˜ (4ì§€í‘œ) ---")
        print(f" Â  ğŸŸ¢ ì¶©ì‹¤ë„ (Faithfulness): Â  Â  Â  Â {f:.4f}")
        print(f" Â  ğŸŸ¢ ê´€ë ¨ì„± (Answer Relevancy): Â  Â {ar:.4f}")
        print(f" Â  ğŸŸ¢ ë¬¸ë§¥ ì¬í˜„ìœ¨ (Context Recall): {cr:.4f}")
        print(f" Â  ğŸŸ¢ ë¬¸ë§¥ ì •ë°€ë„ (Context Precision): {cp:.4f}")

        final_score = harmonic_mean([f, ar, cr, cp])
        print(f" Â  Â  Â  ğŸ“Š ìµœì¢… RAGAS ì¡°í™”í‰ê·  ì ìˆ˜: {final_score:.4f}")
        return {**state, "ragas_score": final_score}

    except Exception as e:
        msg = str(e)
        need_ref = "requires the following additional columns ['reference']" in msg
        print(f" Â  Â  Â  âš ï¸ 4ì§€í‘œ í‰ê°€ ì‹¤íŒ¨: {e}")
        if need_ref:
            print(" Â  Â  Â  â„¹ï¸ reference(ì •ë‹µ) ìƒì„± ì‹¤íŒ¨ë¡œ 4ì§€í‘œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 2ì§€í‘œë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        else:
            print(" Â  Â  Â  â„¹ï¸ ì˜ˆì™¸ë¡œ ì¸í•´ 2ì§€í‘œë¡œ í´ë°±í•©ë‹ˆë‹¤.")

    # 2) 2ì§€í‘œ í´ë°± (reference ë¶ˆí•„ìš”)
    try:
        metrics_two = [faithfulness, answer_relevancy]
        result = evaluate(
            dataset,
            metrics=metrics_two,
            llm=llm,
            embeddings=embedding_model,
            raise_exceptions=False,
        )
        df = to_df(result)
        f  = float(df.get("faithfulness", [0.0])[0])
        ar = float(df.get("answer_relevancy", [0.0])[0])

        print("\n--- ğŸ“Š RAGAS í‰ê°€ ìƒì„¸ ì ìˆ˜ (2ì§€í‘œ í´ë°±) ---")
        print(f" Â  ğŸŸ¢ ì¶©ì‹¤ë„ (Faithfulness): Â  Â  {f:.4f}")
        print(f" Â  ğŸŸ¢ ê´€ë ¨ì„± (Answer Relevancy): {ar:.4f}")
        print(f" Â  âšª ë¬¸ë§¥ ì¬í˜„ìœ¨ (Context Recall): N/A (ì •ë‹µ(reference) ì—†ìŒ)")
        print(f" Â  âšª ë¬¸ë§¥ ì •ë°€ë„ (Context Precision): N/A (ì •ë‹µ(reference) ì—†ìŒ)")

        final_score = (f + ar) / 2.0
        print(f" Â  Â  Â  ğŸ“Š ìµœì¢… RAGAS í‰ê·  ì ìˆ˜(í´ë°±): {final_score:.4f}")
    except Exception as e2:
        print(f" Â  Â  Â  âŒ RAGAS 2ì§€í‘œ í´ë°± ì¤‘ ì˜¤ë¥˜: {e2}")
        final_score = 0.0

    return {**state, "ragas_score": final_score}


def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not TAVILY_API_KEY:
        print(" Â  Â  Â  âš ï¸ TAVILY_API_KEYê°€ ì—†ì–´ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”", "web_contexts": []}
    
    print(f" Â  Â  Â  ğŸ” ì›¹ ê²€ìƒ‰ì–´: '{question}'")
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question}) 
    
    web_contexts: List[str] = []
    for r in results:
        title = (r.get("title") or "").strip()
        content = (r.get("content") or r.get("snippet") or "").strip()
        url = (r.get("url") or "").strip()
        passage = f"{title}\n{content}\nURL: {url}".strip()
        web_contexts.append(passage)

    sr = json.dumps(results, ensure_ascii=False)
    print(f" Â  Â  Â  ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê°œ ìˆ˜ì‹  ì™„ë£Œ.")
    return {**state, "web_search_results": sr, "web_contexts": web_contexts}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results", "")
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print(" Â  Â  Â  âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "answer_source": "ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨"}
    
    print(f" Â  Â  Â  â–¶ ì…ë ¥ ì›¹ ì»¨í…ìŠ¤íŠ¸: '{search_results[:150].replace('\n', ' ')}...'")
    chain = (web_prompt | llm | StrOutputParser())
    ans = chain.invoke({"question": question, "search_results": search_results})
    print(f" Â  Â  Â  ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    return {**state, "answer": ans, "answer_source": "ì›¹ ê²€ìƒ‰"}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    source = state.get("answer_source", "ì•Œ ìˆ˜ ì—†ìŒ")
    score = state.get("ragas_score")
    score_text = f"{score:.4f}" if score is not None else "N/A"
    
    print("\n" + "="*50)
    print("ğŸ¤– ìµœ ì¢… ë‹µ ë³€")
    print("="*50)
    print(f"âœ… ë‹µë³€ ì¶œì²˜: {source} (RAGAS ì ìˆ˜: {score_text})")
    print(f"\n{answer}")
    print("="*50)

    log_file = state.get("log_file") or ""
    q_for_log = state.get("question") or ""
    append_conversation_to_file(q_for_log, answer, source, log_file)
    return state

# --- ë¼ìš°í„° ---
def master_router(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ê²½ë¡œ ê²°ì • ---")
    score = state.get("ragas_score", 0.0)
    source = state.get("answer_source", "")
    rag_retries = state.get("rag_retry_count", 0)
    
    SCORE_THRESHOLD = 0.7
    RAG_RETRY_LIMIT = 2
    
    if source == "ì›¹ ê²€ìƒ‰":
        print(" Â  Â  Â  â¡ï¸ ê²°ì •: ì›¹ ë‹µë³€ í‰ê°€ ì™„ë£Œ. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"

    print(f" Â  Â  Â  ğŸ“Š í‰ê°€ ì ìˆ˜ (ë‚´ë¶€ DB): {score:.4f} (ì„ê³„ê°’: {SCORE_THRESHOLD})")
    print(f" Â  Â  Â  ğŸ”„ RAG ì¬ì‹œë„: {rag_retries}/{RAG_RETRY_LIMIT}")

    if score >= SCORE_THRESHOLD:
        print(" Â  Â  Â  â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ í†µê³¼. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"
    else:
        if rag_retries < RAG_RETRY_LIMIT:
            print(f" Â  Â  Â  â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬. {rag_retries + 1}ë²ˆì§¸ ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ëŒì•„ê°‘ë‹ˆë‹¤.")
            return "retry_rag"
        else:
            print(f" Â  Â  Â  â¡ï¸ ê²°ì •: RAG ì¬ì‹œë„ í•œë„ ë„ë‹¬. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.")
            return "augment_with_web"


# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    print("â–¶ [ê·¸ë˜í”„ ì„¤ì •] LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘...")
    g = StateGraph(GraphState)
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("ragas_eval", ragas_eval_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_edge("generate_rag", "ragas_eval")
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "ragas_eval")

    g.add_conditional_edges(
        "ragas_eval",
        master_router,
        {
            "retry_rag": "generate_rag",
            "augment_with_web": "web_search",
            "end_journey": "generate_answer"
        }
    )
    
    g.add_edge("generate_answer", END)
    print("â–¶ [ê·¸ë˜í”„ ì„¤ì •] LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì™„ë£Œ.")
    return g.compile()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch (RAGAS í‰ê°€ í¬í•¨) ì‹œì‘ (ì¢…ë£Œ: exit ë˜ëŠ” quit ì…ë ¥)")

    log_dir = "milvusdb_crop65llm_logs"
    print(f"â–¶ [ë©”ì¸ ì‹¤í–‰] ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±: {log_dir}")
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()

    print("â–¶ [ë©”ì¸ ì‹¤í–‰] ê·¸ë˜í”„ ì‹œê°í™” ì‹œë„...")
    graph_image_path = "milvus_agent_workflow_llm.png"
    if os.path.exists(graph_image_path):
        print(f"\nâ„¹ï¸ LangGraph ì‹œê°í™” ì´ë¯¸ì§€ '{graph_image_path}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒëµí•©ë‹ˆë‹¤.")
    else:
        try:
            with open(graph_image_path, "wb") as f:
                f.write(app.get_graph().draw_mermaid_png())
            print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(" Â  (ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” 'mermaid-cli'ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    print("\nâ–¶ [ë©”ì¸ ì‹¤í–‰] ëŒ€í™” ë£¨í”„ ì‹œì‘. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    while True:
        q = input("\n\nì§ˆë¬¸> ").strip()
        if not q or q.lower() in ("exit", "quit"):
            print("ğŸ’¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸšª")
            break

        print("-" * 60)
        print(f"ğŸš€ ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{q}'")
        app.invoke({"question": q, "log_file": session_log_file})
        print(f"ğŸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ.")
        print("-" * 60)
        
    print("\ní”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")