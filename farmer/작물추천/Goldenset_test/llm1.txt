import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(find_dotenv())

# Milvus / Embedding ì„¤ì •
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# LLM ì„¤ì •
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7")) # <-- TEMPERATURE ìœ ì§€

# Web Search ì„¤ì •
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# LangChain / LangGraph
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.runnables.graph import MermaidDrawMethod
from pymilvus import connections

# --- í”„ë¡¬í”„íŠ¸ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    """LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤."""
    question: Optional[str]
    vectorstore: Optional[Milvus]
    context: Optional[str]
    answer: Optional[str]
    web_search_results: Optional[str]

# --- Embeddings ë° LLM ---
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    """ChatGroq LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# --- LangGraph ë…¸ë“œë“¤ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    """Milvus ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ê³  ìƒíƒœì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        print("    - Milvus ì—°ê²°ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        connections.connect(alias="default", host="localhost", port="19530")

    try:
        vs = Milvus(
            embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"host": "localhost", "port": "19530"}
        )
        print("    âœ… Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ. (ì»¬ë ‰ì…˜: {})".format(MILVUS_COLLECTION))
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"    âŒ Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    """ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ Milvusì—ì„œ ê²€ìƒ‰í•˜ê³  ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")

    if not question or not vectorstore:
        print("    âŒ ì˜¤ë¥˜: ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    print(f"    - ì§ˆë¬¸: '{question}'")
    # ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¬¸ì„œ ê²€ìƒ‰
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)
    
    context = ""
    print(f"    âœ… {len(docs_with_scores)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")
    print("    ğŸ“„ **ê²€ìƒ‰ëœ ë¬¸ì„œ ë° ìœ ì‚¬ë„ ì ìˆ˜:**")
    for i, (doc, score) in enumerate(docs_with_scores):
        context += f"\n\n{doc.page_content}"
        print(f"    - ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{doc.page_content[:100]}...'")

    return {**state, "context": context}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    """ê²€ìƒ‰ëœ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    context = state.get("context")
    question = state.get("question")

    if not context or not question:
        print("    âŒ ì˜¤ë¥˜: ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\n    â¡ï¸ **LLMì´ ì°¸ê³ í•˜ëŠ” ì „ì²´ ë¬¸ë§¥:**")
    print("    " + "="*80)
    print(f"    {context}")
    print("    " + "="*80)
    
    print("\n    - LLMì— ë‹µë³€ ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
    chain = (
        rag_prompt
        | make_llm()
        | StrOutputParser()
    )
    
    ans = chain.invoke({"context": context, "question": question})
    print("    âœ… RAG ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ìƒì„±ëœ ë‹µë³€: '{ans[:100]}...'")
    return {**state, "answer": ans}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    """Tavilyë¥¼ ì‚¬ìš©í•´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        print("    âŒ ì˜¤ë¥˜: ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    if not TAVILY_API_KEY:
        print("    âš ï¸ ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”"}
        
    print(f"    - ì§ˆë¬¸: '{question}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ì‹œì‘...")
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question})
    search_results_str = "\n\n".join([json.dumps(r) for r in results])
    print("    âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹ .")
    print(f"    - ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    return {**state, "web_search_results": search_results_str}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results")
    
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("    âš ï¸ ê²½ê³ : ì›¹ ê²€ìƒ‰ ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ì—¬ ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
    print("    - LLMì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
    chain = (
        web_prompt
        | make_llm()
        | StrOutputParser()
    )
    
    ans = chain.invoke({"question": question, "search_results": search_results})
    print("    âœ… ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ìƒì„±ëœ ë‹µë³€: '{ans[:100]}...'")
    return {**state, "answer": ans}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    """ìµœì¢… ë‹µë³€ì„ ì¶œë ¥í•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤."""
    print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ ---")
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    print(answer)
    print("---------------------\n")
    
    # ë‹¤ìŒ ì§ˆë¬¸ì„ ë°›ê±°ë‚˜ ì¢…ë£Œë¥¼ ê²°ì •
    while True:
        next_q = input("ì§ˆë¬¸> ").strip()
        if next_q.lower() in ("exit", "quit"):
            print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ìš”ì²­. ğŸ")
            return {**state, "question": "quit"}
        if next_q:
            print(f"ğŸ’¬ ë‹¤ìŒ ì§ˆë¬¸: '{next_q}'")
            return {**state, "question": next_q}

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… ---
def route_to_web_search(state: GraphState) -> str:
    """RAG ë‹µë³€ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    print("--- ğŸ§­ ë¼ìš°í„° ì‹œì‘: ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ---")
    answer = state.get("answer")
    if answer and "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in answer:
        print("    â†ªï¸ RAG ë‹µë³€ ì‹¤íŒ¨! 'ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("    - ë‹¤ìŒ ë…¸ë“œë¡œ 'web_search'ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
        return "web_search"
    else:
        print("    ğŸ‰ RAG ë‹µë³€ ì„±ê³µ! ìœ íš¨í•œ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("    - ë‹¤ìŒ ë…¸ë“œë¡œ 'generate_answer'ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
        return "generate_answer"

def route_next_step(state: GraphState) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ ë£¨í”„ë¥¼ ê³„ì†í• ì§€ ì¢…ë£Œí• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    question = state.get("question")
    if question == "quit":
        return "end"
    else:
        return "continue"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    """LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ë° ì›¹ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì„ ì •ì˜í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""
    g = StateGraph(GraphState)
    
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    
    g.add_conditional_edges(
        "generate_rag",
        route_to_web_search,
        {"web_search": "web_search", "generate_answer": "generate_answer"}
    )
    
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "generate_answer")
    
    # generate_answerì—ì„œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    g.add_conditional_edges(
        "generate_answer",
        route_next_step,
        {"continue": "load_milvus", "end": END} # <-- ë¬´í•œ ë£¨í”„ ë° ì¢…ë£Œ ì¡°ê±´ ì¶”ê°€
    )
    
    return g.compile()

# --- ëŒ€í™” ê¸°ë¡ ëˆ„ì  ì €ì¥ í•¨ìˆ˜ ---
def append_conversation_to_file(question: str, answer: str, filename: str):
    """ì§ˆë¬¸ê³¼ ë‹µë³€ì„ JSON íŒŒì¼ì— ëˆ„ì í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    
    conversation_data = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer
    }
    
    if os.path.exists(filename) and os.path.getsize(filename) > 0:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                history: List[Dict] = json.load(f)
        except json.JSONDecodeError:
            print(f"    âš ï¸ ê²½ê³ : ê¸°ì¡´ '{filename}' íŒŒì¼ì´ ì†ìƒë˜ì–´ ìƒˆ íŒŒì¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            history = []
    else:
        history = []
    
    history.append(conversation_data)
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
        print(f"    âœ… ëŒ€í™” ê¸°ë¡ì´ '{filename}'ì— ì„±ê³µì ìœ¼ë¡œ ëˆ„ì ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"    âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch ì‹œì‘ (exit/quitë¡œ ì¢…ë£Œ)")
    
    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"
    
    app = build_graph()
    try:
        graph_image_path = "milvus_agent_workflow_llm.png"
        Path(graph_image_path).parent.mkdir(parents=True, exist_ok=True)
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(" (Mermaid API í˜¸ì¶œ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    # ì²« ì§ˆë¬¸ë§Œ ì™¸ë¶€ì—ì„œ ë°›ë„ë¡ ìˆ˜ì •
    q = input("ì§ˆë¬¸> ").strip()
    if q.lower() in ("exit", "quit"):
        print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ì´ ì¢…ë£Œë©ë‹ˆë‹¤. ğŸšª")
    else:
        final_state = app.invoke({"question": q})
        # ìµœì¢… ìƒíƒœì— 'answer'ê°€ ìˆê³ , 'question'ì´ 'quit'ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì €ì¥
        if final_state.get("question") != "quit":
            append_conversation_to_file(q, final_state.get("answer"), session_log_file)