import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.runnables.graph import MermaidDrawMethod
from pymilvus import connections

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    question: Optional[str]            # ì‚¬ìš©ì ì§ˆë¬¸
    vectorstore: Optional[Milvus]      # Milvus ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    context: Optional[str]             # Milvusì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    answer: Optional[str]              # LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€
    web_search_results: Optional[str]  # ì›¹ ê²€ìƒ‰ ê²°ê³¼
    user_decision: Optional[str]       # "yes"|"no"
    # (ì˜µì…˜) ì¶”ê°€ ë¡œê¹…
    decision_reason: Optional[str]

# --- Embeddings ë° LLM ---
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# ì‹œê°„ ë¯¼ê° í‚¤ì›Œë“œ(ìˆìœ¼ë©´ ì›¹ê²€ìƒ‰ ìš°ì„ )
TIME_SENSITIVE = re.compile(
    r"(ìµœì‹ |ì˜¤ëŠ˜|ë°©ê¸ˆ|ì§€ê¸ˆ|ì‹¤ì‹œê°„|ë³€ê²½|ì—…ë°ì´íŠ¸|ë‰´ìŠ¤|ê°€ê²©|í™˜ìœ¨|ì£¼ê°€|ì¼ì •|ìŠ¤ì¼€ì¤„|ì˜ˆë³´|ë‚ ì”¨|ëª¨ì§‘|ì±„ìš©|ì¬ê³ |íŒë§¤|ìš´í•­|ë°œí‘œ)"
)

# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        print("    - Milvus ì—°ê²°ì´ ì—†ì–´ ìƒˆë¡œ ì—°ê²°í•©ë‹ˆë‹¤.")
        # URI/TOKENì„ ì“°ëŠ” ì—°ê²°ë²•(í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
        # connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
        connections.connect(alias="default", host="localhost", port="19530")

    try:
        vs = Milvus(
            embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"host": "localhost", "port": "19530"},
        )
        print(f"    âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"    âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")

    if not question or not vectorstore:
        print("    âŒ ì˜¤ë¥˜: ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    print(f"    - ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)

    context = ""
    print(f"    âœ… {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰.")
    print("    ğŸ“„ ê²€ìƒ‰ ë¬¸ì„œ/ìœ ì‚¬ë„ ì ìˆ˜:")
    for i, (doc, score) in enumerate(docs_with_scores):
        context += f"\n\n{doc.page_content}"
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f"    - ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")

    return {**state, "context": context}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    context = state.get("context")
    question = state.get("question")

    if not context or not question:
        print("    âŒ ì˜¤ë¥˜: ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    print("\n    â¡ï¸ **LLM ì°¸ê³  ë¬¸ë§¥(ìš”ì•½ í‘œì‹œ ìƒëµ, ì „ì²´ëŠ” ë‚´ë¶€ ì‚¬ìš©)**")
    chain = (rag_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"context": context, "question": question})
    print("    âœ… RAG ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans}

def user_decision_node(state: GraphState) -> Dict[str, Any]:
    """
    âœ… ìë™ íŒë‹¨ ë…¸ë“œ
    ê·œì¹™:
      - Tavily í‚¤ê°€ ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë¶ˆê°€ â†’ 'no'
      - RAGê°€ ì‹¤íŒ¨ë¬¸êµ¬ í¬í•¨(ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” â€¦) â†’ 'yes'
      - ì§ˆë¬¸ì´ ì‹œê°„ë¯¼ê° í‚¤ì›Œë“œ í¬í•¨ â†’ 'yes'
      - ê·¸ ì™¸ â†’ 'no'
    """
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì‚¬ìš©ì ê²°ì •(ìë™) ---")
    q = state.get("question", "") or ""
    ans = state.get("answer", "") or ""

    if not TAVILY_API_KEY:
        reason = "no_tavily_api_key"
        print(f"    â†ªï¸ ì›¹ê²€ìƒ‰ ë¶ˆê°€: {reason}")
        return {**state, "user_decision": "no", "decision_reason": reason}

    rag_failed = "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in ans
    time_sensitive = bool(TIME_SENSITIVE.search(q))

    if rag_failed or time_sensitive:
        reason = "rag_failed" if rag_failed else "time_sensitive"
        print(f"    â†ªï¸ ì›¹ê²€ìƒ‰ ì§„í–‰: {reason}")
        return {**state, "user_decision": "yes", "decision_reason": reason}

    print("    â†ªï¸ ì›¹ê²€ìƒ‰ ê±´ë„ˆëœ€: context_sufficient")
    return {**state, "user_decision": "no", "decision_reason": "context_sufficient"}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        print("    âŒ ì˜¤ë¥˜: ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if not TAVILY_API_KEY:
        print("    âš ï¸ ê²½ê³ : TAVILY_API_KEY ë¯¸ì„¤ì • â†’ ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”"}

    print(f"    - '{question}' ì›¹ ê²€ìƒ‰ ì‹œì‘...")
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question})
    search_results_str = "\n\n".join([json.dumps(r, ensure_ascii=False) for r in results])
    print("    âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹ :", len(results), "ê°œ")
    return {**state, "web_search_results": search_results_str}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results")

    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("    âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ ë¶€ì¡± â†’ ì›¹ê¸°ë°˜ ë‹µë³€ ë¶ˆê°€")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    chain = (web_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"question": question, "search_results": search_results})
    print("    âœ… ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    """ìµœì¢… ë‹µë³€ë§Œ ì¶œë ¥í•˜ê³  ìë™ ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •."""
    print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ ---")
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    print(answer)
    print("---------------------\n")
    # ğŸ” ë” ì´ìƒ ì‚¬ìš©ì ì…ë ¥ ì—†ì´ ì¢…ë£Œ
    return {**state, "question": "quit"}

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… ---
def route_to_web_search(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: RAG ê²°ê³¼ ê¸°ë°˜ ë¶„ê¸° ---")
    answer = state.get("answer") or ""
    if "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in answer:
        print("    â†ªï¸ RAG ì‹¤íŒ¨ â†’ user_decision")
        return "user_decision"
    else:
        print("    ğŸ‰ RAG ì„±ê³µ â†’ generate_answer")
        return "generate_answer"

def route_user_decision(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ì‚¬ìš©ì ê²°ì •(ìë™) ì²˜ë¦¬ ---")
    decision = state.get("user_decision")
    if decision == "yes":
        print("    â†ªï¸ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ â†’ web_search")
        return "do_web_search"
    else:
        print("    â†ªï¸ ì›¹ ê²€ìƒ‰ ìƒëµ â†’ generate_answer")
        return "skip_web_search"

def route_next_step(state: GraphState) -> str:
    # ì‚¬ìš©ì ì…ë ¥ ì—†ì´ ì¦‰ì‹œ ì¢…ë£Œ
    return "end" if state.get("question") == "quit" else "continue"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)

    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("user_decision", user_decision_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")

    g.add_conditional_edges(
        "generate_rag",
        route_to_web_search,
        {"user_decision": "user_decision", "generate_answer": "generate_answer"},
    )

    g.add_conditional_edges(
        "user_decision",
        route_user_decision,
        {"do_web_search": "web_search", "skip_web_search": "generate_answer"},
    )

    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "generate_answer")

    g.add_conditional_edges(
        "generate_answer",
        route_next_step,
        {"continue": "load_milvus", "end": END},
    )
    return g.compile()

# --- ëŒ€í™” ê¸°ë¡ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, filename: str):
    conversation_data = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
    }

    if os.path.exists(filename) and os.path.getsize(filename) > 0:
        try:
            with open(filename, "r", encoding="utf-8") as f:
                history: List[Dict] = json.load(f)
        except json.JSONDecodeError:
            print(f"    âš ï¸ '{filename}' ì†ìƒ â†’ ìƒˆ íŒŒì¼ë¡œ ì‹œì‘")
            history = []
    else:
        history = []

    history.append(conversation_data)

    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
        print(f"    âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥: {filename}")
    except Exception as e:
        print(f"    âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch ì‹œì‘ (ìë™ ì§„í–‰, exit/quit í•„ìš” ì—†ìŒ)")

    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()
    try:
        graph_image_path = "milvus_agent_workflow_llm.png"
        Path(graph_image_path).parent.mkdir(parents=True, exist_ok=True)
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))
        print(f"\nâœ… LangGraph êµ¬ì¡° ì €ì¥: '{graph_image_path}'")
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì˜¤ë¥˜: {e}")
        print(" (Mermaid API í˜¸ì¶œ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    # ìµœì´ˆ ì§ˆë¬¸ë§Œ 1íšŒ ì…ë ¥(í•„ìš”ì‹œ argparse ë“±ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    q = input("ì§ˆë¬¸> ").strip()
    if not q or q.lower() in ("exit", "quit"):
        print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ì´ ì¢…ë£Œë©ë‹ˆë‹¤. ğŸšª")
    else:
        final_state = app.invoke({"question": q})
        initial_question = q
        final_answer = final_state.get("answer", "")
        append_conversation_to_file(initial_question, final_answer, session_log_file)
