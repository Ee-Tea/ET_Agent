import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime

# --- 환경 변수 로드 ---
# .env 파일에서 환경 변수를 불러와 애플리케이션에 설정합니다.
load_dotenv(find_dotenv())

# --- Milvus / Embedding 모델 설정 ---
# 벡터 데이터베이스(Milvus)와 임베딩 모델에 대한 설정을 정의합니다.
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM(Large Language Model) 설정 ---
# 답변을 생성하는 데 사용될 LLM에 대한 설정을 정의합니다.
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7")) # <-- LLM의 창의성/일관성 조절

# --- Web Search 설정 ---
# 웹 검색 도구인 Tavily의 API 키를 정의합니다.
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEY가 .env에 설정되어야 합니다.")

# --- 라이브러리 임포트 ---
# LangChain, LangGraph, Milvus 등 필요한 라이브러리를 임포트합니다.
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.runnables.graph import MermaidDrawMethod
from pymilvus import connections

# --- 프롬프트 정의 ---
# LLM에게 어떤 역할을 부여하고 어떤 형식으로 답변을 생성할지 지시하는 프롬프트를 정의합니다.
RAG_PROMPT_TMPL = """
당신은 대한민국 농업 작물 재배 방법 전문가입니다.
아래 '문맥'만 사용해 질문에 답하세요.

[문맥]
{context}

규칙:
- 문맥에 없는 정보/추측/한자 금지.
- 한글로만 작성.
- 단계/설명은 "한 문장씩 줄바꿈".
- 문맥에 근거 없으면: "주어진 정보로는 답변할 수 없습니다."

질문: {question}
답변:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
당신은 대한민국 농업 작물 재배 방법 전문가입니다.
아래 '웹 검색 결과'와 '질문'을 바탕으로 답변을 생성하세요.

[웹 검색 결과]
{search_results}

규칙:
- 검색 결과만 사용해 질문에 답하세요.
- 한글로만 작성.
- 답변이 불가능하면 "주어진 정보로는 답변할 수 없습니다."라고 작성하세요.

🟢 질문: {question}
✨ 답변:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- 상태 정의 (State Definition) ---
# LangGraph의 각 노드 간에 전달될 상태 객체의 구조를 정의합니다.
class GraphState(TypedDict):
    """LangGraph의 상태를 정의합니다."""
    question: Optional[str]              # 사용자 질문
    vectorstore: Optional[Milvus]        # Milvus 벡터스토어 객체
    context: Optional[str]               # Milvus에서 검색된 문서 내용
    answer: Optional[str]                # LLM이 생성한 최종 답변
    web_search_results: Optional[str]    # 웹 검색 결과

# --- Embeddings 및 LLM 인스턴스 생성 ---
# 임베딩 모델과 LLM 객체를 생성하는 함수입니다.
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    """ChatGroq LLM 인스턴스를 생성합니다."""
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# --- LangGraph 노드들 (Nodes) ---
# 그래프의 각 단계를 구성하는 함수들입니다.
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    """Milvus 벡터스토어를 로드하고 상태에 추가합니다."""
    print("--- 🧩 노드 시작: Milvus 벡터스토어 로드 ---")
    
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        print("    - Milvus 연결이 존재하지 않아 새로 연결을 시도합니다.")
        connections.connect(alias="default", host="localhost", port="19530")

    try:
        vs = Milvus(
            embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"host": "localhost", "port": "19530"}
        )
        print("    ✅ Milvus 벡터스토어 로드 완료. (컬렉션: {})".format(MILVUS_COLLECTION))
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"    ❌ Milvus 벡터스토어 로드 실패: {e}")
        raise ConnectionError("Milvus 벡터스토어 로드 실패")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    """질문에 대한 관련 문서를 Milvus에서 검색하고 유사도 점수를 출력합니다."""
    print("--- 🧩 노드 시작: 문서 검색 ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")

    if not question or not vectorstore:
        print("    ❌ 오류: 질문 또는 벡터스토어가 누락되었습니다.")
        raise ValueError("질문 또는 벡터스토어가 누락되었습니다.")

    print(f"    - 질문: '{question}'")
    # 유사도 점수와 함께 문서 검색
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)
    
    context = ""
    print(f"    ✅ {len(docs_with_scores)}개의 관련 문서 검색 완료.")
    print("    📄 **검색된 문서 및 유사도 점수:**")
    for i, (doc, score) in enumerate(docs_with_scores):
        context += f"\n\n{doc.page_content}"
        print(f"    - 문서 {i+1} (점수: {score:.4f}): '{doc.page_content[:100]}...'")

    return {**state, "context": context}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    """검색된 문맥과 질문을 바탕으로 LLM을 사용해 답변을 생성합니다."""
    print("--- 🧩 노드 시작: RAG 답변 생성 ---")
    context = state.get("context")
    question = state.get("question")

    if not context or not question:
        print("    ❌ 오류: 문맥 또는 질문이 누락되었습니다.")
        raise ValueError("문맥 또는 질문이 누락되었습니다.")
    
    print("\n    ➡️ **LLM이 참고하는 전체 문맥:**")
    print("    " + "="*80)
    print(f"    {context}")
    print("    " + "="*80)
    
    print("\n    - LLM에 답변 생성을 요청합니다.")
    chain = (
        rag_prompt
        | make_llm()
        | StrOutputParser()
    )
    
    ans = chain.invoke({"context": context, "question": question})
    print("    ✅ RAG 답변 생성 완료.")
    print(f"    - 생성된 답변: '{ans[:100]}...'")
    return {**state, "answer": ans}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    """Tavily를 사용해 웹 검색을 수행합니다."""
    print("--- 🧩 노드 시작: 웹 검색 ---")
    question = state.get("question")
    if not question:
        print("    ❌ 오류: 질문이 누락되었습니다.")
        raise ValueError("질문이 누락되었습니다.")
        
    if not TAVILY_API_KEY:
        print("    ⚠️ 경고: TAVILY_API_KEY가 설정되지 않아 웹 검색을 건너뜁니다.")
        return {**state, "web_search_results": "웹 검색 비활성화"}
        
    print(f"    - 질문: '{question}'에 대한 웹 검색 시작...")
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question})
    search_results_str = "\n\n".join([json.dumps(r) for r in results])
    print("    ✅ 웹 검색 결과 수신.")
    print(f"    - 검색 결과: {len(results)}개")
    return {**state, "web_search_results": search_results_str}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    """웹 검색 결과를 바탕으로 답변을 생성합니다."""
    print("--- 🧩 노드 시작: 웹 기반 답변 생성 ---")
    question = state.get("question")
    search_results = state.get("web_search_results")
    
    if not question or not search_results or search_results == "웹 검색 비활성화":
        print("    ⚠️ 경고: 웹 검색 정보가 불충분하여 답변 생성이 불가능합니다.")
        return {**state, "answer": "주어진 정보로는 답변할 수 없습니다."}
        
    print("    - LLM에 웹 검색 결과를 바탕으로 답변 생성을 요청합니다.")
    chain = (
        web_prompt
        | make_llm()
        | StrOutputParser()
    )
    
    ans = chain.invoke({"question": question, "search_results": search_results})
    print("    ✅ 웹 기반 답변 생성 완료.")
    print(f"    - 생성된 답변: '{ans[:100]}...'")
    return {**state, "answer": ans}
    
def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    """최종 답변을 출력하고 사용자에게 다음 질문을 입력받습니다. 무한 루프의 시작점 역할을 합니다."""
    print("\n--- 🤖 최종 답변 ---")
    answer = state.get("answer", "답변 생성 실패")
    print(answer)
    print("---------------------\n")
    
    # 다음 질문을 받거나 종료를 결정
    while True:
        next_q = input("질문> ").strip()
        if next_q.lower() in ("exit", "quit"):
            print("💬 파이프라인 종료 요청. 🏁")
            return {**state, "question": "quit"}
        if next_q:
            print(f"💬 다음 질문: '{next_q}'")
            return {**state, "question": next_q}

# --- 조건부 라우팅 (Conditional Edges) ---
# 노드의 출력에 따라 다음 노드를 동적으로 결정하는 함수들입니다.
def route_to_web_search(state: GraphState) -> str:
    """RAG 답변에 따라 웹 검색 또는 최종 답변 노드를 결정합니다."""
    print("--- 🧭 라우터 시작: 다음 단계 결정 ---")
    answer = state.get("answer")
    if answer and "주어진 정보로는 답변할 수 없습니다." in answer:
        print("    ↪️ RAG 답변 실패! '주어진 정보로는 답변할 수 없습니다.'가 포함되어 있습니다.")
        print("    - 다음 노드로 'web_search'를 선택합니다.")
        return "web_search"
    else:
        print("    🎉 RAG 답변 성공! 유효한 답변이 생성되었습니다.")
        print("    - 다음 노드로 'generate_answer'를 선택합니다.")
        return "generate_answer"

def route_next_step(state: GraphState) -> str:
    """사용자 입력에 따라 루프를 계속할지 종료할지 결정합니다."""
    question = state.get("question")
    if question == "quit":
        return "end"
    else:
        return "continue"

# --- 그래프 빌드 ---
# LangGraph를 사용하여 전체 파이프라인의 구조를 정의하고 컴파일합니다.
def build_graph():
    """LangGraph를 사용하여 RAG 및 웹 검색 파이프라인을 정의하고 컴파일합니다."""
    g = StateGraph(GraphState)
    
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    
    # RAG 답변 결과에 따라 웹 검색 여부를 자동으로 결정합니다.
    g.add_conditional_edges(
        "generate_rag",
        route_to_web_search,
        {"web_search": "web_search", "generate_answer": "generate_answer"}
    )
    
    # 웹 검색 후 웹 기반 답변 생성 노드로 이동합니다.
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "generate_answer")
    
    # 최종 답변 출력 후, 사용자 입력에 따라 루프를 재시작하거나 종료합니다.
    g.add_conditional_edges(
        "generate_answer",
        route_next_step,
        {"continue": "load_milvus", "end": END}
    )
    
    return g.compile()

# --- 대화 기록 저장 함수 ---
# 사용자와의 대화를 파일로 저장하는 유틸리티 함수입니다.
def append_conversation_to_file(question: str, answer: str, filename: str):
    """질문과 답변을 JSON 파일에 누적하여 저장합니다."""
    
    conversation_data = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer
    }
    
    if os.path.exists(filename) and os.path.getsize(filename) > 0:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                history: List[Dict] = json.load(f)
        except json.JSONDecodeError:
            print(f"    ⚠️ 경고: 기존 '{filename}' 파일이 손상되어 새 파일로 시작합니다.")
            history = []
    else:
        history = []
    
    history.append(conversation_data)
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
        print(f"    ✅ 대화 기록이 '{filename}'에 성공적으로 누적되었습니다.")
    except Exception as e:
        print(f"    ❌ 대화 기록 저장 중 오류 발생: {e}")

# --- 메인 실행 ---
# 파이프라인을 실행하고 사용자 입력을 처리하는 메인 함수입니다.
if __name__ == "__main__":
    print("💬 Milvus 기반 LangGraph RAG + WebSearch 시작 (exit/quit로 종료)")
    
    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"
    
    app = build_graph()
    try:
        graph_image_path = "milvus_agent_workflow_llm.png"
        Path(graph_image_path).parent.mkdir(parents=True, exist_ok=True)
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))
        print(f"\n✅ LangGraph 구조가 '{graph_image_path}' 파일로 저장되었습니다.")
    except Exception as e:
        print(f"❌ 그래프 시각화 중 오류 발생: {e}")
        print(" (Mermaid API 호출 또는 네트워크 문제일 수 있습니다.)")

    while True:
        q = input("질문> ").strip()
        if q.lower() in ("exit", "quit"):
            print("💬 파이프라인이 종료됩니다. 🚪")
            break
        if not q:
            continue
        
        try:
            final_state = app.invoke({"question": q})
            
            # 최종 답변 및 기록
            final_answer = final_state.get("answer", "답변 생성 실패")
            append_conversation_to_file(q, final_answer, session_log_file)
            
        except Exception as e:
            print(f"RAG 파이프라인 실행 중 오류: {e}")