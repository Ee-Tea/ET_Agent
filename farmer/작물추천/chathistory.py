# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
# pip install langchain-huggingface langchain_community langchain-core langchain-groq langgraph pymilvus python-dotenv tavily-python ragas datasets

import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re

# --- RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-8b-8192")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from pymilvus import connections

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€ì„ ì¢…í•©í•˜ê³  ì •ë¦¬í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ë˜, ì§ì ‘ì ì¸ ë‚´ìš©ì´ ì—†ë”ë¼ë„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœëŒ€í•œ ìœ ìš©í•œ ë‹µë³€ì„ ë§Œë“œì„¸ìš”.
- ë‚´ìš©ì€ ëª…í™•í•˜ê²Œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ê²€ìƒ‰ ê²°ê³¼ë¡œ ì •ë§ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•  ë•Œë§Œ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- [ì¤‘ìš”] ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)


# --- ğŸ”¥ ë³€ê²½: ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]
    vectorstore: Optional[Milvus]
    context: Optional[str]
    answer: Optional[str]
    web_search_results: Optional[str]
    log_file: Optional[str]
    answer_source: Optional[str]
    ragas_score: Optional[float]
    rag_retry_count: Optional[int] # RAG ì¬ì‹œë„ íšŸìˆ˜ë§Œ ì¹´ìš´íŠ¸


# --- Embeddings ë° LLM ---
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)


# --- ìœ í‹¸: ëŒ€í™” ë¡œê·¸ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, source: str, filename: str):
    sentences = re.split(r'(?<=[.!?])\s+', answer.strip())
    sentences = [s for s in sentences if s]
    data = {"timestamp": datetime.now().isoformat(), "question": question, "answer": sentences, "source": source}
    if filename:
        try:
            hist = []
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                with open(filename, "r", encoding="utf-8") as f: hist = json.load(f)
            hist.append(data)
            with open(filename, "w", encoding="utf-8") as f: json.dump(hist, f, ensure_ascii=False, indent=4)
            print(f"       ğŸ’¾ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"       âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")


# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("\n--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
    try:
        vs = Milvus(embedding_model, collection_name=MILVUS_COLLECTION, connection_args={"uri": MILVUS_URI, "token": MILVUS_TOKEN})
        print(f"       âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"       âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")
    if not question or not vectorstore: raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"       ğŸ“¥ ê²€ìƒ‰ ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=2)

    context = ""
    print(f"       ğŸ“„ {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")
    for i, (doc, score) in enumerate(docs_with_scores):
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f"         â–¶ ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")
        context += f"\n\n{doc.page_content}"
    print(f"       ğŸ“ ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ì")
    # RAG ì¬ì‹œë„ ì¹´ìš´í„°ë¥¼ ì—¬ê¸°ì„œ ì´ˆê¸°í™”
    return {**state, "context": context, "rag_retry_count": 0}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    rag_retries = state.get("rag_retry_count", 0) + 1
    print(f"       ğŸ”„ RAG ìƒì„± ì‹œë„: {rag_retries}ë²ˆì§¸")

    context = state.get("context", "")
    question = state.get("question")
    if not question: raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"       â–¶ ì…ë ¥ ì»¨í…ìŠ¤íŠ¸: '{context[:100].replace('\n', ' ')}...'")
    chain = (rag_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"context": context, "question": question})
    print(f"       ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    return {**state, "answer": ans, "answer_source": "ë‚´ë¶€ DB", "rag_retry_count": rag_retries}

def ragas_eval_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAGAS ë‹µë³€ í‰ê°€ ---")
    question = state.get("question")
    answer = state.get("answer", "")
    source = state.get("answer_source", "N/A")

    # 1. ë‹µë³€ ì†ŒìŠ¤ì— ë”°ë¼ í‰ê°€ ê¸°ì¤€ì´ ë  ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ ì„ íƒí•©ë‹ˆë‹¤.
    if source == "ì›¹ ê²€ìƒ‰":
        eval_context = state.get("web_search_results", "")
        context_source_name = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
    else: # "ë‚´ë¶€ DB" ë˜ëŠ” ê¸°íƒ€
        eval_context = state.get("context", "")
        context_source_name = "ë‚´ë¶€ DB ë¬¸ì„œ"

    print(f"       â–¶ í‰ê°€ ëŒ€ìƒ ({source}): '{answer[:100].replace('\n', ' ')}...'")
    print(f"       â–¶ í‰ê°€ ê¸°ì¤€ ({context_source_name}): '{eval_context[:100].replace('\n', ' ')}...'")
    print(f"       â–¶ ê¸°ì¤€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(eval_context)} ì")


    if not all([question, answer, eval_context]):
        print("       âš ï¸ í‰ê°€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "ragas_score": 0.0}

    dataset_dict = {"question": [question], "answer": [answer], "contexts": [[eval_context]]}
    dataset = Dataset.from_dict(dataset_dict)

    try:
        # 2. ë§¤ë²ˆ LLM ê°ì²´ë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šê³ , ì´ë¯¸ ìƒì„±ëœ ì „ì—­ llm ê°ì²´ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
        result = evaluate(
            dataset, 
            metrics=[faithfulness, answer_relevancy], 
            mllm=llm,  # ì „ì—­ llm ê°ì²´ ì‚¬ìš©
            embeddings=embedding_model, 
            raise_exceptions=False
        )
        score = result.get("ragas_score", 0.0)
        print(f"       ğŸ“Š RAGAS í‰ê°€ ì ìˆ˜: {score:.4f}")
    except Exception as e:
        print(f"       âŒ RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        score = 0.0
        
    return {**state, "ragas_score": score}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question: raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not TAVILY_API_KEY:
        print("       âš ï¸ TAVILY_API_KEYê°€ ì—†ì–´ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”"}
    
    print(f"       ğŸ” ì›¹ ê²€ìƒ‰ì–´: '{question}'")
    search_tool = TavilySearchResults(max_results=1)
    results = search_tool.invoke({"query": question})
    sr = "\n\n".join([json.dumps(r, ensure_ascii=False) for r in results])
    print(f"       ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê°œ ìˆ˜ì‹  ì™„ë£Œ.")
    return {**state, "web_search_results": sr}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results", "")
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("       âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "answer_source": "ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨"}
    
    print(f"       â–¶ ì…ë ¥ ì›¹ ì»¨í…ìŠ¤íŠ¸: '{search_results[:150].replace('\n', ' ')}...'")
    chain = (web_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"question": question, "search_results": search_results})
    print(f"       ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    return {**state, "answer": ans, "answer_source": "ì›¹ ê²€ìƒ‰"}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    source = state.get("answer_source", "ì•Œ ìˆ˜ ì—†ìŒ")
    score = state.get("ragas_score")
    score_text = f"{score:.4f}" if score is not None else "N/A"
    
    print("\n" + "="*50)
    print("ğŸ¤– ìµœ ì¢… ë‹µ ë³€")
    print("="*50)
    print(f"âœ… ë‹µë³€ ì¶œì²˜: {source} (RAGAS ì ìˆ˜: {score_text})")
    print(f"\n{answer}")
    print("="*50)

    log_file = state.get("log_file") or ""
    q_for_log = state.get("question") or ""
    append_conversation_to_file(q_for_log, answer, source, log_file)
    return state

# --- ğŸ”¥ ë³€ê²½: í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°í„° ---
# --- ë¼ìš°í„° ---
def master_router(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ê²½ë¡œ ê²°ì • ---")
    score = state.get("ragas_score", 0.0)
    source = state.get("answer_source", "")
    rag_retries = state.get("rag_retry_count", 0)
    
    SCORE_THRESHOLD = 0.7
    RAG_RETRY_LIMIT = 2
    
    if source == "ì›¹ ê²€ìƒ‰":
        print("       â¡ï¸ ê²°ì •: ì›¹ ë‹µë³€ í‰ê°€ ì™„ë£Œ. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"

    print(f"       ğŸ“Š í‰ê°€ ì ìˆ˜ (ë‚´ë¶€ DB): {score:.4f} (ì„ê³„ê°’: {SCORE_THRESHOLD})")
    print(f"       ğŸ”„ RAG ì¬ì‹œë„: {rag_retries}/{RAG_RETRY_LIMIT}")

    if score >= SCORE_THRESHOLD:
        print("       â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ í†µê³¼. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"
    else:
        if rag_retries < RAG_RETRY_LIMIT:
            # --- ğŸ”¥ ë³€ê²½: ìš”ì²­í•˜ì‹  printë¬¸ ì¶”ê°€ ---
            print(f"       â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬. {rag_retries + 1}ë²ˆì§¸ ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ëŒì•„ê°‘ë‹ˆë‹¤.")
            return "retry_rag"
        else:
            print(f"       â¡ï¸ ê²°ì •: RAG ì¬ì‹œë„ í•œë„ ë„ë‹¬. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.")
            return "augment_with_web"


# --- ğŸ”¥ ë³€ê²½: ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("ragas_eval", ragas_eval_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_edge("generate_rag", "ragas_eval")
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "ragas_eval") # ì›¹ ë‹µë³€ë„ í‰ê°€

    # í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ… ë¡œì§
    g.add_conditional_edges(
        "ragas_eval",
        master_router,
        {
            "retry_rag": "generate_rag",        # RAG ì¬ì‹œë„
            "augment_with_web": "web_search",   # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜
            "end_journey": "generate_answer"    # ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™
        }
    )
    
    g.add_edge("generate_answer", END)
    return g.compile()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch (RAGAS í‰ê°€ í¬í•¨) ì‹œì‘ (ì¢…ë£Œ: exit ë˜ëŠ” quit ì…ë ¥)")

    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()

    try:
        graph_image_path = "milvus_agent_workflow_llm_with_ragas.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   (ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” 'mermaid-cli'ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    while True:
        q = input("\n\nì§ˆë¬¸> ").strip()
        if not q or q.lower() in ("exit", "quit"):
            print("ğŸ’¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸšª")
            break

        print("-" * 60)
        print(f"ğŸš€ ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{q}'")
        app.invoke({"question": q, "log_file": session_log_file})
        print(f"ğŸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ.")
        print("-" * 60)
        
    print("\ní”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")