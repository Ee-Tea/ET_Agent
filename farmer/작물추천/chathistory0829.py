# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# pip install langchain-huggingface langchain_community langchain-core langchain-groq langgraph pymilvus python-dotenv tavily-python ragas datasets

print("â–¶ [ì´ˆê¸°í™”] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹œì‘...")
import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re
import numpy as np  # âœ… ì¶”ê°€: ì¶”ì¶œì‹ referenceìš© ìœ ì‚¬ë„ ê³„ì‚°

# --- RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
print("â–¶ [ì´ˆê¸°í™”] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ...")
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-8b-8192")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from pymilvus import connections
print("â–¶ [ì´ˆê¸°í™”] ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ìœ¼ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€ì„ ì¢…í•©í•˜ê³  ì •ë¦¬í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ë˜, ì§ì ‘ì ì¸ ë‚´ìš©ì´ ì—†ë”ë¼ë„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœëŒ€í•œ ìœ ìš©í•œ ë‹µë³€ì„ ë§Œë“œì„¸ìš”.
- ë‚´ìš©ì€ ëª…í™•í•˜ê²Œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ê²€ìƒ‰ ê²°ê³¼ë¡œ ì •ë§ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•  ë•Œë§Œ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- [ì¤‘ìš”] ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# (ì°¸ê³ ) ì´ì „ì—” LLMìœ¼ë¡œ í•©ì„± referenceë¥¼ ë§Œë“¤ì—ˆì§€ë§Œ, ì´ì œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
SYNTHETIC_REF_PROMPT = ChatPromptTemplate.from_template("""
[UNUSED] í•©ì„± reference í”„ë¡¬í”„íŠ¸ (í˜„ì¬ëŠ” LLM-free ì¶”ì¶œì‹ reference ì‚¬ìš©)
ì§ˆë¬¸: {question}
ì»¨í…ìŠ¤íŠ¸: {context}
ì •ë‹µ:
""")

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]
    vectorstore: Optional[Milvus]
    context: Optional[str]
    answer: Optional[str]
    web_search_results: Optional[str]
    log_file: Optional[str]
    answer_source: Optional[str]
    ragas_score: Optional[float]
    rag_retry_count: Optional[int]
    original_docs: List[str]
    web_contexts: List[str]
    reference: Optional[str]
    rag_tokens: Optional[Dict[str, Any]]
    web_tokens: Optional[Dict[str, Any]]
    ragas_details: Optional[Dict[str, Any]]

# --- Embeddings ë° LLM ---
print("â–¶ [ì´ˆê¸°í™”] ì„ë² ë”© ë° LLM ëª¨ë¸ ê°ì²´ ìƒì„± ì‹œì‘...")
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)
llm = ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)
print("â–¶ [ì´ˆê¸°í™”] ì„ë² ë”© ë° LLM ëª¨ë¸ ê°ì²´ ìƒì„± ì™„ë£Œ.")

# --- ìœ í‹¸: ëŒ€í™” ë¡œê·¸ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, source: str, score: Optional[float], token_usage: Optional[Dict[str, Any]], filename: str):
    sentences = re.split(r'(?<=[.!?])\s+', answer.strip())
    sentences = [s for s in sentences if s]
    data = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": sentences,
        "source": source,
        "ragas_score": score,
        "token_usage": token_usage
    }
    if filename:
        try:
            hist = []
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                with open(filename, "r", encoding="utf-8") as f: hist = json.load(f)
            hist.append(data)
            with open(filename, "w", encoding="utf-8") as f: json.dump(hist, f, ensure_ascii=False, indent=4)
            print(f"       ğŸ’¾ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"       âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

# --- ì¶”ì¶œì‹ reference ìœ í‹¸ (LLM-free) ---
def _split_sentences(text: str) -> List[str]:
    if not text:
        return []
    # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ê°œí–‰ ê¸°ì¤€ ë¶„í• 
    parts = re.split(r"(?<=[\.!?])\s+|\n+", text)
    return [p.strip() for p in parts if p and p.strip()]

def build_extractive_reference_from_contexts(contexts: List[str], question: str, embedder: HuggingFaceEmbeddings, top_k: int = 5, max_chars: int = 1800) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ ì›ë¬¸ì—ì„œ ë¬¸ì¥ë“¤ì„ ì¶”ì¶œí•˜ì—¬, ì§ˆë¬¸-ë¬¸ì¥ ì„ë² ë”© ìœ ì‚¬ë„ ìƒìœ„ top_k ë¬¸ì¥ìœ¼ë¡œ referenceë¥¼ êµ¬ì„±.
    - LLM ìƒì„±/ìš”ì•½ ì—†ìŒ (ì¶”ì¶œì‹, extractive)
    """
    sents: List[str] = []
    for c in contexts:
        sents.extend(_split_sentences(c))

    # ë¬¸ì¥ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì•ë¶€ë¶„ ì¼ë¶€ë§Œ ìƒ˜í”Œë§ (ì•ˆì •ì„±)
    if len(sents) > 2000:
        sents = sents[:2000]

    if not sents:
        return ""

    try:
        q_emb = embedder.embed_query(question)
        s_embs = embedder.embed_documents(sents)
        q = np.array(q_emb, dtype=np.float32)
        S = np.array(s_embs, dtype=np.float32)
        q_norm = q / (np.linalg.norm(q) + 1e-12)
        S_norm = S / (np.linalg.norm(S, axis=1, keepdims=True) + 1e-12)
        scores = S_norm @ q_norm  # cosine similarity
        idx = np.argsort(-scores)[:max(top_k, 1)]
        picked = [sents[i] for i in idx]
        reference = " ".join(picked)
        return reference[:max_chars]
    except Exception as e:
        print(f"       âš ï¸ ì¶”ì¶œì‹ reference ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return " ".join(contexts)[:max_chars]  # ìµœí›„ì˜ ìˆ˜ë‹¨: ì»¨í…ìŠ¤íŠ¸ ì•ë¶€ë¶„

# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("\n--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
    try:
        vs = Milvus(embedding_model, collection_name=MILVUS_COLLECTION, connection_args={"uri": MILVUS_URI, "token": MILVUS_TOKEN})
        print(f"       âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"       âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")
    if not question or not vectorstore: raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"       ğŸ“¥ ê²€ìƒ‰ ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=2)

    context = ""
    print(f"       ğŸ“„ {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")
    for i, (doc, score) in enumerate(docs_with_scores):
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f"         â–¶ ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")
        context += f"\n\n{doc.page_content}"
    print(f"       ğŸ“ ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ì")
    
    original_docs = [doc.page_content for doc, score in docs_with_scores]
    return {**state, "context": context, "rag_retry_count": 0, "original_docs": original_docs}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    rag_retries = state.get("rag_retry_count", 0) + 1
    print(f"       ğŸ”„ RAG ìƒì„± ì‹œë„: {rag_retries}ë²ˆì§¸")

    context = state.get("context", "")
    question = state.get("question")
    if not question: raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"       â–¶ ì…ë ¥ ì»¨í…ìŠ¤íŠ¸: '{context[:100].replace('\n', ' ')}...'")
    
    response = llm.invoke(rag_prompt.format(context=context, question=question))
    ans = response.content
    token_usage = response.response_metadata.get("token_usage", {})

    print(f"       ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    print(f"       ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ (RAG): prompt={token_usage.get('prompt_tokens', 0)}, completion={token_usage.get('completion_tokens', 0)}, total={token_usage.get('total_tokens', 0)}")
    
    return {**state, "answer": ans, "answer_source": "ë‚´ë¶€ DB", "rag_retry_count": rag_retries, "rag_tokens": token_usage}

def ragas_eval_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAGAS ë‹µë³€ í‰ê°€ ---")
    question = state.get("question")
    answer = state.get("answer", "")
    source = state.get("answer_source", "N/A")

    # ì»¨í…ìŠ¤íŠ¸ í™•ë³´
    if source == "ì›¹ ê²€ìƒ‰":
        eval_context = state.get("web_search_results", "")
        context_source_name = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
        contexts_for_ragas = state.get("web_contexts", [])
    else:
        eval_context = state.get("context", "")
        context_source_name = "ë‚´ë¶€ DB ë¬¸ì„œ"
        contexts_for_ragas = state.get("original_docs", [])
        if not contexts_for_ragas and eval_context:
            contexts_for_ragas = [eval_context]

    print(f"       â–¶ í‰ê°€ ëŒ€ìƒ ({source}): '{answer[:100].replace('\n', ' ')}...'")
    head = (eval_context[:100] if isinstance(eval_context, str) else str(eval_context)[:100]).replace('\n', ' ')
    print(f"       â–¶ í‰ê°€ ê¸°ì¤€ ({context_source_name}): '{head}...'")

    if not question or not answer or not contexts_for_ragas:
        print("       âš ï¸ í‰ê°€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
        state["ragas_details"] = {
            "mode": "skipped",
            "faithfulness": None,
            "answer_relevancy": None,
            "context_recall": None,
            "context_precision": None,
            "final_score": 0.0,
            "used_reference": False,
        }
        return {**state, "ragas_score": 0.0}

    # âœ… LLM-free: ì»¨í…ìŠ¤íŠ¸ ì›ë¬¸ì—ì„œ 'ì¶”ì¶œì‹ reference' ìƒì„± (ì„ë² ë”© ìœ ì‚¬ë„ top-k ë¬¸ì¥)
    print("       âœ‚ï¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œì‹ reference ìƒì„±(LLM ìƒì„± ê¸ˆì§€)...")
    reference = build_extractive_reference_from_contexts(
        contexts_for_ragas, question, embedding_model, top_k=5, max_chars=1800
    )
    used_reference = bool(reference)
    state["reference"] = reference if used_reference else None
    if not used_reference:
        print("       âš ï¸ ì¶”ì¶œì‹ reference ë¹„ì–´ìˆìŒ â†’ ì»¨í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ì„ ëŒ€ì²´ ì‚¬ìš©")
        reference = " ".join(contexts_for_ragas)[:1800]
        used_reference = bool(reference)
        state["reference"] = reference if used_reference else None

    # ë°ì´í„°ì…‹ êµ¬ì„±
    dataset_dict = {
        "question": [question],
        "answer": [answer],
        "contexts": [contexts_for_ragas],
        "reference": [reference],  # 4ì§€í‘œ í•„ìˆ˜
    }
    dataset = Dataset.from_dict(dataset_dict)

    # ìœ í‹¸
    def harmonic_mean(vals: List[Optional[float]]) -> float:
        xs = [float(v) for v in vals if v is not None and v > 0.0]
        if not xs: return 0.0
        return len(xs) / sum(1.0 / v for v in xs)

    def to_df(result):
        if hasattr(result, "to_pandas"):
            return result.to_pandas()
        import pandas as pd
        try:
            return pd.DataFrame(result)
        except Exception:
            return pd.DataFrame()

    # 4ì§€í‘œ í‰ê°€
    f = ar = cr = cp = None
    final_score = 0.0
    try:
        metrics_all = [faithfulness, answer_relevancy, context_recall, context_precision]
        result = evaluate(
            dataset,
            metrics=metrics_all,
            llm=llm,
            embeddings=embedding_model,
            raise_exceptions=False,
        )
        df = to_df(result)

        try:
            if "faithfulness" in df.columns:
                f = float(df["faithfulness"].iloc[0])
            if "answer_relevancy" in df.columns:
                ar = float(df["answer_relevancy"].iloc[0])
            if "context_recall" in df.columns:
                cr = float(df["context_recall"].iloc[0])
            if "context_precision" in df.columns:
                cp = float(df["context_precision"].iloc[0])
        except Exception as ex:
            print(f"       âš ï¸ ì ìˆ˜ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {ex}")

        final_score = harmonic_mean([f, ar, cr, cp])

        def fmt(x):
            return "N/A" if x is None else f"{x:.4f}"
        print("\n--- ğŸ“Š RAGAS í‰ê°€ ìƒì„¸ ì ìˆ˜ ---")
        print(f"    ğŸŸ¢ ì¶©ì‹¤ë„ (Faithfulness):           {fmt(f)}")
        print(f"    ğŸŸ¢ ê´€ë ¨ì„± (Answer Relevancy):       {fmt(ar)}")
        print(f"    ğŸŸ¢ ë¬¸ë§¥ ì¬í˜„ìœ¨ (Context Recall):    {fmt(cr)}")
        print(f"    ğŸŸ¢ ë¬¸ë§¥ ì •ë°€ë„ (Context Precision): {fmt(cp)}")
        print(f"        ğŸ“Š ìµœì¢… RAGAS ì¡°í™”í‰ê·  ì ìˆ˜:     {final_score:.4f}")

    except Exception as e:
        print(f"       âŒ RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        final_score = 0.0

    state["ragas_details"] = {
        "mode": "4-metrics" if all(v is not None for v in [f, ar, cr, cp]) else "partial",
        "faithfulness": f,
        "answer_relevancy": ar,
        "context_recall": cr,
        "context_precision": cp,
        "final_score": final_score,
        "used_reference": used_reference,
    }

    return {**state, "ragas_score": final_score}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not TAVILY_API_KEY:
        print("       âš ï¸ TAVILY_API_KEYê°€ ì—†ì–´ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”", "web_contexts": []}
    
    print(f"       ğŸ” ì›¹ ê²€ìƒ‰ì–´: '{question}'")
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question}) 
    
    web_contexts: List[str] = []
    for r in results:
        title = (r.get("title") or "").strip()
        content = (r.get("content") or r.get("snippet") or "").strip()
        url = (r.get("url") or "").strip()
        passage = f"{title}\n{content}\nURL: {url}".strip()
        web_contexts.append(passage)

    sr = json.dumps(results, ensure_ascii=False)
    print(f"       ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê°œ ìˆ˜ì‹  ì™„ë£Œ.")
    return {**state, "web_search_results": sr, "web_contexts": web_contexts}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results", "")
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("       âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "answer_source": "ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨"}
    
    print(f"       â–¶ ì…ë ¥ ì›¹ ì»¨í…ìŠ¤íŠ¸: '{search_results[:150].replace('\n', ' ')}...'")
    
    response = llm.invoke(web_prompt.format(question=question, search_results=search_results))
    ans = response.content
    token_usage = response.response_metadata.get("token_usage", {})
    
    print(f"       ğŸ’¬ ìƒì„±ëœ ë‹µë³€: '{ans[:100].replace('\n', ' ')}...'")
    print(f"       ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ (Web): prompt={token_usage.get('prompt_tokens', 0)}, completion={token_usage.get('completion_tokens', 0)}, total={token_usage.get('total_tokens', 0)}")
    
    return {**state, "answer": ans, "answer_source": "ì›¹ ê²€ìƒ‰", "web_tokens": token_usage}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    source = state.get("answer_source", "ì•Œ ìˆ˜ ì—†ìŒ")
    score = state.get("ragas_score")
    details = state.get("ragas_details", {})
    score_text = f"{score:.4f}" if score is not None else "N/A"
    
    token_usage = state.get("rag_tokens") or state.get("web_tokens")

    print("\n" + "="*50)
    print("ğŸ¤– ìµœ ì¢… ë‹µ ë³€")
    print("="*50)
    print(f"âœ… ë‹µë³€ ì¶œì²˜: {source} (RAGAS ì ìˆ˜: {score_text})")
    print(f"ğŸ”§ í‰ê°€ëª¨ë“œ: {details.get('mode','N/A')}   |   reference ì‚¬ìš©: {details.get('used_reference')}")
    print("ğŸ“Š ì„¸ë¶€ ì§€í‘œ:")
    def fmt(x): return "N/A" if x is None else f"{x:.4f}"
    print(f"   - Faithfulness:         {fmt(details.get('faithfulness'))}")
    print(f"   - Answer Relevancy:     {fmt(details.get('answer_relevancy'))}")
    print(f"   - Context Recall:       {fmt(details.get('context_recall'))}")
    print(f"   - Context Precision:    {fmt(details.get('context_precision'))}")
    print(f"   - Final (harmonic/avg): {fmt(details.get('final_score'))}")
    if token_usage:
        print("\nğŸ“ˆ ì´ í„´ì˜ í† í° ì‚¬ìš©ëŸ‰:")
        print(f"   - í”„ë¡¬í”„íŠ¸: {token_usage.get('prompt_tokens', 0)}")
        print(f"   - ì‘ë‹µ:     {token_usage.get('completion_tokens', 0)}")
        print(f"   - ì´ëŸ‰:     {token_usage.get('total_tokens', 0)}")
    print("\n" + answer)
    print("="*50)

    log_file = state.get("log_file") or ""
    q_for_log = state.get("question") or ""
    append_conversation_to_file(q_for_log, answer, source, score, token_usage, log_file)
    return state

# --- ë¼ìš°í„° ---
def master_router(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ê²½ë¡œ ê²°ì • ---")
    score = state.get("ragas_score", 0.0)
    source = state.get("answer_source", "")
    rag_retries = state.get("rag_retry_count", 0)
    
    SCORE_THRESHOLD = 0.7
    RAG_RETRY_LIMIT = 2
    
    if source == "ì›¹ ê²€ìƒ‰":
        print("       â¡ï¸ ê²°ì •: ì›¹ ë‹µë³€ í‰ê°€ ì™„ë£Œ. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"

    print(f"       ğŸ“Š í‰ê°€ ì ìˆ˜ (ë‚´ë¶€ DB): {score:.4f} (ì„ê³„ê°’: {SCORE_THRESHOLD})")
    print(f"       ğŸ”„ RAG ì¬ì‹œë„: {rag_retries}/{RAG_RETRY_LIMIT}")

    if score >= SCORE_THRESHOLD:
        print("       â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ í†µê³¼. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "end_journey"
    else:
        if rag_retries < RAG_RETRY_LIMIT:
            print(f"       â¡ï¸ ê²°ì •: RAG ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬. {rag_retries + 1}ë²ˆì§¸ ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ëŒì•„ê°‘ë‹ˆë‹¤.")
            return "retry_rag"
        else:
            print(f"       â¡ï¸ ê²°ì •: RAG ì¬ì‹œë„ í•œë„ ë„ë‹¬. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.")
            return "augment_with_web"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    print("â–¶ [ê·¸ë˜í”„ ì„¤ì •] LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘...")
    g = StateGraph(GraphState)
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("ragas_eval", ragas_eval_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_edge("generate_rag", "ragas_eval")
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "ragas_eval")

    g.add_conditional_edges(
        "ragas_eval",
        master_router,
        {
            "retry_rag": "generate_rag",
            "augment_with_web": "web_search",
            "end_journey": "generate_answer"
        }
    )
    
    g.add_edge("generate_answer", END)
    print("â–¶ [ê·¸ë˜í”„ ì„¤ì •] LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì™„ë£Œ.")
    return g.compile()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch (RAGAS í‰ê°€ í¬í•¨) ì‹œì‘ (ì¢…ë£Œ: exit ë˜ëŠ” quit ì…ë ¥)")
    
    log_dir = "milvusdb_crop65llm_logs"
    print(f"â–¶ [ë©”ì¸ ì‹¤í–‰] ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±: {log_dir}")
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_overall_tokens = 0

    print("â–¶ [ë©”ì¸ ì‹¤í–‰] ê·¸ë˜í”„ ì‹œê°í™” ì‹œë„...")
    graph_image_path = "milvus_agent_workflow_llm.png"
    if os.path.exists(graph_image_path):
        print(f"\nâ„¹ï¸ LangGraph ì‹œê°í™” ì´ë¯¸ì§€ '{graph_image_path}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒëµí•©ë‹ˆë‹¤.")
    else:
        try:
            with open(graph_image_path, "wb") as f:
                f.write(app.get_graph().draw_mermaid_png())
            print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  (ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” 'mermaid-cli'ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    print("\nâ–¶ [ë©”ì¸ ì‹¤í–‰] ëŒ€í™” ë£¨í”„ ì‹œì‘. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    while True:
        q = input("\n\nì§ˆë¬¸> ").strip()
        if not q or q.lower() in ("exit", "quit"):
            print("ğŸ’¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸšª")
            print("\n" + "="*50)
            print("âœ¨ ì„¸ì…˜ ì¢…ë£Œ: ì´ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ âœ¨")
            print("="*50)
            print(f"ì´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt_tokens}")
            print(f"ì´ ì™„ë£Œ í† í°:     {total_completion_tokens}")
            print(f"ì´ ì‚¬ìš© í† í°:     {total_overall_tokens}")
            print("="*50)
            break

        print("-" * 60)
        print(f"ğŸš€ ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{q}'")
        final_state = app.invoke({"question": q, "log_file": session_log_file})
        
        tokens_this_turn = final_state.get("rag_tokens") or final_state.get("web_tokens")
        if tokens_this_turn:
            total_prompt_tokens += tokens_this_turn.get("prompt_tokens", 0)
            total_completion_tokens += tokens_this_turn.get("completion_tokens", 0)
            total_overall_tokens += tokens_this_turn.get("total_tokens", 0)
            
        print(f"ğŸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ.")
        print("-" * 60)
        
    print("\ní”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
