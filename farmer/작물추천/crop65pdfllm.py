# run_graph.py
import os
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from langchain_core.runnables.graph import MermaidDrawMethod 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
load_dotenv(find_dotenv()) 

# Ïã§Ìñâ Ï§ëÏù∏ .py ÌååÏùºÏù¥ ÏûàÎäî Ìè¥Îçî
BASE_DIR = Path(__file__).resolve().parent  

# ÏÉÅÎåÄÍ≤ΩÎ°úÎ°ú Î≤°ÌÑ∞DB ÏßÄÏ†ï
# 1) Í≤ΩÎ°ú ÏßÄÏ†ï (forward slash)
VECTOR_DB_PATH = Path("faiss_pdf_db")

print("CWD:", Path.cwd())
print("VECTOR_DB_PATH (relative):", VECTOR_DB_PATH.as_posix())
print("index.faiss Ï°¥Ïû¨:", (VECTOR_DB_PATH / "index.faiss").exists())
print("index.pkl   Ï°¥Ïû¨:", (VECTOR_DB_PATH / "index.pkl").exists())

# 2) ÏûÑÎ≤†Îî© + Î°úÎìú
EMBED_MODEL_NAME = "jhgan/ko-sroberta-multitask"
embeddings = HuggingFaceEmbeddings(model_name=os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask"))

vectorstore = FAISS.load_local(
    VECTOR_DB_PATH.as_posix(),
    embeddings,
    allow_dangerous_deserialization=True,
)
print("‚úÖ FAISS Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú ÏôÑÎ£å")

# === ÏÑ§Ï†ï ===
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYÍ∞Ä .envÏóê ÏÑ§Ï†ïÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.")

# === LangChain / LangGraph ===
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END

# --- ÌîÑÎ°¨ÌîÑÌä∏ ---
PROMPT_TMPL = """
ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ ÎÜçÏóÖ ÏûëÎ¨º Ïû¨Î∞∞ Î∞©Î≤ï Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
ÏïÑÎûò 'Î¨∏Îß•'Îßå ÏÇ¨Ïö©Ìï¥ ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.

[Î¨∏Îß•]
{context}

Í∑úÏπô:
- Î¨∏Îß•Ïóê ÏóÜÎäî Ï†ïÎ≥¥/Ï∂îÏ∏°/ÌïúÏûê Í∏àÏßÄ.
- ÌïúÍ∏ÄÎ°úÎßå ÏûëÏÑ±.
- Îã®Í≥Ñ/ÏÑ§Î™ÖÏùÄ "Ìïú Î¨∏Ïû•Ïî© Ï§ÑÎ∞îÍøà".
- Î¨∏Îß•Ïóê Í∑ºÍ±∞ ÏóÜÏúºÎ©¥: "Ï£ºÏñ¥ÏßÑ Ï†ïÎ≥¥Î°úÎäî ÎãµÎ≥ÄÌï† Ïàò ÏóÜÏäµÎãàÎã§."

ÏßàÎ¨∏: {question}
ÎãµÎ≥Ä:
"""
rag_prompt = ChatPromptTemplate.from_template(PROMPT_TMPL)

# --- ÏÉÅÌÉú Ï†ïÏùò ---
class GraphState(TypedDict):
    question: Optional[str]
    vectorstore: Optional[Any]
    context: Optional[str]
    answer: Optional[str]

# --- Í≥µÌÜµ Ìï®Ïàò ---
def load_vectorstore(db_path: str) -> Any:
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)

def retrieve(vs: Any, question: str, k: int = 5) -> str:
    retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": k})
    docs = retriever.invoke(question)
    # ÏõêÌïòÎ©¥ Ï∂úÏ≤ò ÌëúÏãú: "\n\n".join([f"(p{d.metadata.get('page')}:{d.metadata.get('source')}) {d.page_content}" for d in docs])
    return "\n\n".join([d.page_content for d in docs])

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# --- LangGraph ÎÖ∏Îìú ---
def load_vs_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú")
    vs = load_vectorstore(VECTOR_DB_PATH)
    return {**state, "vectorstore": vs}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Í≤ÄÏÉâ")
    if not state.get("vectorstore"):
        raise ValueError("vectorstoreÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    q = state["question"] or ""
    ctx = retrieve(state["vectorstore"], q, k=5)
    return {**state, "context": ctx}

def generate_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: ÏÉùÏÑ±")
    if not state.get("context") or not state.get("question"):
        raise ValueError("context/question ÎàÑÎùΩ")
    chain = (
        {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
        | rag_prompt
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"]})
    return {**state, "answer": ans}

# --- Í∑∏ÎûòÌîÑ ÎπåÎìú ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_vs", load_vs_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate", generate_node)

    g.add_edge("load_vs", "retrieve")
    g.add_edge("retrieve", "generate")
    g.add_edge("generate", END)

    g.set_entry_point("load_vs")
    return g.compile()

if __name__ == "__main__":
    print("üí¨ LangGraph RAG ÏãúÏûë (exit/quit Ï¢ÖÎ£å)")
    app = build_graph()

    # ‚îÄ‚îÄ Í∑∏ÎûòÌîÑ ÏãúÍ∞ÅÌôî ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        graph_image_path = BASE_DIR / "agent_workflow_llm.png"
        png_bytes = app.get_graph().draw_mermaid_png(
            # Í∏∞Î≥∏Í∞íÏùÄ Mermaid.ink API ÏÇ¨Ïö©. Ïò§ÌîÑÎùºÏù∏/Î∞©ÌôîÎ≤Ω ÌôòÍ≤ΩÏù¥Î©¥ PYPPETEERÍ∞Ä Îçî ÏïàÏ†Ñ.
            draw_method=MermaidDrawMethod.API
            # draw_method=MermaidDrawMethod.PYPPETEER,  # pyppeteer ÏÑ§Ïπò Ïãú ÎåÄÏïà
        )
        with open(graph_image_path, "wb") as f:
            f.write(png_bytes)
        print(f"\n‚úÖ LangGraph Íµ¨Ï°∞Í∞Ä '{graph_image_path}' ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
    except Exception as e:
        # Ïã§Ìå® Ïãú: ASCII Îã§Ïù¥Ïñ¥Í∑∏Îû® Ï∂úÎ†• + Mermaid ÏÜåÏä§ .mmdÎ°ú Ï†ÄÏû• (Î∞±ÏóÖ)
        print(f"‚ö†Ô∏è Í∑∏ÎûòÌîÑ ÏãúÍ∞ÅÌôî Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        try:
            ascii_map = app.get_graph().draw_ascii()
            print("\n[ASCII Graph]")
            print(ascii_map)
            mermaid_src = app.get_graph().draw_mermaid()
            mmd_path = BASE_DIR / "agent_workflow.mmd"
            with open(mmd_path, "w", encoding="utf-8") as f:
                f.write(mermaid_src)
            print(f"üìù Mermaid ÏÜåÏä§Î•º '{mmd_path}'Î°ú Ï†ÄÏû•ÌñàÏäµÎãàÎã§. (mermaid.live Îì±ÏóêÏÑú Î†åÎçî Í∞ÄÎä•)")
        except Exception as e2:
            print(f"Ï∂îÍ∞Ä Î∞±ÏóÖÎèÑ Ïã§Ìå®: {e2}")
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    while True:
        q = input("ÏßàÎ¨∏> ").strip()
        if q.lower() in ("exit", "quit"):
            break
        if not q:
            continue
        try:
            final_state = app.invoke({"question": q})
            print("\n--- ÎãµÎ≥Ä ---")
            print(final_state["answer"])
            print("------------\n")
        except Exception as e:
            print(f"‚ùå Ïò§Î•ò: {e}\n")