import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from langchain_core.runnables.graph import MermaidDrawMethod 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
# --- ì›¹ ê²€ìƒ‰ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
# --- ì±„íŒ… ê¸°ë¡ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
# ----------------------------------------
load_dotenv(find_dotenv()) 

# ì‹¤í–‰ ì¤‘ì¸ .py íŒŒì¼ì´ ìˆëŠ” í´ë”
BASE_DIR = Path(__file__).resolve().parent

# ìƒëŒ€ê²½ë¡œë¡œ ë²¡í„°DB ì§€ì •
# 1) ê²½ë¡œ ì§€ì • (forward slash)
VECTOR_DB_PATH = Path("faiss_pdf_db")
CHAT_HISTORY_PATH = Path("chat_history.json") # ëŒ€í™” ê¸°ë¡ íŒŒì¼ ê²½ë¡œ ì¶”ê°€

print("CWD:", Path.cwd())
print("VECTOR_DB_PATH (relative):", VECTOR_DB_PATH.as_posix())
print("index.faiss ì¡´ì¬:", (VECTOR_DB_PATH / "index.faiss").exists())
print("index.pkl ì¡´ì¬:", (VECTOR_DB_PATH / "index.pkl").exists())

# 2) ì„ë² ë”© + ë¡œë“œ
EMBED_MODEL_NAME = "jhgan/ko-sroberta-multitask"
embeddings = HuggingFaceEmbeddings(model_name=os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask"))

try:
    vectorstore = FAISS.load_local(
        VECTOR_DB_PATH.as_posix(),
        embeddings,
        allow_dangerous_deserialization=True,
    )
    print("âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    vectorstore = None

# === ì„¤ì • ===
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
# --- Tavily API í‚¤ ì„¤ì • ---
os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
if not os.getenv("TAVILY_API_KEY"):
    raise ValueError("TAVILY_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# === LangChain / LangGraph ===

# --- í”„ë¡¬í”„íŠ¸ (ì±„íŒ… ê¸°ë¡ ì¶”ê°€) ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ê³¼ 'ëŒ€í™” ê¸°ë¡'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_SEARCH_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ê´€ë ¨ ì§€ì‹ê³¼ ì¼ë°˜ ìƒì‹ì— í•´ë°•í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ëŒ€í™” ê¸°ë¡'ê³¼ 'ê²€ìƒ‰ ê²°ê³¼'ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì§ˆë¬¸'ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

[ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
web_search_prompt = PromptTemplate.from_template(WEB_SEARCH_PROMPT_TMPL)

QUESTION_TYPE_PROMPT_TMPL = """
ì£¼ì–´ì§„ ì§ˆë¬¸ì´ "ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²•"ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ë©´ 'true'ë¥¼, ê·¸ ì™¸ì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë©´ 'false'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
'true' ë˜ëŠ” 'false' ë‘ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
question_type_prompt = ChatPromptTemplate.from_template(QUESTION_TYPE_PROMPT_TMPL)
# -----------------------------

# --- ìƒíƒœ ì •ì˜ (chat_history ì¶”ê°€) ---
class GraphState(TypedDict):
    question: Optional[str]
    chat_history: List[BaseMessage]
    vectorstore: Optional[Any]
    context: Optional[str]
    answer: Optional[str]
    is_context_sufficient: Optional[bool]
    is_crop_question: Optional[bool]

# --- ê³µí†µ í•¨ìˆ˜ ---
def load_vectorstore(db_path: str) -> Any:
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    try:
        return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def retrieve(vs: Any, question: str, k: int = 5) -> str:
    retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": k})
    docs = retriever.invoke(question)
    return "\n\n".join([d.page_content for d in docs])

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# --- ì±„íŒ… ê¸°ë¡ì„ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ ---
def format_chat_history(history: List[BaseMessage]) -> str:
    formatted_history = ""
    for message in history:
        if isinstance(message, HumanMessage):
            formatted_history += f"ì‚¬ìš©ì: {message.content}\n"
        elif isinstance(message, AIMessage):
            formatted_history += f"AI: {message.content}\n"
    return formatted_history

# --- LangGraph ë…¸ë“œ ---
def load_vs_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ")
    vs = load_vectorstore(VECTOR_DB_PATH)
    return {**state, "vectorstore": vs}

def is_crop_question_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    
    formatted_chat_history = format_chat_history(chat_history)
    
    chain = question_type_prompt | make_llm() | StrOutputParser()
    try:
        result = chain.invoke({"question": question, "chat_history": formatted_chat_history}).strip().lower()
        is_crop_q = result == "true"
        print(f"LLMì´ íŒë‹¨í•œ ì§ˆë¬¸ ìœ í˜•: {'ì‘ë¬¼ ê´€ë ¨' if is_crop_q else 'ì¼ë°˜ ì§ˆë¬¸'}")
    except Exception as e:
        print(f"âŒ LLM ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’(ì¼ë°˜ ì§ˆë¬¸)ìœ¼ë¡œ ì„¤ì •: {e}")
        is_crop_q = False
        
    return {**state, "is_crop_question": is_crop_q}

def retrieve_and_generate_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±")
    vs = state.get("vectorstore")
    if not vs:
        return {**state, "answer": "ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    q = state["question"] or ""
    chat_history = state.get("chat_history", [])
    
    formatted_chat_history = format_chat_history(chat_history)
    
    # ì—¬ê¸°ì„œ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í™•ì¥í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: "ê·¸ê±°" -> "ê°€ì§€"ë¡œ ë³€í™˜
    # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ retrieverì— ë˜ì§‘ë‹ˆë‹¤.
    retrieval_query = formatted_chat_history + "\n" + q if formatted_chat_history else q
    ctx = retrieve(vs, retrieval_query, k=5)
    
    chain = (
        {"context": RunnablePassthrough(), "question": RunnablePassthrough(), "chat_history": RunnablePassthrough()}
        | rag_prompt
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": ctx, "question": q, "chat_history": formatted_chat_history})
    
    return {**state, "context": ctx, "answer": ans}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì›¹ ê²€ìƒ‰")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    tavily_tool = TavilySearchResults(max_results=5)
    
    formatted_chat_history = format_chat_history(chat_history)
    
    print("ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
    try:
        search_results = tavily_tool.invoke({"query": question})
        search_results_str = "\n".join([str(res) for res in search_results])
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {**state, "answer": "ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
    
    print("ğŸ’¡ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    chain = (
        {"search_results": RunnablePassthrough(), "question": RunnablePassthrough(), "chat_history": RunnablePassthrough()}
        | web_search_prompt
        | make_llm()
        | StrOutputParser()
    )
    web_answer = chain.invoke({"search_results": search_results_str, "question": question, "chat_history": formatted_chat_history})
    
    return {**state, "answer": web_answer}

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜ ---
def decide_route_from_type(state: GraphState) -> str:
    print("ğŸ§© ë…¸ë“œ: ê²½ë¡œ ê²°ì • (ì§ˆë¬¸ ìœ í˜•)")
    is_crop_question = state.get("is_crop_question")
    
    if is_crop_question:
        print("ê²°ì •: ì‘ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì´ë¯€ë¡œ ë‚´ë¶€ DB ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        return "retrieve_and_generate"
    else:
        print("ê²°ì •: ì¼ë°˜ ì§ˆë¬¸ì´ë¯€ë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return "web_search"

# --- ìˆ˜ì •ëœ decide_route_from_retrieve í•¨ìˆ˜ ---
def decide_route_from_retrieve(state: GraphState) -> str:
    print("ğŸ§© ë…¸ë“œ: ê²½ë¡œ ê²°ì • (DB ê²€ìƒ‰ ê²°ê³¼)")
    answer = state.get("answer")
    
    if "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." not in answer:
        print("ê²°ì •: ë‚´ë¶€ DB ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í–ˆìœ¼ë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return "end"
    else:
        print("ê²°ì •: ë‚´ë¶€ DBì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë¯€ë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return "web_search"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)
    
    # ë…¸ë“œ ì¶”ê°€
    g.add_node("load_vs", load_vs_node)
    g.add_node("is_crop_question", is_crop_question_node)
    g.add_node("retrieve_and_generate", retrieve_and_generate_node)
    g.add_node("web_search", web_search_node)

    # ì—£ì§€ ì—°ê²°
    g.set_entry_point("load_vs")
    
    g.add_edge("load_vs", "is_crop_question")
    
    g.add_conditional_edges(
        "is_crop_question",
        decide_route_from_type,
        {
            "retrieve_and_generate": "retrieve_and_generate",
            "web_search": "web_search"
        }
    )
    
    g.add_conditional_edges(
        "retrieve_and_generate",
        decide_route_from_retrieve,
        {
            "end": END,
            "web_search": "web_search"
        }
    )
    
    g.add_edge("web_search", END)

    return g.compile()

# --- JSON íŒŒì¼ ê´€ë ¨ í•¨ìˆ˜ ---
def load_chat_history() -> List[BaseMessage]:
    """JSON íŒŒì¼ì—ì„œ ì±„íŒ… ê¸°ë¡ì„ ë¶ˆëŸ¬ì™€ LangChain ë©”ì‹œì§€ ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if CHAT_HISTORY_PATH.exists():
        try:
            with open(CHAT_HISTORY_PATH, "r", encoding="utf-8") as f:
                history_data = json.load(f)
            
            chat_history = []
            for item in history_data:
                if item["role"] == "user":
                    chat_history.append(HumanMessage(content=item["content"]))
                elif item["role"] == "assistant":
                    chat_history.append(AIMessage(content=item["content"]))
            print(f"âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ({len(chat_history)}ê°œ)ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return chat_history
        except (json.JSONDecodeError, FileNotFoundError) as e:
            print(f"âŒ ëŒ€í™” ê¸°ë¡ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    return []

def save_chat_history(history: List[BaseMessage]):
    """LangChain ë©”ì‹œì§€ ê°ì²´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        history_data = []
        for message in history:
            if isinstance(message, HumanMessage):
                history_data.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                history_data.append({"role": "assistant", "content": message.content})
        
        with open(CHAT_HISTORY_PATH, "w", encoding="utf-8") as f:
            json.dump(history_data, f, ensure_ascii=False, indent=4)
        print("âœ… ëŒ€í™” ê¸°ë¡ì„ 'chat_history.json' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("ğŸ’¬ LangGraph RAG + WebSearch ì‹œì‘ (exit/quit ì¢…ë£Œ)")
    app = build_graph()

    # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    chat_history: List[BaseMessage] = load_chat_history()

    # â”€â”€ ê·¸ë˜í”„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        graph_image_path = BASE_DIR / "agent_workflow_llm.png"
        png_bytes = app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API
        )
        with open(graph_image_path, "wb") as f:
            f.write(png_bytes)
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            ascii_map = app.get_graph().draw_ascii()
            print("\n[ASCII Graph]")
            print(ascii_map)
            mermaid_src = app.get_graph().draw_mermaid()
            mmd_path = BASE_DIR / "agent_workflow.mmd"
            with open(mmd_path, "w", encoding="utf-8") as f:
                f.write(mermaid_src)
            print(f"ğŸ“ Mermaid ì†ŒìŠ¤ë¥¼ '{mmd_path}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (mermaid.live ë“±ì—ì„œ ë Œë” ê°€ëŠ¥)")
        except Exception as e2:
            print(f"ì¶”ê°€ ë°±ì—…ë„ ì‹¤íŒ¨: {e2}")
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    while True:
        q = input("ì§ˆë¬¸> ").strip()
        if q.lower() in ("exit", "quit"):
            save_chat_history(chat_history)
            break
        if not q:
            continue
        try:
            # ì§ˆë¬¸ê³¼ ì±„íŒ… ê¸°ë¡ì„ í•¨ê»˜ ì „ë‹¬
            final_state = app.invoke({"question": q, "chat_history": chat_history})
            answer = final_state["answer"]
            
            print("\n--- ë‹µë³€ ---")
            print(answer)
            print("------------\n")
            
            # í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
            chat_history.append(HumanMessage(content=q))
            chat_history.append(AIMessage(content=answer))
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}\n")