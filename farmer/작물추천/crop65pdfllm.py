# run_graph.py
import os
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv()) 

# === ÏÑ§Ï†ï ===
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_pdf_db")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.8"))

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYÍ∞Ä .envÏóê ÏÑ§Ï†ïÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.")

# === LangChain / LangGraph ===
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END

# --- ÌîÑÎ°¨ÌîÑÌä∏ ---
PROMPT_TMPL = """
ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ ÎÜçÏóÖ ÏûëÎ¨º Ïû¨Î∞∞ Î∞©Î≤ï Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
ÏïÑÎûò 'Î¨∏Îß•'Îßå ÏÇ¨Ïö©Ìï¥ ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.

[Î¨∏Îß•]
{context}

Í∑úÏπô:
- Î¨∏Îß•Ïóê ÏóÜÎäî Ï†ïÎ≥¥/Ï∂îÏ∏°/ÌïúÏûê Í∏àÏßÄ.
- ÌïúÍ∏ÄÎ°úÎßå ÏûëÏÑ±.
- Îã®Í≥Ñ/ÏÑ§Î™ÖÏùÄ "Ìïú Î¨∏Ïû•Ïî© Ï§ÑÎ∞îÍøà".
- Î¨∏Îß•Ïóê Í∑ºÍ±∞ ÏóÜÏúºÎ©¥: "Ï£ºÏñ¥ÏßÑ Ï†ïÎ≥¥Î°úÎäî ÎãµÎ≥ÄÌï† Ïàò ÏóÜÏäµÎãàÎã§."

ÏßàÎ¨∏: {question}
ÎãµÎ≥Ä:
"""
rag_prompt = ChatPromptTemplate.from_template(PROMPT_TMPL)

# --- ÏÉÅÌÉú Ï†ïÏùò ---
class GraphState(TypedDict):
    question: Optional[str]
    vectorstore: Optional[Any]
    context: Optional[str]
    answer: Optional[str]

# --- Í≥µÌÜµ Ìï®Ïàò ---
def load_vectorstore(db_path: str) -> Any:
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)

def retrieve(vs: Any, question: str, k: int = 5) -> str:
    retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": k})
    docs = retriever.invoke(question)
    # ÏõêÌïòÎ©¥ Ï∂úÏ≤ò ÌëúÏãú: "\n\n".join([f"(p{d.metadata.get('page')}:{d.metadata.get('source')}) {d.page_content}" for d in docs])
    return "\n\n".join([d.page_content for d in docs])

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# --- LangGraph ÎÖ∏Îìú ---
def load_vs_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú")
    vs = load_vectorstore(VECTOR_DB_PATH)
    return {**state, "vectorstore": vs}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Í≤ÄÏÉâ")
    if not state.get("vectorstore"):
        raise ValueError("vectorstoreÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    q = state["question"] or ""
    ctx = retrieve(state["vectorstore"], q, k=5)
    return {**state, "context": ctx}

def generate_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: ÏÉùÏÑ±")
    if not state.get("context") or not state.get("question"):
        raise ValueError("context/question ÎàÑÎùΩ")
    chain = (
        {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
        | rag_prompt
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"]})
    return {**state, "answer": ans}

# --- Í∑∏ÎûòÌîÑ ÎπåÎìú ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_vs", load_vs_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate", generate_node)

    g.add_edge("load_vs", "retrieve")
    g.add_edge("retrieve", "generate")
    g.add_edge("generate", END)

    g.set_entry_point("load_vs")
    return g.compile()

def run(state: dict) -> dict:
    """
    Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞ÏóêÏÑú Ìò∏Ï∂ú Í∞ÄÎä•Ìïú entrypoint Ìï®Ïàò.
    ÏûÖÎ†•: {"query": ÏßàÎ¨∏}
    Î∞òÌôò: {"pred_answer": ÎãµÎ≥Ä}
    """
    app = build_graph()
    question = state.get("query", "")
    if not question:
        return {"pred_answer": "ÏßàÎ¨∏Ïù¥ ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§."}
    try:
        result = app.invoke({"question": question})
        answer = result.get("answer", "ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®")
        return {"pred_answer": answer}
    except Exception as e:
        return {"pred_answer": f"crop65pdfllm Ïã§Ìñâ Ï§ë Ïò§Î•ò: {e}"}

if __name__ == "__main__":
    print("üí¨ LangGraph RAG ÏãúÏûë (exit/quit Ï¢ÖÎ£å)")
    app = build_graph()
    while True:
        q = input("ÏßàÎ¨∏> ").strip()
        if q.lower() in ("exit", "quit"):
            break
        if not q:
            continue
        try:
            final_state = app.invoke({"question": q})
            print("\n--- ÎãµÎ≥Ä ---")
            print(final_state["answer"])
            print("------------\n")
        except Exception as e:
            print(f"‚ùå Ïò§Î•ò: {e}\n")
