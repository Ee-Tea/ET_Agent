import os
from glob import glob
from typing import List, Any

from dotenv import load_dotenv
load_dotenv()

# === ì„¤ì • ===
PDF_DIR = os.getenv("PDF_DIR", "./cropinfo")
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_pdf_db")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

# === LangChain ===
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# === ì‹ ê·œ/ë³€ê²½ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìœ í‹¸ ===
import json
import hashlib
from pathlib import Path

MANIFEST_FILE = "manifest.json"

def _abs(p: str) -> str:
    return str(Path(p).resolve())

def _sha256_file(path: str, buf_size: int = 1 << 20) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            b = f.read(buf_size)
            if not b:
                break
            h.update(b)
    return h.hexdigest()

def _manifest_path(db_path: str) -> str:
    return os.path.join(db_path, MANIFEST_FILE)

def _load_manifest(db_path: str) -> dict:
    try:
        with open(_manifest_path(db_path), "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except Exception:
        return {}

def _save_manifest(db_path: str, manifest: dict) -> None:
    os.makedirs(db_path, exist_ok=True)
    with open(_manifest_path(db_path), "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)

# === ë‹¨ê³„ë³„ í•¨ìˆ˜ ===
def load_all_pdfs(pdf_dir: str) -> List[Any]:
    print("ğŸ“¥ [1/4] PDF ë¡œë“œ ë‹¨ê³„ ì‹œì‘")
    paths = sorted(glob(os.path.join(pdf_dir, "*.pdf")))
    if not paths:
        raise FileNotFoundError(f"PDFê°€ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")
    print(f"ğŸ“‚ ì´ íŒŒì¼ ìˆ˜: {len(paths)}")

    manifest = _load_manifest(VECTOR_DB_PATH)  # { abs_path: {"sha256": "..."} }
    all_docs = []
    skipped, to_process = 0, 0

    for idx, p in enumerate(paths, start=1):
        abs_path = _abs(p)
        try:
            file_hash = _sha256_file(abs_path)

            # ë³€ê²½ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if manifest.get(abs_path, {}).get("sha256") == file_hash:
                skipped += 1
                print(f"[{idx}/{len(paths)}] â­ï¸ ë³€ê²½ ì—†ìŒ: {os.path.basename(p)} (skip)")
                continue

            print(f"[{idx}/{len(paths)}] ğŸ“¥ íŒŒì¼ ë¡œë“œ ì¤‘: {os.path.basename(p)}")
            docs = PyPDFLoader(abs_path).load()

            # ì´í›„ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° ì£¼ì…
            for d in docs:
                d.metadata["source"] = abs_path       # ì ˆëŒ€ ê²½ë¡œ
                d.metadata["file_sha256"] = file_hash # íŒŒì¼ í•´ì‹œ

            all_docs.extend(docs)
            to_process += 1
            print(f"    âœ… ë¡œë“œ ì™„ë£Œ (pages={len(docs)})")
        except Exception as e:
            print(f"[{idx}/{len(paths)}] â— ë¡œë“œ ì‹¤íŒ¨: {p} -> {e}")

    print(f"ğŸ“š ì´ í˜ì´ì§€ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    print(f"â­ï¸ ìŠ¤í‚µ: {skipped}ê°œ, â³ ì‹ ê·œ/ë³€ê²½: {to_process}ê°œ")
    print("ğŸ“¥ [1/4] PDF ë¡œë“œ ë‹¨ê³„ ì™„ë£Œ")
    return all_docs


def split_documents(documents: List[Any]) -> List[Any]:
    print("âœ‚ï¸ [2/4] ë¬¸ì„œ ë¶„í•  ë‹¨ê³„ ì‹œì‘")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len
    )
    splits = splitter.split_documents(documents)
    print(f"âœ‚ï¸ ì²­í¬ ìˆ˜: {len(splits)}")
    print("âœ‚ï¸ [2/4] ë¬¸ì„œ ë¶„í•  ë‹¨ê³„ ì™„ë£Œ")
    return splits


def build_or_update_faiss(splits: List[Any], db_path: str) -> None:
    print("ğŸ§® [3/4] ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ ì‹œì‘")
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)

    # ì¸ë±ìŠ¤ ë¡œë“œ/ìƒì„±
    if os.path.exists(db_path):
        print(f"ğŸ“¦ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: {db_path}")
        vs = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    else:
        print("ğŸ§± ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì¤‘â€¦")
        vs = None

    # === splitsë¥¼ íŒŒì¼ë³„ë¡œ ë¬¶ê¸° (source ê¸°ì¤€) ===
    file_map = {}
    for s in splits:
        fname = s.metadata.get("source", "unknown.pdf")
        file_map.setdefault(fname, []).append(s)

    file_list = list(file_map.items())
    total_files = len(file_list)

    # íŒŒì¼ë³„ ì„ë² ë”©
    for idx, (fname, file_splits) in enumerate(file_list, start=1):
        chunks = len(file_splits)
        # í˜ì´ì§€ ìˆ˜ ì¶”ì •: split ë©”íƒ€ë°ì´í„°ì˜ page ê°’ì„ unique count
        pages = len({sp.metadata.get("page") for sp in file_splits if "page" in sp.metadata})

        if vs:
            vs.add_documents(file_splits)
        else:
            vs = FAISS.from_documents(file_splits, embeddings)

        # âœ… íŒŒì¼ë³„ ì„ë² ë”© ì™„ë£Œ ë¡œê·¸
        print(f"[{idx}/{total_files}] âœ… {os.path.basename(fname)} ì„ë² ë”© ì™„ë£Œ "
              f"(pages={pages if pages else 'N/A'}, chunks={chunks})")

    # ì €ì¥
    if vs:
        vs.save_local(db_path)
    print(f"ğŸ’¾ ìµœì¢… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {db_path}")

    # === ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ê°±ì‹ : ì´ë²ˆì— ì²˜ë¦¬ëœ íŒŒì¼ì˜ sha256 ê¸°ë¡ ===
    manifest = _load_manifest(db_path)
    for fname, file_splits in file_list:
        sha = None
        for sp in file_splits:
            sha = sp.metadata.get("file_sha256")
            if sha:
                break
        if sha:
            manifest[_abs(fname)] = {"sha256": sha}
    _save_manifest(db_path, manifest)

    # ì´í•© ë¡œê·¸
    total_pages = sum(len({sp.metadata.get("page") for sp in fs if "page" in sp.metadata})
                      for _, fs in file_list)
    total_chunks = sum(len(fs) for _, fs in file_list)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼: íŒŒì¼={total_files}, í˜ì´ì§€={total_pages}, ì²­í¬={total_chunks}")
    print("ğŸ§® [3/4] ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ ì™„ë£Œ")
    

if __name__ == "__main__":
    print("ğŸš€ ì¸ë±ìŠ¤ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    docs = load_all_pdfs(PDF_DIR)             # âœ… ì‹ ê·œ/ë³€ê²½ íŒŒì¼ë§Œ ë°˜í™˜ (ìŠ¤í‚µ ë¡œê·¸ í¬í•¨)
    splits = split_documents(docs)             # âœ‚ï¸ [2/4] ë¡œê·¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
    build_or_update_faiss(splits, VECTOR_DB_PATH)  # íŒŒì¼ë³„ ì„ë² ë”© ì™„ë£Œ ë¡œê·¸ + manifest ê°±ì‹ 
    print(f"ğŸ‰ [4/4] ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ (ì €ì¥ ê²½ë¡œ: {VECTOR_DB_PATH})")
