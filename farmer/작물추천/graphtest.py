import os
from glob import glob
from typing import List, Any, TypedDict
import json
import hashlib
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

# === ì„¤ì • ===
PDF_DIR = os.getenv("PDF_DIR", "./cropinfo")
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_pdf_db")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

# === LangChain ===
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# === LangGraph ===
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document

# === ì‹ ê·œ/ë³€ê²½ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìœ í‹¸ ===
MANIFEST_FILE = "manifest.json"

def _abs(p: str) -> str:
    return str(Path(p).resolve())

def _sha256_file(path: str, buf_size: int = 1 << 20) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            b = f.read(buf_size)
            if not b:
                break
            h.update(b)
    return h.hexdigest()

def _manifest_path(db_path: str) -> str:
    return os.path.join(db_path, MANIFEST_FILE)

def _load_manifest(db_path: str) -> dict:
    try:
        with open(_manifest_path(db_path), "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except Exception:
        return {}

def _save_manifest(db_path: str, manifest: dict) -> None:
    os.makedirs(db_path, exist_ok=True)
    with open(_manifest_path(db_path), "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)

# === Graph State ì •ì˜ ===
class GraphState(TypedDict):
    """
    RAG ì¸ë±ìŠ¤ ë¹Œë“œ ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤.
    """
    documents: List[Document]
    splits: List[Document]
    error: str

# === ë‹¨ê³„ë³„ ë…¸ë“œ í•¨ìˆ˜ ===
def load_all_pdfs_node(state: GraphState) -> GraphState:
    """
    PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ë³€ê²½ëœ íŒŒì¼ë§Œ ì‹ë³„í•˜ì—¬ ìƒíƒœì— ì €ì¥í•˜ëŠ” ë…¸ë“œ.
    """
    print("ğŸ“¥ [1/4] PDF ë¡œë“œ ë‹¨ê³„ ì‹œì‘")
    paths = sorted(glob(os.path.join(PDF_DIR, "*.pdf")))
    if not paths:
        return {"error": f"PDFê°€ ì—†ìŠµë‹ˆë‹¤: {PDF_DIR}"}
    print(f"ğŸ“‚ ì´ íŒŒì¼ ìˆ˜: {len(paths)}")

    manifest = _load_manifest(VECTOR_DB_PATH)
    all_docs = []
    skipped, to_process = 0, 0

    for idx, p in enumerate(paths, start=1):
        abs_path = _abs(p)
        try:
            file_hash = _sha256_file(abs_path)

            if manifest.get(abs_path, {}).get("sha256") == file_hash:
                skipped += 1
                print(f"[{idx}/{len(paths)}] â­ï¸ ë³€ê²½ ì—†ìŒ: {os.path.basename(p)} (skip)")
                continue

            print(f"[{idx}/{len(paths)}] ğŸ“¥ íŒŒì¼ ë¡œë“œ ì¤‘: {os.path.basename(p)}")
            docs = PyPDFLoader(abs_path).load()

            for d in docs:
                d.metadata["source"] = abs_path
                d.metadata["file_sha256"] = file_hash

            all_docs.extend(docs)
            to_process += 1
            print(f"    âœ… ë¡œë“œ ì™„ë£Œ (pages={len(docs)})")
        except Exception as e:
            print(f"[{idx}/{len(paths)}] â— ë¡œë“œ ì‹¤íŒ¨: {p} -> {e}")
            return {"error": f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {p} -> {e}"}

    print(f"ğŸ“š ì´ í˜ì´ì§€ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    print(f"â­ï¸ ìŠ¤í‚µ: {skipped}ê°œ, â³ ì‹ ê·œ/ë³€ê²½: {to_process}ê°œ")
    print("ğŸ“¥ [1/4] PDF ë¡œë“œ ë‹¨ê³„ ì™„ë£Œ")
    return {"documents": all_docs, "splits": []}

def split_documents_node(state: GraphState) -> GraphState:
    """
    ë¡œë“œëœ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìƒíƒœì— ì €ì¥í•˜ëŠ” ë…¸ë“œ.
    """
    print("âœ‚ï¸ [2/4] ë¬¸ì„œ ë¶„í•  ë‹¨ê³„ ì‹œì‘")
    documents = state.get("documents", [])
    if not documents:
        print("â­ï¸ ë¶„í• í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"splits": []}
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len
    )
    splits = splitter.split_documents(documents)
    print(f"âœ‚ï¸ ì²­í¬ ìˆ˜: {len(splits)}")
    print("âœ‚ï¸ [2/4] ë¬¸ì„œ ë¶„í•  ë‹¨ê³„ ì™„ë£Œ")
    return {"splits": splits}

def build_or_update_faiss_node(state: GraphState) -> GraphState:
    """
    ë¶„í• ëœ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±/ê°±ì‹ í•˜ëŠ” ë…¸ë“œ.
    """
    print("ğŸ§® [3/4] ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ ì‹œì‘")
    splits = state.get("splits", [])
    if not splits:
        print("â­ï¸ ì„ë² ë”©í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    
    if os.path.exists(VECTOR_DB_PATH):
        print(f"ğŸ“¦ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: {VECTOR_DB_PATH}")
        vs = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        print("ğŸ§± ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì¤‘â€¦")
        vs = None

    file_map = {}
    for s in splits:
        fname = s.metadata.get("source", "unknown.pdf")
        file_map.setdefault(fname, []).append(s)

    file_list = list(file_map.items())
    total_files = len(file_list)

    for idx, (fname, file_splits) in enumerate(file_list, start=1):
        chunks = len(file_splits)
        pages = len({sp.metadata.get("page") for sp in file_splits if "page" in sp.metadata})

        if vs:
            vs.add_documents(file_splits)
        else:
            vs = FAISS.from_documents(file_splits, embeddings)

        print(f"[{idx}/{total_files}] âœ… {os.path.basename(fname)} ì„ë² ë”© ì™„ë£Œ "
              f"(pages={pages if pages else 'N/A'}, chunks={chunks})")

    if vs:
        vs.save_local(VECTOR_DB_PATH)
    print(f"ğŸ’¾ ìµœì¢… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {VECTOR_DB_PATH}")

    manifest = _load_manifest(VECTOR_DB_PATH)
    for fname, file_splits in file_list:
        sha = next((sp.metadata.get("file_sha256") for sp in file_splits if "file_sha256" in sp.metadata), None)
        if sha:
            manifest[_abs(fname)] = {"sha256": sha}
    _save_manifest(VECTOR_DB_PATH, manifest)

    total_pages = sum(len({sp.metadata.get("page") for sp in fs if "page" in sp.metadata})
                      for _, fs in file_list)
    total_chunks = sum(len(fs) for _, fs in file_list)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼: íŒŒì¼={total_files}, í˜ì´ì§€={total_pages}, ì²­í¬={total_chunks}")
    print("ğŸ§® [3/4] ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ ì™„ë£Œ")
    return {}

# === ê·¸ë˜í”„ ë¹Œë“œ í•¨ìˆ˜ ===
def build_graph():
    """
    LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ ë¹Œë“œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    graph = StateGraph(GraphState)
    graph.add_node("load_all_pdfs", load_all_pdfs_node)
    graph.add_node("split_documents", split_documents_node)
    graph.add_node("build_or_update_faiss", build_or_update_faiss_node)

    graph.add_edge("load_all_pdfs", "split_documents")
    graph.add_edge("split_documents", "build_or_update_faiss")
    graph.add_edge("build_or_update_faiss", END)

    graph.set_entry_point("load_all_pdfs")
    return graph.compile()

# === ë©”ì¸ ì‹¤í–‰ ë¡œì§ ===
if __name__ == "__main__":
    app = build_graph()
    
    # â”€â”€ ê·¸ë˜í”„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        from langgraph.graph import MermaidDrawMethod
        graph_image_path = Path(".") / "index_build_graph.png"
        png_bytes = app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API
        )
        with open(graph_image_path, "wb") as f:
            f.write(png_bytes)
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            ascii_map = app.get_graph().draw_ascii()
            print("\n[ASCII Graph]")
            print(ascii_map)
            mermaid_src = app.get_graph().draw_mermaid()
            mmd_path = Path(".") / "index_build_graph.mmd"
            with open(mmd_path, "w", encoding="utf-8") as f:
                f.write(mermaid_src)
            print(f"ğŸ“ Mermaid ì†ŒìŠ¤ë¥¼ '{mmd_path}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (mermaid.live ë“±ì—ì„œ ë Œë” ê°€ëŠ¥)")
        except Exception as e2:
            print(f"ì¶”ê°€ ë°±ì—…ë„ ì‹¤íŒ¨: {e2}")
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    initial_state = {"documents": [], "splits": [], "error": None}
    print("\nğŸš€ ì¸ë±ìŠ¤ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    final_state = app.invoke(initial_state)

    if final_state.get("error"):
        print(f"â— ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {final_state['error']}")
    else:
        print(f"ğŸ‰ [4/4] ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ (ì €ì¥ ê²½ë¡œ: {VECTOR_DB_PATH})")
