import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re


# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
# .env íŒŒì¼ì´ ì—†ë‹¤ë©´ ì§ì ‘ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
# ì˜ˆ: os.environ["GROQ_API_KEY"] = "YOUR_API_KEY"
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-8b-8192") # ë” ë¹ ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from pymilvus import connections

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)


# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]           # ì‚¬ìš©ì ì§ˆë¬¸
    vectorstore: Optional[Milvus]     # Milvus ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    context: Optional[str]            # Milvusì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    answer: Optional[str]             # LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€
    web_search_results: Optional[str] # ì›¹ ê²€ìƒ‰ ê²°ê³¼
    user_decision: Optional[str]      # "yes"|"no"
    decision_reason: Optional[str]    # ìë™ íŒë‹¨ ì´ìœ  ë¡œê¹…
    log_file: Optional[str]           # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    answer_source: Optional[str]      # ë‹µë³€ ì¶œì²˜ ('ë‚´ë¶€ DB' ë˜ëŠ” 'ì›¹ ê²€ìƒ‰')

# --- Embeddings ë° LLM ---
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# ì‹œê°„ ë¯¼ê° í‚¤ì›Œë“œ
TIME_SENSITIVE = re.compile(
    r"(ìµœì‹ |ì˜¤ëŠ˜|ë°©ê¸ˆ|ì§€ê¸ˆ|ì‹¤ì‹œê°„|ë³€ê²½|ì—…ë°ì´íŠ¸|ë‰´ìŠ¤|ê°€ê²©|í™˜ìœ¨|ì£¼ê°€|ì¼ì •|ìŠ¤ì¼€ì¤„|ì˜ˆë³´|ë‚ ì”¨|ëª¨ì§‘|ì±„ìš©|ì¬ê³ |íŒë§¤|ìš´í•­|ë°œí‘œ)"
)

# --- ìœ í‹¸: ëŒ€í™” ë¡œê·¸ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, source: str, filename: str):
    # answer ë¬¸ì¥ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?])\s+', answer.strip())
    sentences = [s for s in sentences if s]  # ë¹ˆ ë¬¸ìì—´ ì œê±°

    data = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": sentences,   # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
        "source": source
    }

    if filename:
        try:
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                try:
                    with open(filename, "r", encoding="utf-8") as f:
                        hist: List[Dict] = json.load(f)
                except json.JSONDecodeError:
                    print(f"       âš ï¸ '{filename}' ì†ìƒ â†’ ìƒˆ íŒŒì¼ ì‹œì‘")
                    hist = []
            else:
                hist = []
            hist.append(data)
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(hist, f, ensure_ascii=False, indent=4)
            print(f"       âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥: {filename}")
        except Exception as e:
            print(f"       âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")


# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        print("       - Milvus ì—°ê²°ì´ ì—†ì–´ ìƒˆë¡œ ì—°ê²°í•©ë‹ˆë‹¤.")
        connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)

    try:
        vs = Milvus(
            embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"uri": MILVUS_URI, "token": MILVUS_TOKEN},
        )
        print(f"       âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"       âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")
    if not question or not vectorstore:
        raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"       - ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)

    context = ""
    print(f"       âœ… {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰.")
    for i, (doc, score) in enumerate(docs_with_scores):
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f"       - ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")
        context += f"\n\n{doc.page_content}"
    return {**state, "context": context}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    context = state.get("context")
    question = state.get("question")
    if not context or not question:
        raise ValueError("ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    chain = (rag_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"context": context, "question": question})
    print("       âœ… RAG ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"       - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans, "answer_source": "ë‚´ë¶€ DB"}

def user_decision_node(state: GraphState) -> Dict[str, Any]:
    """
    âœ… ìë™ íŒë‹¨:
        - Tavily í‚¤ ì—†ìŒ â†’ 'no'
        - RAG ì‹¤íŒ¨ë¬¸êµ¬ í¬í•¨ â†’ 'yes'
        - ì‹œê°„ë¯¼ê° í‚¤ì›Œë“œ í¬í•¨ â†’ 'yes'
        - ê·¸ ì™¸ â†’ 'no'
    """
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì‚¬ìš©ì ê²°ì •(ìë™) ---")
    q = state.get("question", "") or ""
    ans = state.get("answer", "") or ""
    if not TAVILY_API_KEY:
        print("       â†ªï¸ ì›¹ê²€ìƒ‰ ë¶ˆê°€: no_tavily_api_key")
        return {**state, "user_decision": "no", "decision_reason": "no_tavily_api_key"}
    rag_failed = "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in ans
    time_sensitive = bool(TIME_SENSITIVE.search(q))
    if rag_failed or time_sensitive:
        reason = "rag_failed" if rag_failed else "time_sensitive"
        print(f"       â†ªï¸ ì›¹ê²€ìƒ‰ ì§„í–‰: {reason}")
        return {**state, "user_decision": "yes", "decision_reason": reason}
    print("       â†ªï¸ ì›¹ê²€ìƒ‰ ê±´ë„ˆëœ€: context_sufficient")
    return {**state, "user_decision": "no", "decision_reason": "context_sufficient"}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not TAVILY_API_KEY:
        print("       âš ï¸ TAVILY_API_KEY ë¯¸ì„¤ì • â†’ ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”"}
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question})
    sr = "\n\n".join([json.dumps(r, ensure_ascii=False) for r in results])
    print("       âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹ :", len(results), "ê°œ")
    return {**state, "web_search_results": sr}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results")
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("       âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ ë¶€ì¡± â†’ ì›¹ê¸°ë°˜ ë‹µë³€ ë¶ˆê°€")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "answer_source": "ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨"}
    chain = (web_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"question": question, "search_results": search_results})
    print("       âœ… ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"       - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans, "answer_source": "ì›¹ ê²€ìƒ‰"}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    """
    âœ… ìµœì¢… ë‹µë³€ ì²˜ë¦¬:
      - ë‹µë³€ì„ ì¶œë ¥í•˜ê³  ë¡œê·¸ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
      - ì´ ë…¸ë“œëŠ” ê·¸ë˜í”„ì˜ ìµœì¢… ë‹¨ê³„ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ ---")
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    source = state.get("answer_source", "ì•Œ ìˆ˜ ì—†ìŒ")

    print(f"âœ… ë‹µë³€ ì¶œì²˜: {source}")
    print(answer)
    print("---------------------\n")

    log_file = state.get("log_file") or ""
    q_for_log = state.get("question") or ""
    append_conversation_to_file(q_for_log, answer, source, log_file)

    return state

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… ---
def route_to_web_search(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: RAG ê²°ê³¼ ê¸°ë°˜ ë¶„ê¸° ---")
    answer = (state.get("answer") or "")
    if "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in answer:
        print("       â†ªï¸ RAG ì‹¤íŒ¨ â†’ user_decision")
        return "user_decision"
    print("       ğŸ‰ RAG ì„±ê³µ â†’ generate_answer")
    return "generate_answer"

def route_user_decision(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ì‚¬ìš©ì ê²°ì •(ìë™) ì²˜ë¦¬ ---")
    return "do_web_search" if state.get("user_decision") == "yes" else "skip_web_search"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("user_decision", user_decision_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_edge("generate_rag", "user_decision")
    g.add_conditional_edges("user_decision", route_user_decision,
                            {"do_web_search": "web_search", "skip_web_search": "generate_answer"})
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "generate_answer")
    
    g.add_edge("generate_answer", END)
    
    return g.compile()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch ì‹œì‘ (ì¢…ë£Œ: exit ë˜ëŠ” quit ì…ë ¥)")

    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()

    # --- âœï¸ [ìˆ˜ì •ë¨] ê·¸ë˜í”„ ì‹œê°í™” ---
    try:
        graph_image_path = "milvus_agent_workflow_llm.png"
        with open(graph_image_path, "wb") as f:
            # ë³€ìˆ˜ëª…ì„ agent_appì—ì„œ appìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        # Mermaid-CLIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # https://mermaid-js.github.io/mermaid/getting-started/mermaid-cli.html
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   (ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” 'mermaid-cli'ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    # --- ëŒ€í™” ë£¨í”„ ì‹¤í–‰ ---
    while True:
        q = input("\nì§ˆë¬¸> ").strip()
        if not q or q.lower() in ("exit", "quit"):
            print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸšª")
            break

        # ê·¸ë˜í”„ ì‹¤í–‰
        app.invoke({"question": q, "log_file": session_log_file})
        
    print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ. ğŸ")