import os
import json
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
from datetime import datetime
import re

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(find_dotenv())

# --- Milvus / Embedding ëª¨ë¸ ì„¤ì • ---
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")

# --- LLM ì„¤ì • ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# --- Web Search ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.runnables.graph import MermaidDrawMethod
from pymilvus import connections

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡/í•œì ê¸ˆì§€.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹¨ê³„/ì„¤ëª…ì€ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ".
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{search_results}

ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
- í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.

ğŸŸ¢ ì§ˆë¬¸: {question}
âœ¨ ë‹µë³€:
"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]            # ì‚¬ìš©ì ì§ˆë¬¸
    vectorstore: Optional[Milvus]      # Milvus ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    context: Optional[str]             # Milvusì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    answer: Optional[str]              # LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€
    web_search_results: Optional[str]  # ì›¹ ê²€ìƒ‰ ê²°ê³¼
    user_decision: Optional[str]       # "yes"|"no"
    decision_reason: Optional[str]     # ìë™ íŒë‹¨ ì´ìœ  ë¡œê¹…
    log_file: Optional[str]            # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ

# --- Embeddings ë° LLM ---
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"}
)

def make_llm() -> ChatGroq:
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# ì‹œê°„ ë¯¼ê° í‚¤ì›Œë“œ
TIME_SENSITIVE = re.compile(
    r"(ìµœì‹ |ì˜¤ëŠ˜|ë°©ê¸ˆ|ì§€ê¸ˆ|ì‹¤ì‹œê°„|ë³€ê²½|ì—…ë°ì´íŠ¸|ë‰´ìŠ¤|ê°€ê²©|í™˜ìœ¨|ì£¼ê°€|ì¼ì •|ìŠ¤ì¼€ì¤„|ì˜ˆë³´|ë‚ ì”¨|ëª¨ì§‘|ì±„ìš©|ì¬ê³ |íŒë§¤|ìš´í•­|ë°œí‘œ)"
)

# --- ìœ í‹¸: ëŒ€í™” ë¡œê·¸ ì €ì¥ ---
def append_conversation_to_file(question: str, answer: str, filename: str):
    data = {"timestamp": datetime.now().isoformat(), "question": question, "answer": answer}
    if filename:
        try:
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                try:
                    with open(filename, "r", encoding="utf-8") as f:
                        hist: List[Dict] = json.load(f)
                except json.JSONDecodeError:
                    print(f"    âš ï¸ '{filename}' ì†ìƒ â†’ ìƒˆ íŒŒì¼ ì‹œì‘")
                    hist = []
            else:
                hist = []
            hist.append(data)
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(hist, f, ensure_ascii=False, indent=4)
            print(f"    âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥: {filename}")
        except Exception as e:
            print(f"    âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

# --- LangGraph ë…¸ë“œ ---
def load_milvus_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ---")
    if "default" not in connections.list_connections() or not connections.has_connection("default"):
        print("    - Milvus ì—°ê²°ì´ ì—†ì–´ ìƒˆë¡œ ì—°ê²°í•©ë‹ˆë‹¤.")
        # connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
        connections.connect(alias="default", host="localhost", port="19530")
    try:
        vs = Milvus(
            embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"host": "localhost", "port": "19530"},
        )
        print(f"    âœ… Milvus ë¡œë“œ ì™„ë£Œ (ì»¬ë ‰ì…˜: {MILVUS_COLLECTION})")
        return {**state, "vectorstore": vs}
    except Exception as e:
        print(f"    âŒ Milvus ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise ConnectionError("Milvus ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state.get("question")
    vectorstore = state.get("vectorstore")
    if not question or not vectorstore:
        raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë²¡í„°ìŠ¤í† ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"    - ì§ˆë¬¸: '{question}'")
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)

    context = ""
    print(f"    âœ… {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰.")
    for i, (doc, score) in enumerate(docs_with_scores):
        preview = (doc.page_content or "")[:100].replace("\n", " ")
        print(f"    - ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.4f}): '{preview}...'")
        context += f"\n\n{doc.page_content}"
    return {**state, "context": context}

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: RAG ë‹µë³€ ìƒì„± ---")
    context = state.get("context")
    question = state.get("question")
    if not context or not question:
        raise ValueError("ë¬¸ë§¥ ë˜ëŠ” ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    chain = (rag_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"context": context, "question": question})
    print("    âœ… RAG ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans}

def user_decision_node(state: GraphState) -> Dict[str, Any]:
    """
    âœ… ìë™ íŒë‹¨:
      - Tavily í‚¤ ì—†ìŒ â†’ 'no'
      - RAG ì‹¤íŒ¨ë¬¸êµ¬ í¬í•¨ â†’ 'yes'
      - ì‹œê°„ë¯¼ê° í‚¤ì›Œë“œ í¬í•¨ â†’ 'yes'
      - ê·¸ ì™¸ â†’ 'no'
    """
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì‚¬ìš©ì ê²°ì •(ìë™) ---")
    q = state.get("question", "") or ""
    ans = state.get("answer", "") or ""
    if not TAVILY_API_KEY:
        print("    â†ªï¸ ì›¹ê²€ìƒ‰ ë¶ˆê°€: no_tavily_api_key")
        return {**state, "user_decision": "no", "decision_reason": "no_tavily_api_key"}
    rag_failed = "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in ans
    time_sensitive = bool(TIME_SENSITIVE.search(q))
    if rag_failed or time_sensitive:
        reason = "rag_failed" if rag_failed else "time_sensitive"
        print(f"    â†ªï¸ ì›¹ê²€ìƒ‰ ì§„í–‰: {reason}")
        return {**state, "user_decision": "yes", "decision_reason": reason}
    print("    â†ªï¸ ì›¹ê²€ìƒ‰ ê±´ë„ˆëœ€: context_sufficient")
    return {**state, "user_decision": "no", "decision_reason": "context_sufficient"}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰ ---")
    question = state.get("question")
    if not question:
        raise ValueError("ì§ˆë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not TAVILY_API_KEY:
        print("    âš ï¸ TAVILY_API_KEY ë¯¸ì„¤ì • â†’ ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”")
        return {**state, "web_search_results": "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”"}
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": question})
    sr = "\n\n".join([json.dumps(r, ensure_ascii=False) for r in results])
    print("    âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹ :", len(results), "ê°œ")
    return {**state, "web_search_results": sr}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("--- ğŸ§© ë…¸ë“œ ì‹œì‘: ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ---")
    question = state.get("question")
    search_results = state.get("web_search_results")
    if not question or not search_results or search_results == "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”":
        print("    âš ï¸ ì›¹ ê²€ìƒ‰ ì •ë³´ ë¶€ì¡± â†’ ì›¹ê¸°ë°˜ ë‹µë³€ ë¶ˆê°€")
        return {**state, "answer": "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    chain = (web_prompt | make_llm() | StrOutputParser())
    ans = chain.invoke({"question": question, "search_results": search_results})
    print("    âœ… ì›¹ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    print(f"    - ë¯¸ë¦¬ë³´ê¸°: '{ans[:100]}...'")
    return {**state, "answer": ans}

def generate_answer_node(state: GraphState) -> Dict[str, Any]:
    """
    ğŸ” ë¬´í•œ ë£¨í”„ ëª¨ë“œ:
      - ë‹µë³€ ì¶œë ¥ í›„ ì¦‰ì‹œ ë‹¤ìŒ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ state.question ê°±ì‹ 
      - 'exit'/'quit' ì…ë ¥ ì‹œì—ë§Œ ì¢…ë£Œ ì‹ í˜¸(question='quit') ë°˜í™˜
      - ë§¤ ë¼ìš´ë“œ ì¦‰ì‹œ ë¡œê·¸ ì €ì¥
    """
    print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ ---")
    answer = state.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    print(answer)
    print("---------------------\n")

    # ë¡œê·¸ ì €ì¥ (ë¼ìš´ë“œë³„)
    log_file = state.get("log_file") or ""
    q_for_log = state.get("question") or ""
    append_conversation_to_file(q_for_log, answer, log_file)

    # ë‹¤ìŒ ì§ˆë¬¸ ì…ë ¥(ë¬´í•œ ë£¨í”„)
    while True:
        next_q = input("ì§ˆë¬¸> ").strip()
        if next_q:
            if next_q.lower() in ("exit", "quit"):
                print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ìš”ì²­. ğŸ")
                return {**state, "question": "quit"}
            print(f"ğŸ’¬ ë‹¤ìŒ ì§ˆë¬¸: '{next_q}'")
            return {**state, "question": next_q}

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… ---
def route_to_web_search(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: RAG ê²°ê³¼ ê¸°ë°˜ ë¶„ê¸° ---")
    answer = (state.get("answer") or "")
    if "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in answer:
        print("    â†ªï¸ RAG ì‹¤íŒ¨ â†’ user_decision")
        return "user_decision"
    print("    ğŸ‰ RAG ì„±ê³µ â†’ generate_answer")
    return "generate_answer"

def route_user_decision(state: GraphState) -> str:
    print("--- ğŸ§­ ë¼ìš°í„°: ì‚¬ìš©ì ê²°ì •(ìë™) ì²˜ë¦¬ ---")
    return "do_web_search" if state.get("user_decision") == "yes" else "skip_web_search"

def route_next_step(state: GraphState) -> str:
    return "end" if state.get("question") == "quit" else "continue"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_milvus", load_milvus_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("user_decision", user_decision_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)
    g.add_node("generate_answer", generate_answer_node)

    g.set_entry_point("load_milvus")
    g.add_edge("load_milvus", "retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_conditional_edges("generate_rag", route_to_web_search,
                            {"user_decision": "user_decision", "generate_answer": "generate_answer"})
    g.add_conditional_edges("user_decision", route_user_decision,
                            {"do_web_search": "web_search", "skip_web_search": "generate_answer"})
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", "generate_answer")
    g.add_conditional_edges("generate_answer", route_next_step,
                            {"continue": "load_milvus", "end": END})
    return g.compile()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ’¬ Milvus ê¸°ë°˜ LangGraph RAG + WebSearch ì‹œì‘ (ë¬´í•œ ë£¨í”„: exit/quit ë¡œ ì¢…ë£Œ)")

    # ì„¸ì…˜ ë¡œê·¸ íŒŒì¼
    log_dir = "milvusdb_crop65llm_logs"
    Path(log_dir).mkdir(exist_ok=True)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_log_file = f"{log_dir}/conversation_log_{session_timestamp}.json"

    app = build_graph()

    # ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥(ì„ íƒ)
    try:
        graph_image_path = "milvus_agent_workflow_llm.png"
        Path(graph_image_path).parent.mkdir(parents=True, exist_ok=True)
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))
        print(f"\nâœ… LangGraph êµ¬ì¡° ì €ì¥: '{graph_image_path}'")
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì˜¤ë¥˜: {e}")

    # ìµœì´ˆ 1íšŒ ì§ˆë¬¸ ë°›ê³ , ì´í›„ëŠ” ê·¸ë˜í”„ ë‚´ë¶€ì—ì„œ ë¬´í•œ ë£¨í”„
    q = input("ì§ˆë¬¸> ").strip()
    if q.lower() in ("exit", "quit") or not q:
        print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ì´ ì¢…ë£Œë©ë‹ˆë‹¤. ğŸšª")
    else:
        app.invoke({"question": q, "log_file": session_log_file})
    print("ğŸ’¬ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ. ğŸ")