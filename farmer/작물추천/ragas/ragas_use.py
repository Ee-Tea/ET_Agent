# (Milvus + Web Search Agent + Ragas Evaluation)
# ------------------------------------------------------------
# 1) Chat (Milvus + Tavily Web Search)
# 2) Golden-set Evaluation (custom similarity-based eval)
# 3) Ragas Evaluation (Amnesty QA dataset with Ollama)
# ------------------------------------------------------------
import os
import json
import re
from typing import TypedDict, Optional, Any, Dict, List
from dotenv import load_dotenv, find_dotenv
from datetime import datetime
import numpy as np
import pandas as pd
import time
from numpy.linalg import norm  # âœ… ë²¡í„° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(find_dotenv())

# --- ì„¤ì • ---
GOLDENSET_CSV = r"C:\Rookies_project\ET_Agent\farmer\ì‘ë¬¼ì¶”ì²œ\Goldenset_test\Goldenset_test1.csv"
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "root:milvus")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION", "hongyoungjun")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-8b-8192")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.75"))
RETRIEVAL_K = int(os.getenv("RETRIEVAL_K", "5"))

# ğŸ”§ Milvus score â†’ similarity ë³€í™˜ ê´€ë ¨ í™˜ê²½ë³€ìˆ˜
MILVUS_METRIC = os.getenv("MILVUS_METRIC", "cosine").lower()  # cosine | l2 | ip
MILVUS_SCORE_IS_DISTANCE = os.getenv("MILVUS_SCORE_IS_DISTANCE", "true").lower() == "true"
MILVUS_IP_RESCALE_01 = os.getenv("MILVUS_IP_RESCALE_01", "true").lower() == "true"

# --- í•„ìˆ˜ API í‚¤ í™•ì¸ ---
if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
if not TAVILY_API_KEY:
    print("âš ï¸ ê²½ê³ : TAVILY_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from pymilvus import connections

# --- ìƒìˆ˜/í—¬í¼ ---
NO_INFO_PHRASE = "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

def normalize(text: Optional[str]) -> str:
    if not text:
        return ""
    t = text.replace("\n", " ").replace("\r", " ")
    t = re.sub(r"\s+", " ", t)
    t = t.replace(".", "").strip()
    return t

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
RAG_PROMPT_TMPL = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 'ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
[ë¬¸ë§¥]: {context}
ê·œì¹™:
- ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´/ì¶”ì¸¡ ê¸ˆì§€. í•œê¸€ë¡œë§Œ ì‘ì„±.
- ë¬¸ë§¥ì— ê·¼ê±° ì—†ìœ¼ë©´: "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
ì§ˆë¬¸: {question}
ë‹µë³€:"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TMPL)

WEB_PROMPT_TMPL = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼'ì™€ 'ì§ˆë¬¸'ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
[ì›¹ ê²€ìƒ‰ ê²°ê³¼]:
{search_results}

ê·œì¹™:
- ë°˜ë“œì‹œ 'ì›¹ ê²€ìƒ‰ ê²°ê³¼' ë‚´ ê·¼ê±°ë¡œë§Œ ë‹µí•˜ì„¸ìš”. í•œê¸€ë¡œë§Œ ì‘ì„±.
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.
- í•­ìƒ ë‹µë³€ ë§ˆì§€ë§‰ì— "ì°¸ê³  ë§í¬:" ì„¹ì…˜ì„ ë§Œë“¤ê³ , ì ì ˆí•œ ìƒìœ„ 1~3ê°œ URLì„ bulletë¡œ ì²¨ë¶€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:"""
web_prompt = ChatPromptTemplate.from_template(WEB_PROMPT_TMPL)

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict, total=False):
    question: Optional[str]
    # ë¶„ë¦¬ëœ ì»¨í…ìŠ¤íŠ¸
    context_internal: Optional[str]       # ë‚´ë¶€ DB ì»¨í…ìŠ¤íŠ¸
    context_web: Optional[str]            # ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸(ì •ë¦¬ëœ ê²°ê³¼)
    answer: Optional[str]
    web_search_results: Optional[str]
    answer_source: Optional[str]
    log_file: Optional[str]
    no_info: Optional[bool]
    force_web: Optional[bool]
    retrieved_docs: Optional[List[Dict[str, Any]]]
    retrieval_time_ms: Optional[float]

# --- ê³µí†µ ê°ì²´ ---
embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME, model_kwargs={"device": "cpu"})
llm = ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)
web_search_tool = TavilySearchResults(max_results=3, api_key=TAVILY_API_KEY) if TAVILY_API_KEY else None

# --- í…ìŠ¤íŠ¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì„ë² ë”© í¬í•¨) ---
def cosine_sim(txt1: str, txt2: str) -> float:
    if not txt1 or not txt2:
        return 0.0
    v1 = np.array(embedding_model.embed_query(txt1))
    v2 = np.array(embedding_model.embed_query(txt2))
    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)
    return float(np.dot(v1, v2) / (n1 * n2)) if n1 > 0 and n2 > 0 else 0.0

# ì˜¤íƒ€ ë°©ì§€ìš© ë³„ì¹­
def consine(txt1: str, txt2: str) -> float:
    return cosine_sim(txt1, txt2)

# --- Milvus score â†’ similarity ë³€í™˜ ---
def convert_score_to_similarity(raw: float) -> float:
    if MILVUS_METRIC == "cosine":
        sim = 1.0 - float(raw) if MILVUS_SCORE_IS_DISTANCE else float(raw)
        return max(0.0, min(1.0, sim))
    elif MILVUS_METRIC == "l2":
        d = float(raw) if MILVUS_SCORE_IS_DISTANCE else max(0.0, 1.0 - float(raw))
        sim = 1.0 / (1.0 + max(0.0, d))
        return max(0.0, min(1.0, sim))
    elif MILVUS_METRIC == "ip":
        val = -float(raw) if MILVUS_SCORE_IS_DISTANCE else float(raw)
        if MILVUS_IP_RESCALE_01:
            sim = (val + 1.0) / 2.0
            return max(0.0, min(1.0, sim))
        return val
    else:
        return max(0.0, min(1.0, float(raw)))

# --- ë¡œê·¸ ìœ í‹¸: ë‚´ë¶€/ì›¹ ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì €ì¥ ---
def append_log(state: GraphState):
    log_file = state.get("log_file")
    if not log_file:
        return
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": state.get("question"),
        "answer": state.get("answer"),
        "source": state.get("answer_source"),
        "context_internal": state.get("context_internal", ""),
        "context_web": state.get("context_web", ""),
    }
    history = []
    if os.path.exists(log_file) and os.path.getsize(log_file) > 0:
        with open(log_file, "r", encoding="utf-8") as f:
            try:
                history = json.load(f)
            except json.JSONDecodeError:
                pass
    history.append(log_entry)
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

# --- LangGraph ë…¸ë“œ ---
def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© [1/4] Milvus DBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    question = state["question"]
    start = time.perf_counter()
    try:
        connections.connect(alias="default", uri=MILVUS_URI, token=MILVUS_TOKEN)
        vector_store = Milvus(
            embedding_function=embedding_model,
            collection_name=MILVUS_COLLECTION,
            connection_args={"uri": MILVUS_URI, "token": MILVUS_TOKEN},
        )
        docs_with_scores = vector_store.similarity_search_with_score(question, k=RETRIEVAL_K)

        retrieved_docs_dump = []
        for idx, (doc, score) in enumerate(docs_with_scores, start=1):
            sim = convert_score_to_similarity(float(score))
            full_text = doc.page_content or ""
            preview = full_text[:120].replace("\n", " ")
            print(f"  -> #{idx} raw={float(score):.6f} sim={sim:.4f} | {preview!r}")
            retrieved_docs_dump.append({
                "rank": idx,
                "raw_score": float(score),
                "similarity": float(sim),
                "metadata": getattr(doc, "metadata", {}),
                "text": full_text[:2000]
            })

        # ë‚´ë¶€ DB ì»¨í…ìŠ¤íŠ¸ëŠ” ìƒìœ„ kê°œ í•©ì¹¨
        context_internal = "\n\n".join([d.page_content for d, _ in docs_with_scores])
        elapsed_ms = (time.perf_counter() - start) * 1000.0
        print(f"  -> {len(docs_with_scores)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ. ({elapsed_ms:.1f} ms)")

        return {
            **state,
            "context_internal": context_internal,
            "retrieved_docs": retrieved_docs_dump,
            "retrieval_time_ms": elapsed_ms
        }
    finally:
        if "default" in connections.list_connections():
            connections.disconnect("default")

def generate_rag_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© [2/4] ë‚´ë¶€ DB ì •ë³´ë¡œ 1ì°¨ ë‹µë³€ ìƒì„± ì¤‘...")
    chain = rag_prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": state.get("context_internal", ""), "question": state["question"]})
    no_info = NO_INFO_PHRASE in normalize(answer)
    print("  -> 1ì°¨ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    return {**state, "answer": answer, "answer_source": "ë‚´ë¶€ DB", "no_info": no_info}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© [3/4] Tavily ì›¹ ê²€ìƒ‰ ì¤‘...")
    if not web_search_tool:
        print("  -> Tavily API í‚¤ê°€ ì—†ì–´ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "web_search_results": "Tavily API í‚¤ ì—†ìŒ", "context_web": ""}

    try:
        results = web_search_tool.invoke({"query": state["question"]}) or []
    except Exception as e:
        print(f"  -> Tavily í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        results = []

    lines = []
    for idx, r in enumerate(results, start=1):
        title = r.get("title") or r.get("url") or f"ê²°ê³¼ {idx}"
        url = r.get("url") or ""
        snippet = (r.get("content") or r.get("snippet") or "").strip()
        snippet = re.sub(r"\s+", " ", snippet)[:500]
        lines.append(f"- ì œëª©: {title}\n  URL: {url}\n  ìš”ì•½: {snippet}")

    if not lines:
        search_results = "- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        print("  -> 0ê°œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸.")
    else:
        search_results = "\n".join(lines)
        print(f"  -> {len(results)}ê°œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸.")

    # ì›¹ ì»¨í…ìŠ¤íŠ¸ëŠ” ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ìš”ì•½ ë¬¸ìì—´(ë™ì¼)
    return {**state, "web_search_results": search_results, "context_web": search_results}

def generate_web_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© [4/4] ì›¹ ê²€ìƒ‰ ì •ë³´ë¡œ 2ì°¨ ë‹µë³€ ìƒì„± ì¤‘...")
    chain = web_prompt | llm | StrOutputParser()
    answer = chain.invoke({
        "question": state["question"],
        "search_results": state.get("web_search_results", "- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    })
    print("  -> 2ì°¨ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    return {**state, "answer": answer, "answer_source": "ì›¹ ê²€ìƒ‰", "no_info": NO_INFO_PHRASE in normalize(answer)}

# --- ì¡°ê±´ë¶€ ë¼ìš°íŒ… ---
def should_web_search(state: GraphState) -> str:
    print("ğŸ§­ ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ ì¤‘...")
    answer = state.get("answer", "")
    question = state.get("question", "")
    context_internal = state.get("context_internal", "") or ""
    no_info = state.get("no_info", False)

    if not TAVILY_API_KEY:
        print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ì•ˆ í•¨ (TAVILY_API_KEY ì—†ìŒ)")
        return "end"

    if state.get("force_web"):
        print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (force_web)")
        return "continue"

    if no_info or (NO_INFO_PHRASE in normalize(answer)):
        print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë‚´ë¶€ DBì— ì •ë³´ ì—†ìŒ)")
        return "continue"

    time_pattern = r"(ìµœì‹ |ì˜¤ëŠ˜|ë‰´ìŠ¤|ê°€ê²©|í˜„ì¬|ìµœê·¼|ë³€ê²½|ê°œì •|ë°œí‘œ|ê³µì§€|ì‹¤ì‹œê°„|ì‹œì„¸|ì—…ë°ì´íŠ¸)"
    if re.search(time_pattern, question):
        print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ì‹œê°„ ë¯¼ê° ì§ˆë¬¸)")
        return "continue"

    if len(normalize(context_internal)) < 30:
        print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë¬¸ë§¥ ë¹ˆì•½)")
        return "continue"

    print("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ ì•ˆ í•¨ (ë‚´ë¶€ DB ì •ë³´ë¡œ ì¶©ë¶„)")
    return "end"

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("retrieve", retrieve_node)
    g.add_node("generate_rag", generate_rag_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_web", generate_web_node)

    g.set_entry_point("retrieve")
    g.add_edge("retrieve", "generate_rag")
    g.add_conditional_edges(
        "generate_rag",
        should_web_search,
        {"continue": "web_search", "end": END}
    )
    g.add_edge("web_search", "generate_web")
    g.add_edge("generate_web", END)

    return g.compile()

# =======================
# ëª¨ë“œ 1: ì±„íŒ…
# =======================
def chat(app, log_file: str):
    print("\n=== ì±„íŒ… ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œ: ë¹ˆ ì¤„ Enter ë˜ëŠ” 'exit') ===")
    while True:
        q = input("\nì§ˆë¬¸> ").strip()
        if not q or q.lower() in {"exit", "quit"}:
            break
        try:
            final_state = app.invoke({"question": q, "log_file": log_file})
            answer = final_state.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            source = final_state.get("answer_source", "ì•Œ ìˆ˜ ì—†ìŒ")
            print(f"\nğŸ¤– ë‹µë³€ (ì¶œì²˜: {source}):\n{answer}")
            append_log(final_state)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] {e}")
    print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

# =======================
# ëª¨ë“œ 2: ê³¨ë“ ì…‹ í‰ê°€
# =======================
def _calc_search_similarity_max_from_milvus(retrieved_docs: Optional[List[Dict[str, Any]]]) -> float:
    if not retrieved_docs:
        return 0.0
    sims = [float(x.get("similarity", 0.0)) for x in retrieved_docs]
    return max(sims) if sims else 0.0

def _calc_search_similarity_max_fallback(question: str, retrieved_docs: Optional[List[Dict[str, Any]]]) -> float:
    if not retrieved_docs:
        return 0.0
    sims = []
    for item in retrieved_docs:
        txt = item.get("text", "") or ""
        sims.append(cosine_sim(question, txt))
    return max(sims) if sims else 0.0

def evaluate_goldenset(app, csv_path: str, log_file: str):
    """
    ê³¨ë“ ì…‹ í‰ê°€:
    - answer_similarity: golden_answer â†” generated_answer ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë²¡í„° ê¸°ë°˜)
    - search_similarity_max: Milvus ê²€ìƒ‰ ìœ ì‚¬ë„(ë³€í™˜ê°’) ìµœëŒ“ê°’(ì—†ìœ¼ë©´ ì§ˆë¬¸â†”ë¬¸ì„œ ì½”ì‚¬ì¸ ìµœëŒ€ê°’)
    - sim / similarity: ìµœì¢… ì‚¬ìš© ì»¨í…ìŠ¤íŠ¸ â†” generated_answer ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë²¡í„° ê¸°ë°˜)
      * ìµœì¢… ì¶œì²˜ê°€ 'ë‚´ë¶€ DB'ë©´ context_internal, 'ì›¹ ê²€ìƒ‰'ì´ë©´ context_web ì‚¬ìš©
    - source: ìµœì¢… ë‹µë³€ ì¶œì²˜ ('ë‚´ë¶€ DB' | 'ì›¹ ê²€ìƒ‰')
    """
    # ì´ í•¨ìˆ˜ ì•ˆì—ì„œë§Œ ì“°ëŠ” ë²¡í„° ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    def cosine_similarity(vec1, vec2):
        v1 = np.asarray(vec1, dtype=np.float32)
        v2 = np.asarray(vec2, dtype=np.float32)
        denom = (norm(v1) * norm(v2))
        if denom == 0.0:
            return 0.0
        return float(np.dot(v1, v2) / denom)

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except Exception:
        df = pd.read_csv(csv_path, encoding="cp949")

    rows = df.to_dict("records")
    total = len(rows)
    print(f"\n=== ê³¨ë“ ì…‹ í‰ê°€ ì‹œì‘: {total}ê°œ ìƒ˜í”Œ | ì„ê³„ê°’: {SIM_THRESHOLD} ===")

    results, correct = [], 0

    for i, row in enumerate(rows, start=1):
        q, g = str(row.get("question", "")), str(row.get("answer", ""))
        if not q or not g:
            continue

        print(f"\n[{i}/{total}] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘")
        print(f"ì§ˆë¬¸: {q}")

        # 1ì°¨ ì‹œë„
        used_second = False
        try:
            state1 = app.invoke({"question": q, "log_file": log_file})
            a1 = state1.get("answer", "")
            append_log(state1)
        except Exception as e:
            print(f"  -> âŒ ê·¸ë˜í”„ ì‹¤í–‰ ì˜¤ë¥˜(1ì°¨): {e}")
            state1, a1 = {}, ""

        # ë²¡í„° ì„ë² ë”© í›„ ì½”ì‚¬ì¸ (golden â†” generated)
        golden_vec_1 = embedding_model.embed_query(g) if g else []
        gen_vec_1 = embedding_model.embed_query(a1) if a1 else []
        sim1 = cosine_similarity(golden_vec_1, gen_vec_1)

        src1 = state1.get("answer_source", "N/A")
        no_info1 = state1.get("no_info", False)

        chosen_answer = a1
        chosen_sim = sim1
        chosen_src = src1
        chosen_state = state1

        # 2ì°¨(ì›¹ ê°•ì œ)
        need_second_try = (src1 == "ë‚´ë¶€ DB") and (no_info1 or (sim1 < SIM_THRESHOLD))
        if need_second_try and TAVILY_API_KEY:
            print("  -> 2ì°¨ ì‹œë„: ì›¹ê²€ìƒ‰ ê°•ì œ ì‹¤í–‰(force_web=True)")
            try:
                state2 = app.invoke({"question": q, "log_file": log_file, "force_web": True})
                a2 = state2.get("answer", "")
                append_log(state2)

                golden_vec_2 = golden_vec_1 or (embedding_model.embed_query(g) if g else [])
                gen_vec_2 = embedding_model.embed_query(a2) if a2 else []
                sim2 = cosine_similarity(golden_vec_2, gen_vec_2)

                if sim2 >= chosen_sim:
                    chosen_answer = a2
                    chosen_sim = sim2
                    chosen_src = state2.get("answer_source", "N/A")
                    chosen_state = state2
                    used_second = True
            except Exception as e:
                print(f"  -> âŒ ê·¸ë˜í”„ ì‹¤í–‰ ì˜¤ë¥˜(2ì°¨): {e}")

        # ê²€ìƒ‰ ìœ ì‚¬ë„ ìµœëŒ€ê°’
        search_similarity_max = _calc_search_similarity_max_from_milvus(chosen_state.get("retrieved_docs"))
        if search_similarity_max == 0.0:
            search_similarity_max = _calc_search_similarity_max_fallback(q, chosen_state.get("retrieved_docs"))

        # sim: (ìµœì¢… ì»¨í…ìŠ¤íŠ¸ â†” ìµœì¢… ë‹µë³€)
        ctx_int = chosen_state.get("context_internal", "") or ""
        ctx_web = chosen_state.get("context_web", "") or ""
        if chosen_src == "ë‚´ë¶€ DB":
            chosen_context = ctx_int
        elif chosen_src == "ì›¹ ê²€ìƒ‰":
            chosen_context = ctx_web
        else:
            chosen_context = ctx_int or ctx_web

        ctx_vec = embedding_model.embed_query(chosen_context) if chosen_context else []
        ans_vec = embedding_model.embed_query(chosen_answer) if chosen_answer else []
        sim_ctx_ans = cosine_similarity(ctx_vec, ans_vec)

        ok = chosen_sim >= SIM_THRESHOLD
        if ok:
            correct += 1

        print(f"  -> answer_similarity: {chosen_sim:.4f} | search_similarity_max: {search_similarity_max:.4f}")
        print(f"  -> sim(contextâ†”answer): {sim_ctx_ans:.4f}")
        print(f"  -> í‰ê°€: {'âœ… OK' if ok else 'âŒ FAIL'} | ì¶œì²˜: {chosen_src}{' (2ì°¨)' if used_second else ''}")

        # JSON record (âœ… similarity & source í¬í•¨)
        results.append({
            "index": i,
            "question": q,
            "golden_answer": g,
            "generated_answer": chosen_answer,
            "answer_similarity": float(chosen_sim),           # golden â†” generated
            "search_similarity_max": float(search_similarity_max),
            "sim": float(sim_ctx_ans),                        # ìœ ì§€
            "similarity": float(sim_ctx_ans),                 # ìš”ì²­: ë™ì¼ê°’ìœ¼ë¡œ í‘œê¸°
            "source": chosen_src,                             # ìš”ì²­: 'ë‚´ë¶€ DB' | 'ì›¹ ê²€ìƒ‰'
            "is_correct": bool(ok)
        })

    acc = (correct / total) * 100 if total else 0.0
    print(f"\n=== í‰ê°€ ì™„ë£Œ ===\nì •í™•ë„: {acc:.2f}% ({correct}/{total})")

    report_path = "goldenset_evaluation_report.json"
    report = {"summary": {"total": total, "correct": correct, "accuracy": acc},
              "details": results}
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥: {os.path.abspath(report_path)}")

# =======================
# (ì˜µì…˜) ë³„ë„ ê°„ë‹¨ í‰ê°€ ìœ í‹¸ë“¤
# =======================
def load_golden_dataset(file_path: str):
    """ê³¨ë“  ë°ì´í„°ì…‹ì„ CSV íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(file_path, encoding='utf-8')
    return df.to_dict('records')

def evaluate_chatbot(app, golden_dataset: List[Dict[str, str]]):
    """ê³¨ë“  ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨ ì±—ë´‡ ì„±ëŠ¥ í‰ê°€(JSONì—ëŠ” similarity & source í¬í•¨)."""
    print("\n--- ì±—ë´‡ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (ìœ ì‚¬ë„ ê¸°ë°˜) ---")
    evaluation_results = []
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
    SIMILARITY_THRESHOLD = 0.8

    for i, data in enumerate(golden_dataset):
        question = data['question']
        golden_answer = data['answer']

        print(f"\n[í‰ê°€ {i+1}] ì§ˆë¬¸: {question}")
        print(f"  - ì •ë‹µ: {golden_answer}")

        try:
            final_state = app.invoke({"question": question})
            generated_answer = final_state.get('answer', '')
            src = final_state.get('answer_source', '')
            ctx_int = final_state.get('context_internal', '') or ''
            ctx_web = final_state.get('context_web', '') or ''
            chosen_context = ctx_int if src == "ë‚´ë¶€ DB" else (ctx_web if src == "ì›¹ ê²€ìƒ‰" else (ctx_int or ctx_web))

            # answer_similarity (golden â†” generated) : í…ìŠ¤íŠ¸ ì„ë² ë”© ì½”ì‚¬ì¸
            g_vec = embeddings.embed_query(golden_answer) if golden_answer else []
            a_vec = embeddings.embed_query(generated_answer) if generated_answer else []
            denom = (norm(np.asarray(g_vec)) * norm(np.asarray(a_vec)))
            answer_similarity = float(np.dot(g_vec, a_vec) / denom) if denom else 0.0

            # similarity(sim): ìµœì¢… ì»¨í…ìŠ¤íŠ¸ â†” ë‹µë³€
            c_vec = embeddings.embed_query(chosen_context) if chosen_context else []
            denom2 = (norm(np.asarray(c_vec)) * norm(np.asarray(a_vec)))
            sim_ctx_ans = float(np.dot(c_vec, a_vec) / denom2) if denom2 else 0.0

            is_correct = bool(answer_similarity >= SIMILARITY_THRESHOLD)

            print(f"  - answer_similarity: {answer_similarity:.4f} (ê¸°ì¤€: {SIMILARITY_THRESHOLD})")
            print(f"  - similarity(contextâ†”answer): {sim_ctx_ans:.4f}")
            print(f"  - ì¶œì²˜: {src}")
            print(f"  - ì •ë‹µ ì—¬ë¶€: {'âœ… ì •ë‹µ' if is_correct else 'âŒ ì˜¤ë‹µ'}")

            evaluation_results.append({
                'question': question,
                'golden_answer': golden_answer,
                'generated_answer': generated_answer,
                'answer_similarity': float(answer_similarity),
                'similarity': float(sim_ctx_ans),   # JSONì— í‘œê¸°
                'source': src,                      # JSONì— í‘œê¸°
                'is_correct': is_correct
            })

        except Exception as e:
            print(f"  - ì˜¤ë¥˜ ë°œìƒ: {e}")
            evaluation_results.append({
                'question': question,
                'golden_answer': golden_answer,
                'generated_answer': 'ì˜¤ë¥˜ ë°œìƒ',
                'answer_similarity': 0.0,
                'similarity': 0.0,
                'source': '',
                'is_correct': False
            })

    print("\n--- ì±—ë´‡ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ ---")
    total_questions = len(evaluation_results)
    correct_answers = sum(1 for res in evaluation_results if res['is_correct'])
    accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0
    print(f"\nì´ ì§ˆë¬¸ ìˆ˜: {total_questions}")
    print(f"ì •ë‹µ ìˆ˜: {correct_answers}")
    print(f"ì •í™•ë„: {accuracy:.2f}%")

    with open('evaluation_report.json', 'w', encoding='utf-8') as f:
        json.dump({
            "summary": {"total": total_questions, "correct": correct_answers, "accuracy": accuracy},
            "details": evaluation_results
        }, f, ensure_ascii=False, indent=4)
    print("ìƒì„¸ í‰ê°€ ê²°ê³¼ê°€ 'evaluation_report.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# =======================
# ëª¨ë“œ 3: Ragas í‰ê°€
# =======================
def run_ragas_evaluation():
    print("\n=== Ragas í‰ê°€ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ ===")
    print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    try:
        from datasets import load_dataset
        from ragas import evaluate
        from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision
        from langchain_community.chat_models import ChatOllama
        from langchain_community.embeddings import OllamaEmbeddings
    except ImportError as e:
        print(f"ì˜¤ë¥˜: Ragas ì‹¤í–‰ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ({e})")
        print("pip install -U datasets ragas langchain-community ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return

    print("Amnesty QA ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤ (ìƒ˜í”Œ 2ê°œ)...")
    try:
        amnesty_qa = load_dataset("explodinggradients/amnesty_qa", "english_v2")
        amnesty_subset = amnesty_qa["eval"].select(range(2))
    except Exception as e:
        print(f"ë°ì´í„°ì…‹ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    print("Ollama ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (Ollama ì„œë²„ ì‹¤í–‰ ë° llama3 ëª¨ë¸ í•„ìš”)...")
    try:
        llm = ChatOllama(model="llama3")
        embeddings = OllamaEmbeddings(model="llama3")
        llm.invoke("Hi")
    except Exception as e:
        print(f"Ollama ëª¨ë¸ ì´ˆê¸°í™” ë˜ëŠ” ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        print("Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ê³ , 'ollama pull llama3' ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    print("Ragas í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ë‹¤ì†Œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
    try:
        result = evaluate(
            amnesty_subset,
            metrics=[context_precision, faithfulness, answer_relevancy, context_recall],
            llm=llm,
            embeddings=embeddings,
        )
        print("\n=== Ragas í‰ê°€ ê²°ê³¼ ===")
        print(result)
    except Exception as e:
        print(f"Ragas í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# =======================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =======================
def main():
    print("="*50)
    print("Milvus & ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸")
    print("="*50)
    print("1) ì±„íŒ… ëª¨ë“œ")
    print("2) ê³¨ë“ ì…‹ í‰ê°€ ëª¨ë“œ (ìì²´ ìœ ì‚¬ë„ ê¸°ë°˜)")
    print("3) Ragas í‰ê°€ ëª¨ë“œ (Amnesty QA ë°ì´í„°ì…‹, Ollama í•„ìš”)")
    choice = input("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” [1/2/3]: ").strip()

    if choice in ("1", "2"):
        log_dir = "agent_logs"
        os.makedirs(log_dir, exist_ok=True)
        session_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"log_{session_ts}.json")
        print(f"ì´ë²ˆ ì„¸ì…˜ì˜ ëŒ€í™”ëŠ” {log_file} ì— ê¸°ë¡ë©ë‹ˆë‹¤.")

        app = build_graph()

        if choice == "1":
            chat(app, log_file)
        elif choice == "2":
            evaluate_goldenset(app, GOLDENSET_CSV, log_file)
    elif choice == "3":
        run_ragas_evaluation()
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
