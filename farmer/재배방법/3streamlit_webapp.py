import streamlit as st
import os
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, TypedDict

# Langchain ë° LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.runnables.graph import MermaidDrawMethod # ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ í•„ìš” (ì„ íƒ ì‚¬í•­)

# --- 1. í™˜ê²½ ì„¤ì • ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
if not TAVILY_API_KEY:
    st.error("TAVILY_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë° ë²¡í„° DB ì €ì¥ ê²½ë¡œ
# CSV_FILE_PATHëŠ” ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”.
CSV_FILE_PATH = 'data/ë†ë¦¼ìˆ˜ì‚°ì‹í’ˆêµìœ¡ë¬¸í™”ì •ë³´ì›_ì˜ë†ê°€ì´ë“œ ì¬ë°°ì •ë³´_20230911.csv'
VECTOR_DB_PATH = 'faiss_crop_guide_db'

# data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists('data'):
    os.makedirs('data')

# --- 2. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
llm = ChatGroq(model_name="llama3-70b-8192",
               temperature=0.3,
               api_key=OPENAI_API_KEY)

# ì§ˆë¬¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸
CLASSIFY_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ 'ë†ì—… ë° ì‘ë¬¼ ì¬ë°° ë°©ë²•'ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ë†ì—… ë˜ëŠ” ì‘ë¬¼ ì¬ë°°ì— ê´€ë ¨ì´ ìˆë‹¤ë©´, 'agriculture'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ê·¸ ì™¸ì˜ ëª¨ë“  ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¼ë©´, 'other'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ë‹µë³€ì€ í•œ ë‹¨ì–´ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
classify_prompt = ChatPromptTemplate.from_template(CLASSIFY_PROMPT_TEMPLATE)

# RAG í”„ë¡¬í”„íŠ¸
RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì‘ë¬¼ ì¬ë°° ë°©ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ë‹¤ìŒì€ ì‘ë¬¼ ì¬ë°°ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì œê³µëœ ë‹¤ìŒì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
{context}

ë‹¹ì‹ ì´ ì§€í‚¬ ê·œì¹™ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ì •ë³´, ì €ì˜ ìƒì‹, ì¶”ì¸¡, ê±°ì§“ ì •ë³´, í•œì ë“±ì€ ì ˆëŒ€ ë‹µë³€ì— ë„£ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. í•œê¸€ì´ ì•„ë‹ˆë©´ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³ , í•œê¸€ë¡œë§Œ ë‹µë³€ì„ ì™„ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì¬ë°° ë‹¨ê³„ë‚˜ ì„¤ëª…ì€ ë°˜ë“œì‹œ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ"í•´ì„œ ì¨ì£¼ì„¸ìš”.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì œê³µëœ ì •ë³´ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ë“¯, ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

# ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸
WEB_SEARCH_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”.

ë‹µë³€ ê·œì¹™
1. **ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ**: ì¹œê·¼í•˜ê³  ëª…í™•í•œ ë¬¸ì²´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
2. **ì •ë³´ì˜ ì¶œì²˜ ëª…ì‹œ**: ê²€ìƒ‰ ê²°ê³¼ì— ì œì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ë‹¤ë©´, 'ê²€ìƒ‰ ê²°ê³¼ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëª…í™•í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤.
3. **í•µì‹¬ ìš”ì•½ ë° ì •ë¦¬**: ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¤‘ë³µë˜ëŠ” í•µì‹¬ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
4. **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ**: ë‹µë³€ì€ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë‚ ì§œ, ìˆ«ì, ê¸°ê´€ëª… ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ ì£¼ì„¸ìš”.
5. **í•œê¸€ë¡œë§Œ ë‹µë³€**: ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
web_search_prompt = ChatPromptTemplate.from_template(WEB_SEARCH_PROMPT_TEMPLATE)

# Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
tavily_tool = TavilySearchResults(max_results=5, api_key=TAVILY_API_KEY)

# --- 3. LangGraph ìƒíƒœ ì •ì˜ ---
# LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤. TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœì˜ êµ¬ì¡°ë¥¼ ëª…í™•íˆ í•©ë‹ˆë‹¤.
class GraphState(TypedDict):
    question: Optional[str]
    documents: Optional[List[Dict[str, Any]]]
    splits: Optional[List[Dict[str, Any]]]
    vectorstore: Optional[FAISS]
    context: Optional[str]
    answer: Optional[str]
    search_results: Optional[str]
    classification: Optional[str] # ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥

# --- 4. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---
# ì´ í•¨ìˆ˜ë“¤ì€ LangGraphì˜ ë…¸ë“œë¡œ ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.

def load_csv_data(file_path: str) -> List[Dict[str, Any]]:
    """CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # Streamlitì—ì„œëŠ” ì½˜ì†” ì¶œë ¥ì´ ì‚¬ìš©ìì—ê²Œ ë³´ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•˜ê±°ë‚˜ st.info ë“±ìœ¼ë¡œ ëŒ€ì²´
    # print("---ê¸°ëŠ¥: ë°ì´í„° ë¡œë“œ ì‹œì‘---")
    loader = CSVLoader(file_path=file_path, encoding='utf-8')
    documents = loader.load()
    # print(f"ë¡œë“œëœ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}ê°œ")
    return documents

def split_documents(documents: List[Dict[str, Any]], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict[str, Any]]:
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    # print("---ê¸°ëŠ¥: ë¬¸ì„œ ë¶„í•  ì‹œì‘---")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        is_separator_regex=False,
    )
    splits = text_splitter.split_documents(documents)
    # print(f"ë¶„í• ëœ ì²­í¬ ê°œìˆ˜: {len(splits)}ê°œ")
    return splits

def embed_and_store_vector_db(splits: List[Dict[str, Any]], db_path: str) -> FAISS:
    """ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  ë²¡í„° DBì— ì €ì¥(ë˜ëŠ” ë¡œë“œ)í•©ë‹ˆë‹¤."""
    # print("---ê¸°ëŠ¥: ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ì‹œì‘---")
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

    if not os.path.exists(db_path):
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        vectorstore.save_local(db_path)
        # print(f"ë²¡í„° DBë¥¼ '{db_path}' ê²½ë¡œì— ìƒˆë¡œ ìƒì„± ë° ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        vectorstore = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        # print(f"'{db_path}' ê²½ë¡œì—ì„œ ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vectorstore

def retrieve_relevant_chunks(vectorstore: FAISS, question: str) -> str:
    """ë²¡í„° DBì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    # print("---ê¸°ëŠ¥: ê´€ë ¨ ì²­í¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘---")
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3}) # k=3ìœ¼ë¡œ ì¤„ì—¬ í† í° ì œí•œ ì™„í™”
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    # print(f"ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(docs)}ê°œ")
    return context

def generate_answer_with_llm(context: str, question: str, llm: ChatGroq, prompt_template: ChatPromptTemplate) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # print("---ê¸°ëŠ¥: ë‹µë³€ ìƒì„± ì‹œì‘---")
    rag_chain_internal = (
        {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )
    inputs = {"context": context, "question": question}
    answer = rag_chain_internal.invoke(inputs)
    return answer

# --- 5. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
def load_data_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ë°ì´í„° ë¡œë“œ (Load Data) ì‹¤í–‰---")
    documents = load_csv_data(CSV_FILE_PATH)
    return {**state, "documents": documents}

def split_documents_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ë¬¸ì„œ ë¶„í•  (Split Documents) ì‹¤í–‰---")
    if "documents" not in state or not state["documents"]:
        raise ValueError("ë¬¸ì„œ ë¶„í• ì„ ìœ„í•´ ë¡œë“œëœ ë¬¸ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    splits = split_documents(state["documents"])
    return {**state, "splits": splits}

def embed_and_store_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ (Embed & Store) ì‹¤í–‰---")
    if "splits" not in state or not state["splits"]:
        raise ValueError("ì„ë² ë”©ì„ ìœ„í•´ ë¶„í• ëœ ì²­í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    vectorstore = embed_and_store_vector_db(state["splits"], VECTOR_DB_PATH)
    return {**state, "vectorstore": vectorstore}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ê²€ìƒ‰ (Retrieve) ì‹¤í–‰---")
    if "vectorstore" not in state or not state["vectorstore"]:
        raise ValueError("ê²€ìƒ‰ì„ ìœ„í•´ ë²¡í„° DBê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    question = state["question"]
    vectorstore = state["vectorstore"]
    context = retrieve_relevant_chunks(vectorstore, question)
    return {**state, "context": context}

def generate_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ìƒì„± (Generate) ì‹¤í–‰---")
    if ("context" not in state or not state["context"] or
        "question" not in state or not state["question"]):
        raise ValueError("ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    context = state["context"]
    question = state["question"]
    answer = generate_answer_with_llm(context, question, llm, rag_prompt)
    return {**state, "answer": answer}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ì›¹ ê²€ìƒ‰ (Web Search) ì‹¤í–‰---")
    question = state["question"]
    search_results = tavily_tool.invoke({"query": question})
    search_results_str = "\n".join([str(res) for res in search_results])
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¼ì • í¬ê¸°ë¡œ ì˜ë¼ëƒ„ (í† í° ì œí•œ íšŒí”¼)
    truncated_search_results = search_results_str[:2000] # 2000ìë¡œ ì œí•œ
    # print(f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ {len(truncated_search_results)}ìë¡œ ì˜ë¼ëƒ„.")
    # print(f"ì˜ë¦° ê²€ìƒ‰ ê²°ê³¼: {truncated_search_results[:100]}...")
    web_answer = generate_answer_with_llm(truncated_search_results, question, llm, web_search_prompt)
    return {**state, "answer": web_answer}

def classify_question_node(state: GraphState) -> Dict[str, Any]:
    # print("\n---ë…¸ë“œ: ì§ˆë¬¸ ë¶„ë¥˜ (Classify Question) ì‹¤í–‰---")
    question = state["question"]
    chain = classify_prompt | llm | StrOutputParser()
    classification = chain.invoke({"question": question})
    return {**state, "classification": classification.strip()}

# --- 6. LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ë° ì»´íŒŒì¼ (ì¬ì‚¬ìš©) ---
@st.cache_resource # Streamlit ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±
def build_initial_setup_graph():
    """ì´ˆê¸° ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•ì„ ìœ„í•œ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    initial_builder = StateGraph(GraphState)
    initial_builder.add_node("load_data", load_data_node)
    initial_builder.add_node("split_documents", split_documents_node)
    initial_builder.add_node("embed_and_store", embed_and_store_node)
    initial_builder.set_entry_point("load_data")
    initial_builder.add_edge("load_data", "split_documents")
    initial_builder.add_edge("split_documents", "embed_and_store")
    initial_builder.add_edge("embed_and_store", END)
    return initial_builder.compile()

@st.cache_resource # Streamlit ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±
def build_query_graph():
    """ì§ˆë¬¸ ë¶„ë¥˜, RAG, ì›¹ ê²€ìƒ‰ì„ í†µí•©í•œ ë©”ì¸ ì§ˆì˜ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    query_builder = StateGraph(GraphState)
    
    # ë…¸ë“œ ì¶”ê°€
    query_builder.add_node("classify_question", classify_question_node)
    query_builder.add_node("retrieve", retrieve_node)
    query_builder.add_node("generate", generate_node)
    query_builder.add_node("web_search", web_search_node)
    
    # ì‹œì‘ì  ì„¤ì •
    query_builder.set_entry_point("classify_question")
    
    # ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë¼ìš°íŒ… ë¡œì§ì„ ë³€ê²½í•©ë‹ˆë‹¤.
    def route_classification(state: GraphState):
        if state.get("classification") == "agriculture":
            return "agriculture"
        else:
            return "other"
            
    query_builder.add_conditional_edges(
        "classify_question",
        route_classification,
        {
            "agriculture": "retrieve", # ë†ì—… ê´€ë ¨ ì§ˆë¬¸ -> RAGë¡œ ì´ë™
            "other": "web_search"     # ê¸°íƒ€ ì§ˆë¬¸ -> ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™
        }
    )
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´í›„ ë¬´ì¡°ê±´ ì¢…ë£Œ
    query_builder.add_edge("retrieve", "generate")
    query_builder.add_edge("generate", END)
    
    # ì›¹ ê²€ìƒ‰ í›„ ì¢…ë£Œ
    query_builder.add_edge("web_search", END)
    
    return query_builder.compile()

# --- 7. Streamlit ì•± ë©”ì¸ ë¡œì§ ---
st.set_page_config(page_title="ë†ì—… ì‘ë¬¼ ì±—ë´‡", layout="centered")
st.title("ğŸŒ± ë†ì‘ë¬¼ ì±—ë´‡ ì—ì´ì „íŠ¸")
st.markdown("ê¶ê¸ˆí•œ ë†ì‘ë¬¼ì˜ ì¬ë°° ë° ì¶”ì²œ ë°©ë²•ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!")

# CSSë¥¼ ì‚¬ìš©í•˜ì—¬ st.text í°íŠ¸ë¥¼ Streamlit ê¸°ë³¸ í°íŠ¸ì™€ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
# ì´ëŠ” Streamlit ë‚´ë¶€ í´ë˜ìŠ¤ì— ì˜ì¡´í•˜ë¯€ë¡œ, í–¥í›„ Streamlit ì—…ë°ì´íŠ¸ ì‹œ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
st.markdown("""
<style>
/* ì±—ë´‡ ë‹µë³€ í°íŠ¸ ìŠ¤íƒ€ì¼ í†µì¼ */
.st-chat-message-contents div[data-testid="stText"] {
    font-family: system-ui, -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
    font-size: 1rem; /* ê¸°ë³¸ í°íŠ¸ í¬ê¸°ì™€ ìœ ì‚¬í•˜ê²Œ ì„¤ì • */
    line-height: 1.5; /* ì¤„ ê°„ê²© ì„¤ì • */
    color: var(--text-color); /* Streamlit ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ì‚¬ìš© */
}
/* ì‚¬ìš©ì ì…ë ¥ í°íŠ¸ ìŠ¤íƒ€ì¼ (ê¸°ë³¸ì ìœ¼ë¡œ st.markdownì´ ì²˜ë¦¬) */
.st-chat-message-contents p {
    font-family: system-ui, -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
}
</style>
""", unsafe_allow_html=True)


# LangGraph ì•±ê³¼ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ì—¬ ì•± ìƒˆë¡œê³ ì¹¨ ì‹œ ì¬ì‹¤í–‰ ë°©ì§€
if "rag_app" not in st.session_state:
    with st.spinner("ì±—ë´‡ ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        try:
            # ì´ˆê¸° ì„¤ì • ê·¸ë˜í”„ ì‹¤í–‰ (ë°ì´í„° ë¡œë“œ ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•)
            setup_graph = build_initial_setup_graph()
            initial_state = {"question": "setup"} # ë”ë¯¸ ì§ˆë¬¸
            setup_result = setup_graph.invoke(initial_state)
            st.session_state.vectorstore = setup_result.get("vectorstore") # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
            
            # ë©”ì¸ ì§ˆì˜ ê·¸ë˜í”„ êµ¬ì¶•
            st.session_state.rag_app = build_query_graph()
            
        except Exception as e:
            st.error(f"ì´ˆê¸° ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.stop()

# ì±„íŒ… ê¸°ë¡ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ì„ í™”ë©´ì— í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # AI ë‹µë³€ì€ st.textë¡œ ì¶œë ¥í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ì„œì‹ ë¬´ì‹œ (CSSë¡œ í°íŠ¸ í†µì¼)
        if message["role"] == "assistant":
            st.text(message["content"])
        # ì‚¬ìš©ì ì§ˆë¬¸ì€ st.markdownìœ¼ë¡œ ì¶œë ¥í•˜ì—¬ ê¸°ë³¸ ì„œì‹ ìœ ì§€
        else:
            st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì±—ë´‡ ë‹µë³€ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # ë‹µë³€ì„ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ placeholder
        
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            try:
                # LangGraph ì•± í˜¸ì¶œ (questionê³¼ vectorstore ì „ë‹¬)
                # vectorstoreëŠ” GraphStateì— í¬í•¨ë˜ì–´ì•¼ í•˜ë¯€ë¡œ invoke ì‹œ ì „ë‹¬
                final_state = st.session_state.rag_app.invoke({"question": prompt, "vectorstore": st.session_state.vectorstore})
                
                response = final_state.get('answer', "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                # AI ë‹µë³€ì„ st.textë¡œ ì¶œë ¥í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ì„œì‹ ë¬´ì‹œ
                message_placeholder.text(response) 
                
                # ë‹µë³€ì„ ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"})