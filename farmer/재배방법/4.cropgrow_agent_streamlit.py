import streamlit as st
import os
import re
import requests
import xml.etree.ElementTree as ET
import pandas as pd
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, TypedDict

# Langchain ë° LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_community.tools.tavily_search import TavilySearchResults

# --- 1. í™˜ê²½ ì„¤ì • ---
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
PESTICIDE_API_KEY = os.getenv("PESTICIDE_API_KEY")

# í†µí•©í•  ê°œë³„ ë²¡í„° DB ì €ì¥ ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì´ í´ë”ë“¤ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
DATA_DB_CONFIG = {
    "cultivation": "faiss_crop_guide_db",
    "fertilizer": "faiss_crop_fer_db",
    "pest_disease": "faiss_crop_pest_db",
}

# --- 2. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
# ë‹µë³€ ìƒì„±ì„ ìœ„í•œ LLM
llm = ChatGroq(model_name="llama3-70b-8192",
               temperature=0.7,
               api_key=OPENAI_API_KEY)

# í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ LLM (ë” ê°€ë³ê³  ì •í™•í•œ ëª¨ë¸ ì‚¬ìš©)
llm_keyword = ChatGroq(model_name="llama3-8b-8192",
                       temperature=0.0,
                       api_key=OPENAI_API_KEY)

# ì§ˆë¬¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
CLASSIFY_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì–´ë–¤ ì£¼ì œì— ê´€í•œ ê²ƒì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ 'ë†ì•½'ì— ê´€ë ¨ì´ ìˆë‹¤ë©´, 'pesticide'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ë†ì—…(ë†ì‘ë¬¼ ì¬ë°°, ë¹„ë£Œ, ë³‘í•´ì¶© ë“±)ì— ê´€ë ¨ì´ ìˆë‹¤ë©´, 'agriculture'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ê·¸ ì™¸ì˜ ëª¨ë“  ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¼ë©´, 'other'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ë‹µë³€ì€ í•œ ë‹¨ì–´ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
classify_prompt = ChatPromptTemplate.from_template(CLASSIFY_PROMPT_TEMPLATE)

# RAG í”„ë¡¬í”„íŠ¸ (ë‚´ë¶€ DB ì‚¬ìš©)
RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ë†ì—…ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì œê³µëœ ë‹¤ìŒì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
{context}

ë‹¹ì‹ ì´ ì§€í‚¬ ê·œì¹™ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ì •ë³´, ì €ì˜ ìƒì‹, ì¶”ì¸¡, ê±°ì§“ ì •ë³´, í•œì ë“±ì€ ì ˆëŒ€ ë‹µë³€ì— ë„£ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. í•œê¸€ì´ ì•„ë‹ˆë©´ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³ , í•œê¸€ë¡œë§Œ ë‹µë³€ì„ ì™„ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì¬ë°° ë‹¨ê³„ë‚˜ ì„¤ëª…ì€ ë°˜ë“œì‹œ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ"í•´ì„œ ì¨ì£¼ì„¸ìš”.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì œê³µëœ ì •ë³´ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ë“¯, ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

# ë†ì•½ ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (API ì •ë³´ë§Œ ì‚¬ìš©)
PESTICIDE_RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì™¸ë¶€ APIì—ì„œ ì–»ì€ ë†ì•½ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

# ì™¸ë¶€ APIì—ì„œ ì–»ì€ ì •ë³´:
{api_result}

ë‹¹ì‹ ì´ ì§€í‚¬ ê·œì¹™ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ì •ë³´, ì €ì˜ ìƒì‹, ì¶”ì¸¡, ê±°ì§“ ì •ë³´ ë“±ì€ ì ˆëŒ€ ë‹µë³€ì— ë„£ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. í•œê¸€ì´ ì•„ë‹ˆë©´ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³ , í•œê¸€ë¡œë§Œ ë‹µë³€ì„ ì™„ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì„¤ëª…ì€ ë°˜ë“œì‹œ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ"í•´ì„œ ì¨ì£¼ì„¸ìš”.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì œê³µëœ ì •ë³´ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ íƒœë„ë¡œ, ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
pesticide_rag_prompt = ChatPromptTemplate.from_template(PESTICIDE_RAG_PROMPT_TEMPLATE)

# ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
WEB_SEARCH_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”.

ë‹µë³€ ê·œì¹™
1. **ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ**: ì¹œê·¼í•˜ê³  ëª…í™•í•œ ë¬¸ì²´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
2. **ì •ë³´ì˜ ì¶œì²˜ ëª…ì‹œ**: ê²€ìƒ‰ ê²°ê³¼ì— ì œì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ë‹¤ë©´, 'ê²€ìƒ‰ ê²°ê³¼ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëª…í™•í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤.
3. **í•µì‹¬ ìš”ì•½ ë° ì •ë¦¬**: ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¤‘ë³µë˜ëŠ” í•µì‹¬ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
4. **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ**: ë‹µë³€ì€ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë‚ ì§œ, ìˆ«ì, ê¸°ê´€ëª… ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ ì£¼ì„¸ìš”.
5. **í•œê¸€ë¡œë§Œ ë‹µë³€**: ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
web_search_prompt = ChatPromptTemplate.from_template(WEB_SEARCH_PROMPT_TEMPLATE)

# ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì¶©ë¶„í•œì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
RETRIEVAL_CHECK_PROMPT_TEMPLATE = """
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤.
ì´ ê²€ìƒ‰ëœ ì •ë³´ë§Œìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ ì£¼ì„¸ìš”.

- ì •ë³´ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë‚´ìš©ì´ë¼ë©´ 'sufficient'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ì •ë³´ê°€ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'insufficient'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ë‹µë³€ì€ í•œ ë‹¨ì–´ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ê²€ìƒ‰ëœ ì •ë³´: {context}
ë‹µë³€:
"""
retrieval_check_prompt = ChatPromptTemplate.from_template(RETRIEVAL_CHECK_PROMPT_TEMPLATE)

# í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
KEYWORD_EXTRACT_PROMPT = ChatPromptTemplate.from_template(
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì‘ë¬¼ëª…, ë³‘í•´ì¶©ëª…ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•´. ë‹¤ë¥¸ ë§ì€ í•˜ì§€ë§ˆ.\nì§ˆë¬¸: {question}\ní‚¤ì›Œë“œ:"
)

tavily_tool = TavilySearchResults(max_results=5, api_key=TAVILY_API_KEY)

# --- 3. LangGraph ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    question: Optional[str]
    vectorstore: Optional[FAISS]
    context: Optional[str]
    answer: Optional[str]
    classification: Optional[str]
    retrieval_sufficiency: Optional[str]
    keywords: Optional[str]
    api_result: Optional[str]
    # ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
    answer_source: Optional[str] 

# --- 4. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---
def retrieve_relevant_chunks(vectorstore: FAISS, question: str) -> str:
    """ë²¡í„° DBì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë§¥ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    return context

def generate_answer_with_llm(context: str, question: str, llm: ChatGroq, prompt_template: ChatPromptTemplate) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    rag_chain_internal = (
        {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )
    inputs = {"context": context, "question": question}
    answer = rag_chain_internal.invoke(inputs)
    return answer

# ë†ì•½ API í˜¸ì¶œ í•¨ìˆ˜
def call_pesticide_api(crop_name: str = "", disease_name: str = ""):
    """ë†ì•½ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë†ì•½ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    BASE_URL = "https://psis.rda.go.kr/openApi/service.do"
    if not PESTICIDE_API_KEY:
        return pd.DataFrame()
    params = {
        "apiKey": PESTICIDE_API_KEY,
        "serviceCode": "SVC01",
        "serviceType": "AA001",
        "displayCount": 10,
        "startPoint": 1,
        "cropName": crop_name,
        "diseaseWeedName": disease_name,
        "similarFlag" : "Y",
    }
    try:
        res = requests.get(BASE_URL, params=params)
        res.raise_for_status()
        root = ET.fromstring(res.text)
        rows = []
        for item in root.findall(".//item"):
            rows.append({
                "ì‘ë¬¼ëª…": item.findtext("cropName"),
                "ë³‘í•´ì¶©": item.findtext("diseaseWeedName"),
                "ìš©ë„": item.findtext("useName"),
                "ìƒí‘œëª…": item.findtext("pestiBrandName"),
                "ì‚¬ìš©ë°©ë²•": item.findtext("pestiUse"),
                "í¬ì„ë°°ìˆ˜": item.findtext("dilutUnit"),
                "ì•ˆì „ì‚¬ìš©ê¸°ì¤€(ìˆ˜í™• ì¼ ì „)": item.findtext("useSuittime"),
                "ì•ˆì „ì‚¬ìš©ê¸°ì¤€(íšŒ ì´ë‚´)": item.findtext("useNum"),
            })
        df = pd.DataFrame(rows)
        return df
    except (requests.exceptions.RequestException, ET.ParseError) as e:
        print(f"API í˜¸ì¶œ ë˜ëŠ” XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# --- 5. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
def load_and_merge_dbs_node(state: GraphState) -> Dict[str, Any]:
    """ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°œë³„ ë²¡í„° DBë¥¼ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ë¡œ í†µí•©í•©ë‹ˆë‹¤."""
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    all_dbs_exist = all(os.path.exists(path) for path in DATA_DB_CONFIG.values())

    if not all_dbs_exist:
        st.error("í•„ìš”í•œ ë²¡í„° DB í´ë” ì¤‘ í•˜ë‚˜ ì´ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ê°œë³„ DBë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()
    
    first_db_path = list(DATA_DB_CONFIG.values())[0]
    vectorstore = FAISS.load_local(first_db_path, embeddings, allow_dangerous_deserialization=True)

    for key, db_path in list(DATA_DB_CONFIG.items())[1:]:
        other_vectorstore = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        vectorstore.merge_from(other_vectorstore)
    
    return {**state, "vectorstore": vectorstore}

def classify_question_node(state: GraphState) -> Dict[str, Any]:
    question = state["question"]
    chain = classify_prompt | llm_keyword | StrOutputParser()
    classification = chain.invoke({"question": question})
    classification_str = classification.strip()
    return {**state, "classification": classification_str}

def retrieve_and_check_node(state: GraphState) -> Dict[str, Any]:
    question = state["question"]
    vectorstore = state["vectorstore"]
    context = retrieve_relevant_chunks(vectorstore, question)
    
    chain = retrieval_check_prompt | llm_keyword | StrOutputParser()
    retrieval_sufficiency = chain.invoke({"question": question, "context": context})
    
    return {**state, "context": context, "retrieval_sufficiency": retrieval_sufficiency.strip()}

def generate_node(state: GraphState) -> Dict[str, Any]:
    context = state["context"]
    question = state["question"]
    answer = generate_answer_with_llm(context, question, llm, rag_prompt)
    return {**state, "answer": answer, "answer_source": "internal_db"}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    question = state["question"]
    search_results = tavily_tool.invoke({"query": question})
    search_results_str = "\n".join([str(res) for res in search_results])
    truncated_search_results = search_results_str[:2000]

    web_answer = generate_answer_with_llm(truncated_search_results, question, llm, web_search_prompt)
    
    return {**state, "answer": web_answer, "answer_source": "web_search"}

# --- ë†ì•½ API ê´€ë ¨ ë…¸ë“œ ---
def extract_keywords_node(state: GraphState) -> Dict[str, Any]:
    question = state["question"]
    keyword_chain = KEYWORD_EXTRACT_PROMPT | llm_keyword | StrOutputParser()
    keywords = keyword_chain.invoke({"question": question})
    return {**state, "keywords": keywords}

def call_api_node(state: GraphState) -> Dict[str, Any]:
    keywords = state.get("keywords")
    api_result = "ì™¸ë¶€ APIì—ì„œ ì–»ì€ ì¶”ê°€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    if keywords:
        parsed_keywords = [k.strip() for k in keywords.split(',') if k.strip()]
        crop_name = parsed_keywords[0] if len(parsed_keywords) > 0 else ""
        disease_name = parsed_keywords[1] if len(parsed_keywords) > 1 else ""

        df = call_pesticide_api(crop_name=crop_name, disease_name=disease_name)
        
        if not df.empty:
            api_result = "ì™¸ë¶€ API ê²°ê³¼:\n" + df.to_string(index=False)
        else:
            api_result = "ì™¸ë¶€ API ê²°ê³¼: ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return {**state, "api_result": api_result}

def pesticide_generate_or_websearch_node(state: GraphState) -> Dict[str, Any]:
    question = state["question"]
    api_result = state["api_result"]

    if "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in api_result:
        # API ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ ì›¹ ê²€ìƒ‰ ë…¸ë“œë¡œ ì „í™˜
        # web_search_nodeê°€ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ê·¸ëŒ€ë¡œ ë°˜í™˜
        return web_search_node(state)
    else:
        # API ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ë‹µë³€ ìƒì„±
        pesticide_rag_chain = (
            {"question": RunnablePassthrough(), "api_result": RunnablePassthrough()}
            | pesticide_rag_prompt
            | llm
            | StrOutputParser()
        )
        answer = pesticide_rag_chain.invoke({"question": question, "api_result": api_result})
        return {**state, "answer": answer, "answer_source": "pesticide_api"}

# --- 6. LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ë° ì»´íŒŒì¼ ---
@st.cache_resource
def build_initial_setup_graph():
    """ì´ˆê¸° ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•ì„ ìœ„í•œ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    initial_builder = StateGraph(GraphState)
    initial_builder.add_node("load_and_merge_dbs", load_and_merge_dbs_node)
    initial_builder.set_entry_point("load_and_merge_dbs")
    initial_builder.add_edge("load_and_merge_dbs", END)
    return initial_builder.compile()

@st.cache_resource
def build_query_graph():
    """ì§ˆë¬¸ ë¶„ë¥˜, RAG, ì›¹ ê²€ìƒ‰ì„ í†µí•©í•œ ë©”ì¸ ì§ˆì˜ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    query_builder = StateGraph(GraphState)
    
    query_builder.add_node("classify_question", classify_question_node)
    query_builder.add_node("retrieve_and_check", retrieve_and_check_node)
    query_builder.add_node("generate", generate_node)
    query_builder.add_node("extract_keywords", extract_keywords_node)
    query_builder.add_node("call_api", call_api_node)
    query_builder.add_node("pesticide_generate_or_websearch", pesticide_generate_or_websearch_node)
    query_builder.add_node("web_search", web_search_node)
    
    query_builder.set_entry_point("classify_question")
    
    def route_classification(state: GraphState):
        classification = state.get("classification")
        if classification == "pesticide":
            return "extract_keywords"
        elif classification == "agriculture":
            return "retrieve_and_check"
        else:
            return "web_search"
            
    query_builder.add_conditional_edges(
        "classify_question",
        route_classification,
        {
            "extract_keywords": "extract_keywords",
            "retrieve_and_check": "retrieve_and_check",
            "web_search": "web_search"
        }
    )
    
    def route_retrieval_sufficiency(state: GraphState):
        retrieval_sufficiency = state.get("retrieval_sufficiency")
        return "generate" if retrieval_sufficiency == "sufficient" else "web_search"
            
    query_builder.add_conditional_edges(
        "retrieve_and_check",
        route_retrieval_sufficiency,
        {
            "generate": "generate",
            "web_search": "web_search"
        }
    )
    
    query_builder.add_edge("extract_keywords", "call_api")
    query_builder.add_edge("call_api", "pesticide_generate_or_websearch")
    
    query_builder.add_edge("generate", END)
    query_builder.add_edge("web_search", END)
    query_builder.add_edge("pesticide_generate_or_websearch", END)
    
    return query_builder.compile()

# --- 7. Streamlit ì•± ì‹¤í–‰ ë¡œì§ ---
st.set_page_config(page_title="ğŸŒ± ë†ì‘ë¬¼ ì±—ë´‡ ì—ì´ì „íŠ¸", layout="wide")
st.title("ğŸŒ± ë†ì‘ë¬¼ ì±—ë´‡ ì—ì´ì „íŠ¸")
st.markdown("ê¶ê¸ˆí•œ ë†ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# API í‚¤ ë° DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if not OPENAI_API_KEY or not TAVILY_API_KEY or not PESTICIDE_API_KEY:
    st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
    
# LangGraph ì•±ê³¼ VectorStoreë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©
if "vectorstore" not in st.session_state or "rag_app" not in st.session_state:
    with st.spinner("ì±—ë´‡ ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
        try:
            setup_graph = build_initial_setup_graph()
            setup_result = setup_graph.invoke({"question": "setup"})
            st.session_state.vectorstore = setup_result.get("vectorstore")
            st.session_state.rag_app = build_query_graph()
        except Exception as e:
            st.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.stop()

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            try:
                final_state = st.session_state.rag_app.invoke({
                    "question": prompt,
                    "vectorstore": st.session_state.vectorstore
                })
                
                response = final_state.get('answer', "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                source = final_state.get('answer_source')

                # ë‹µë³€ ì¶œì²˜ì— ë”°ë¼ ë©”ì‹œì§€ ì¶”ê°€
                if source == "web_search":
                    response = "ğŸ’¡ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.\n\n" + response
                elif source == "pesticide_api":
                    response = "ğŸ’¡ ë†ì•½ API ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.\n\n" + response
                else:
                    response = "ğŸ’¡ ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.\n\n" + response

                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")