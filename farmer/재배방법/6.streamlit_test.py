import os
import streamlit as st
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, TypedDict
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnablePassthrough

# --- 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™” ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    # í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

st.title("ğŸŒ± ë†ì‘ë¬¼ ì¬ë°° ì •ë³´ ì±—ë´‡")

# FAISS ë²¡í„° DBê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
VECTOR_DB_PATH = 'faiss_pdf_db' 

# --- 2. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
# ChatGroq LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
llm = ChatGroq(model_name="llama3-70b-8192", 
               temperature=0.7, 
               api_key=OPENAI_API_KEY)

# RAG(Retrieval-Augmented Generation) ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
# chat_history, context, question ë³€ìˆ˜ë¥¼ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë†ì‘ë¬¼ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ì´ì „ ëŒ€í™”:
{chat_history}

ì œê³µëœ ë‹¤ìŒì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
{context}

ì§ˆë¬¸: {question}

ë‹¹ì‹ ì´ ì§€í‚¬ ê·œì¹™ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì§ˆë¬¸ì— ëª¨ë‘ ë¶€í•©í•˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ì •ë³´, ì €ì˜ ìƒì‹, ì¶”ì¸¡, ê±°ì§“ ì •ë³´, í•œì ë“±ì€ ì ˆëŒ€ ë‹µë³€ì— ë„£ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. í•œê¸€ì´ ì•„ë‹ˆë©´ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³ , í•œê¸€ë¡œë§Œ ë‹µë³€ì„ ì™„ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì¬ë°° ë‹¨ê³„ë‚˜ ì„¤ëª…ì€ ë°˜ë“œì‹œ "í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ"í•´ì„œ ì¨ì£¼ì„¸ìš”.
- ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì œê³µëœ ì •ë³´ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ë“¯, ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

# ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
# LLMì´ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ë§ë¶™ì´ì§€ ì•Šë„ë¡ ë§¤ìš° ê°•ë ¥í•˜ê²Œ ì§€ì‹œí•©ë‹ˆë‹¤.
QUERY_TRANSFORM_PROMPT = """
ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ì— ì‚¬ìš©ë  ê°€ì¥ ì ì ˆí•œ ì§ˆë¬¸ì„ **ë‹¨ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ** ìƒì„±í•˜ì„¸ìš”. ì´ ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤.

- ë§Œì•½ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½(context)ì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ì—ë§Œ, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì„ ë³€í™˜í•˜ì„¸ìš”.
- ë§Œì•½ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì „í˜€ ê´€ë ¨ì´ ì—†ëŠ” ìƒˆë¡œìš´ ë‚´ìš©ì´ë¼ë©´, ì˜¤ì§ í˜„ì¬ ì§ˆë¬¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.
- ì ˆëŒ€ë¡œ ë¶ˆí•„ìš”í•œ ì„¤ëª…, ì¸ì‚¬ë§, ë˜ëŠ” ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸: {question}
"""

query_transform_prompt = PromptTemplate.from_template(QUERY_TRANSFORM_PROMPT)

# --- 3. LangGraph ìƒíƒœ ì •ì˜ ---
# LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•œ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class GraphState(TypedDict):
    question: Optional[str]
    context: Optional[str]
    answer: Optional[str]
    vectorstore: Optional[FAISS]
    sources: Optional[List[str]]
    chat_history: Optional[str]

# --- 4. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---
@st.cache_resource
def load_vector_db(db_path: str) -> FAISS:
    """FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    # ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. (ìºì‹±ë˜ì–´ ì¬ì‚¬ìš©ë¨)
    try:
        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    except Exception as e:
        st.error(f"âŒ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê³  VPN/ë°©í™”ë²½ ì„¤ì •ì„ ì ê²€í•´ ì£¼ì„¸ìš”. \n\nì˜¤ë¥˜: {e}")
        st.stop()

    if not os.path.exists(db_path):
        st.error(f"'{db_path}' ê²½ë¡œì— ë²¡í„° DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € DBë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()
    
    # ë¡œì»¬ ê²½ë¡œì—ì„œ FAISS DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    vectorstore = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    return vectorstore

def retrieve_relevant_chunks(vectorstore: FAISS, question: str) -> Dict[str, Any]:
    """ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    # ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œ ë©ì–´ë¦¬(chunk)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    docs = retriever.invoke(question)
    # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
    context = "\n\n".join([doc.page_content for doc in docs])
    # ë¬¸ì„œ ì¶œì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    sources = list(set([doc.metadata.get('source') for doc in docs if doc.metadata.get('source')]))
    return {"context": context, "sources": sources}

# --- 5. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
def retrieve_node(state: GraphState) -> Dict[str, Any]:
    """ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ì§ˆë¬¸ì„ ë³€í™˜í•˜ê³ , ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    question = state["question"]
    chat_history = state["chat_history"]
    vectorstore = state["vectorstore"]
    
    # ì§ˆë¬¸ ë³€í™˜ì„ ìœ„í•œ ì²´ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    query_transform_chain = query_transform_prompt | llm | StrOutputParser()
    
    # LLMì„ í˜¸ì¶œí•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    transformed_question = query_transform_chain.invoke({
        "question": question, 
        "chat_history": chat_history
    })
    
    st.info(f"ğŸ” ê²€ìƒ‰ì„ ìœ„í•œ ì§ˆë¬¸ì„ ë³€í™˜ ì¤‘... (ë³€í™˜ëœ ì§ˆë¬¸: '{transformed_question}')")
    
    # ë²¡í„° DBê°€ ì—†ì„ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if "vectorstore" not in state or not state["vectorstore"]:
        return {"context": None, "sources": [], "question": transformed_question, "chat_history": chat_history}
    
    # ë³€í™˜ëœ ì§ˆë¬¸ìœ¼ë¡œ ë²¡í„° DBì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    retrieval_result = retrieve_relevant_chunks(vectorstore, transformed_question)
    
    return {
        "context": retrieval_result["context"],
        "sources": retrieval_result["sources"],
        "question": question,
        "chat_history": chat_history
    }

def generate_node(state: GraphState) -> Dict[str, Any]:
    """ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤. í˜„ì¬ëŠ” ë”ë¯¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    return {}

# --- 6. LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ë° ì»´íŒŒì¼ ---
@st.cache_resource
def build_rag_workflow():
    """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""
    workflow = StateGraph(GraphState) 
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_node)
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    workflow.set_entry_point("retrieve")
    return workflow.compile()

# --- 7. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
# ì„¸ì…˜ ìƒíƒœì— 'messages'ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state.messages = []
    
try:
    # ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    loaded_vectorstore = load_vector_db(VECTOR_DB_PATH)
except FileNotFoundError as e:
    st.error(f"ì˜¤ë¥˜: {e}")
    st.stop()

# LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.
app = build_rag_workflow()

# RAG ì²´ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
rag_chain = rag_prompt | llm | StrOutputParser()

# ê¸°ì¡´ ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
    if message.get("sources"):
        with st.expander("ì°¸ê³  ìë£Œ ì¶œì²˜"):
            for source in message["sources"]:
                st.markdown(f"- {source}")

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def format_chat_history(messages):
    """ìŠ¤íŠ¸ë¦¼ë¦¿ ë©”ì‹œì§€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    formatted_history = ""
    for msg in messages:
        if msg["role"] == "user":
            formatted_history += f"ì‚¬ìš©ì: {msg['content']}\n"
        else:
            formatted_history += f"ì±—ë´‡: {msg['content']}\n"
    return formatted_history

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    with st.chat_message("assistant"):
        response_container = st.empty()
        try:
            # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            formatted_history = format_chat_history(st.session_state.messages)

            # LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ë‹¨ê³„ì™€ ìƒì„± ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            final_state = app.invoke({
                "question": user_question,
                "vectorstore": loaded_vectorstore,
                "chat_history": formatted_history
            })
            retrieved_context = final_state.get('context')
            retrieved_sources = final_state.get('sources', [])
            
            if not retrieved_context:
                response_text = "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                response_container.markdown(response_text)
            else:
                input_for_prompt = {
                    "context": retrieved_context,
                    "question": user_question,
                    "chat_history": formatted_history
                }
                
                # RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
                response_generator = rag_chain.stream(input_for_prompt)
                response_text = response_container.write_stream(response_generator)
            
            st.session_state.messages.append({"role": "assistant", "content": response_text, "sources": retrieved_sources})
            
            if retrieved_sources:
                with st.expander("ì°¸ê³  ìë£Œ ì¶œì²˜"):
                    for source in retrieved_sources:
                        st.markdown(f"- {source}")
                        
        except Exception as e:
            error_message = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            st.stop()