import os
import pandas as pd
from glob import glob
from typing import List
from dotenv import load_dotenv

from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from pymilvus import utility, connections
from langchain_core.documents import Document

load_dotenv()

# === ì„¤ì • ===
DATA_DIR = "./data/cropinfo" 
COLLECTION_NAME = "crop_grow" # ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ì´ë¦„
EMBED_MODEL_NAME = "jhgan/ko-sroberta-multitask"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

def load_documents() -> List[Document]:
    """
    ì§€ì •ëœ ë””ë ‰í„°ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ì„ Document ê°ì²´ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    all_docs = []
    
    # 1. CSV íŒŒì¼ ë¡œë“œ
    csv_paths = sorted(glob(os.path.join(DATA_DIR, "*.csv")))
    if not csv_paths:
        raise FileNotFoundError(f"'{DATA_DIR}' ë””ë ‰í„°ë¦¬ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    for p in csv_paths:
        try:
            # CSVLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë¡œë“œ
            # source_columnì„ ì§€ì •í•˜ë©´ ë©”íƒ€ë°ì´í„°ì— ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œê°€ ì €ì¥ë©ë‹ˆë‹¤.
            loader = CSVLoader(file_path=p, encoding='utf-8')
            docs = loader.load()
            
            # ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            normalized_path = os.path.normpath(p).replace(os.sep, '/')
            for doc in docs:
                doc.metadata['source'] = normalized_path

            print(f"âœ… CSV íŒŒì¼ ë¡œë“œ: {os.path.basename(p)} (ì´ ë¬¸ì„œ ìˆ˜: {len(docs)})")
            all_docs.extend(docs)
        except Exception as e:
            print(f"â— CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {p} -> ì˜¤ë¥˜: {e}")

    if not all_docs:
        raise FileNotFoundError(f"'{DATA_DIR}' ë””ë ‰í„°ë¦¬ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ê°€ì§„ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    print(f"ğŸ“š ì´ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    return all_docs

def split_documents(documents: List[Document]) -> List[Document]:
    """
    ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", ". ", "? ", "! ", "\n", " "],
    )
    chunks = splitter.split_documents(documents)
    print(f"âœ‚ï¸ ì´ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks

def build_vectorstore(documents: List[Document]) -> None:
    """
    Milvusì— ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    """
    print("ğŸ§  ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ ì¤‘...")

    # ê¸°ì¡´ ì—°ê²° í™•ì¸ ë° ëŠê¸° (ì¤‘ë³µ ì—°ê²° ë°©ì§€)
    if "default" in connections.list_connections():
        connections.disconnect("default")

    # Milvus ì—°ê²° ì„¤ì •
    connections.connect(alias="default", host="localhost", port="19530")
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL_NAME,
        model_kwargs={"device": "cpu"}
    )
    
    # âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ë°ì´í„° ì‚½ì… ë¡œì§
    if utility.has_collection(COLLECTION_NAME):
        print(f"ğŸ”„ '{COLLECTION_NAME}' ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.")
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ì¶”ê°€í•˜ëŠ” ë¡œì§
        vectorstore = Milvus(
            embedding_function=embedding_model,
            collection_name=COLLECTION_NAME,
            connection_args={"host": "localhost", "port": "19530"}
        )
        vectorstore.add_documents(documents=documents)
        print(f"âœ… '{COLLECTION_NAME}' ì»¬ë ‰ì…˜ì— ìƒˆ ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ†• ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}'ì„ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤...")
        
        # Milvus ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        Milvus.from_documents(
            documents=documents,
            embedding=embedding_model,
            collection_name=COLLECTION_NAME,
            connection_args={"host": "localhost", "port": "19530"}
        )
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        docs = load_documents()
        chunks = split_documents(docs)
        build_vectorstore(chunks)
        print("ğŸ‰ ëª¨ë“  ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì–´ Milvusì— ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"â— ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")