import os
import sys
import re
import pandas as pd
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, TypedDict

# Langchain ë° LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
# Milvus ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_community.vectorstores import Milvus as LangChainMilvus
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.retrievers import EnsembleRetriever

# --- 1. í™˜ê²½ ì„¤ì • ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
if not OPENAI_API_KEY:
    print("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()
if not TAVILY_API_KEY:
    print("ì˜¤ë¥˜: TAVILY_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

# Milvus ì—°ê²° ì •ë³´ ë° ì»¬ë ‰ì…˜ ì´ë¦„ ì •ì˜
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
COLLECTION_NAME_INFO = "crop_info"
COLLECTION_NAME_GROW = "crop_grow"

# --- 2. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
# ë‹µë³€ ìƒì„±ì„ ìœ„í•œ LLM
llm = ChatGroq(model_name="llama3-70b-8192",
                temperature=0.7,
                api_key=OPENAI_API_KEY)

# ë³µí•© ì§ˆë¬¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ë†ì•½ ì£¼ì œ ì œê±°)
MULTI_CLASSIFY_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì–´ë–¤ ì£¼ì œì— ê´€í•œ ê²ƒì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì— 'ë†ì‘ë¬¼' ì¬ë°° ë˜ëŠ” ê´€ë¦¬ ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´, 'crop_growth'ë¥¼ í¬í•¨í•˜ì„¸ìš”.
- ê·¸ ì™¸ì˜ ëª¨ë“  ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¼ë©´, 'general'ë¥¼ í¬í•¨í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ì£¼ì œê°€ í¬í•¨ëœ ê²½ìš°, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
- ë‹µë³€ì€ ì˜¤ì§ ì£¼ì œ í‚¤ì›Œë“œë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
multi_classify_prompt = ChatPromptTemplate.from_template(MULTI_CLASSIFY_PROMPT_TEMPLATE)

# DB + ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ (API ë¶€ë¶„ ì œê±°)
DB_AND_WEB_SEARCH_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”.

# DB ê²€ìƒ‰ ê²°ê³¼:
{db_context}

# ì›¹ ê²€ìƒ‰ ê²°ê³¼:
{web_search_results}

ë‹µë³€ ê·œì¹™
1. **ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ**: ì¹œê·¼í•˜ê³  ëª…í™•í•œ ë¬¸ì²´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
2. **ì •ë³´ì˜ ì¶œì²˜ ëª…ì‹œ**: DBì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ì œì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ë‹¤ë©´, 'ê²€ìƒ‰ ê²°ê³¼ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëª…í™•í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤.
3. **í•µì‹¬ ìš”ì•½ ë° ì •ë¦¬**: ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¤‘ë³µë˜ëŠ” í•µì‹¬ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
4. **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ**: ë‹µë³€ì€ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë‚ ì§œ, ìˆ«ì, ê¸°ê´€ëª… ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ ì£¼ì„¸ìš”.
5. **í•œê¸€ë¡œë§Œ ë‹µë³€**: ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
6. **ë‚´ë¶€ DB ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©**: ë‚´ë¶€ DBì— ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ì„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
db_and_web_search_prompt = ChatPromptTemplate.from_template(DB_AND_WEB_SEARCH_PROMPT_TEMPLATE)

tavily_tool = TavilySearchResults(max_results=5, api_key=TAVILY_API_KEY)

# --- 3. LangGraph ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    question: Optional[str]
    retriever: Optional[EnsembleRetriever]
    answer: Optional[str]
    topics: Optional[List[str]]
    db_context: Optional[str]
    web_sources: Optional[List[Dict[str, Any]]]
    db_sources: Optional[List[Dict[str, Any]]]

# --- 4. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---
def create_retriever() -> EnsembleRetriever:
    """ë‘ ê°œì˜ Milvus ì»¬ë ‰ì…˜ì— ì—°ê²°í•˜ì—¬ EnsembleRetrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("---ê¸°ëŠ¥: Milvus ì»¬ë ‰ì…˜ ì—°ê²° ë° EnsembleRetriever ìƒì„± ì‹œì‘---")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

        vectorstore_info = LangChainMilvus(
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME_INFO,
            connection_args={"host": MILVUS_HOST, "port": MILVUS_PORT},
            consistency_level="Bounded"
        )
        print(f"âœ… '{COLLECTION_NAME_INFO}' ì»¬ë ‰ì…˜ì— ì—°ê²°í–ˆìŠµë‹ˆë‹¤.")

        vectorstore_grow = LangChainMilvus(
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME_GROW,
            connection_args={"host": MILVUS_HOST, "port": MILVUS_PORT},
            consistency_level="Bounded"
        )
        print(f"âœ… '{COLLECTION_NAME_GROW}' ì»¬ë ‰ì…˜ì— ì—°ê²°í–ˆìŠµë‹ˆë‹¤.")

        retriever_info = vectorstore_info.as_retriever(search_kwargs={"k": 3})
        retriever_grow = vectorstore_grow.as_retriever(search_kwargs={"k": 3})

        ensemble_retriever = EnsembleRetriever(
            retrievers=[retriever_info, retriever_grow],
            weights=[0.5, 0.5]
        )
        
        print("âœ… EnsembleRetrieverê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return ensemble_retriever
    except Exception as e:
        print(f"Milvus ì—°ê²° ë˜ëŠ” EnsembleRetriever ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def retrieve_relevant_chunks(retriever: EnsembleRetriever, question: str) -> Dict[str, Any]:
    """EnsembleRetrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ì»¬ë ‰ì…˜ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    db_sources = [{"source": doc.metadata.get('source'), "page": doc.metadata.get('page'), "content": doc.page_content} for doc in docs]
    print(f"ê²€ìƒ‰ëœ ì´ ì²­í¬ ìˆ˜: {len(docs)}ê°œ")
    return {"context": context, "db_sources": db_sources}

# --- 5. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
def load_and_merge_dbs_node(state: GraphState) -> Dict[str, Any]:
    """Milvusì˜ EnsembleRetrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n---ë…¸ë“œ: Milvus EnsembleRetriever ìƒì„± ì‹¤í–‰---")
    retriever = create_retriever()
    print("Milvus EnsembleRetriever ë¡œë“œ ì™„ë£Œ.\n")
    return {**state, "retriever": retriever}

def multi_classify_question_node(state: GraphState) -> Dict[str, Any]:
    print("\n---ë…¸ë“œ: ë³µí•© ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤í–‰---")
    question = state["question"]
    chain = multi_classify_prompt | llm | StrOutputParser()
    classification_str = chain.invoke({"question": question}).strip()
    topics = [topic.strip() for topic in classification_str.split(',') if topic.strip()]
    print(f"ì§ˆë¬¸ì´ ë‹¤ìŒ ì£¼ì œë“¤ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤: {topics}")
    return {**state, "topics": topics}
    
def process_topics_and_retrieve_content_node(state: GraphState) -> Dict[str, Any]:
    print("\n---ë…¸ë“œ: ì£¼ì œë³„ ì •ë³´ ê²€ìƒ‰ ë° í†µí•© ì‹¤í–‰---")
    question = state["question"]
    topics = state.get("topics", [])
    
    db_context = ""
    db_sources = []
    web_sources = []

    # 1. DB ê²€ìƒ‰ (ë†ì‘ë¬¼ ì¬ë°° ê´€ë ¨)
    if "crop_growth" in topics:
        print("ğŸ” 'ë†ì‘ë¬¼ ì¬ë°°' ì£¼ì œ ê´€ë ¨ DB ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        retrieval_result = retrieve_relevant_chunks(state["retriever"], question)
        db_context = retrieval_result["context"]
        db_sources = retrieval_result["db_sources"]
        print("âœ… DB ê²€ìƒ‰ ì™„ë£Œ.")

    # 2. ì›¹ ê²€ìƒ‰ (DB ì™¸ ì¼ë°˜ ì •ë³´ ë˜ëŠ” ë³´ì¶© ì •ë³´)
    # 'general' ì£¼ì œê°€ ìˆê±°ë‚˜, DB ê²°ê³¼ê°€ ë¶€ì¡±í•  ê²½ìš° ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰
    if "general" in topics or not db_context:
        print("ğŸŒ 'ì¼ë°˜' ì£¼ì œ ë˜ëŠ” ì •ë³´ ë³´ì¶©ì„ ìœ„í•œ ì›¹ ê²€ìƒ‰ ì¤‘...")
        search_results = tavily_tool.invoke({"query": question})
        web_sources = [{"url": res["url"], "content": res["content"]} for res in search_results]
        print("âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ.")
    
    return {**state, "db_context": db_context, "db_sources": db_sources, "web_sources": web_sources}

def generate_final_answer_node(state: GraphState) -> Dict[str, Any]:
    print("\n---ë…¸ë“œ: ìµœì¢… ë‹µë³€ ìƒì„± ì‹¤í–‰---")
    question = state["question"]
    db_context = state.get("db_context", "ë‚´ë¶€ DBì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    web_search_results = "\n".join([str(res) for res in state.get("web_sources", [])])
    
    if not web_search_results:
        web_search_results = "ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    inputs = {
        "question": question,
        "db_context": db_context,
        "web_search_results": web_search_results
    }
    
    final_chain = db_and_web_search_prompt | llm | StrOutputParser()
    answer = final_chain.invoke(inputs)
    return {**state, "answer": answer}

# --- 6. LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ë° ì»´íŒŒì¼ ---
def build_initial_setup_graph():
    """ì´ˆê¸° ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•ì„ ìœ„í•œ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    initial_builder = StateGraph(GraphState)
    initial_builder.add_node("load_and_merge_dbs", load_and_merge_dbs_node)
    initial_builder.set_entry_point("load_and_merge_dbs")
    initial_builder.add_edge("load_and_merge_dbs", END)
    return initial_builder.compile()

def build_query_graph():
    """ì§ˆë¬¸ ë¶„ë¥˜, RAG, ì›¹ ê²€ìƒ‰ì„ í†µí•©í•œ ë©”ì¸ ì§ˆì˜ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    query_builder = StateGraph(GraphState)
    
    # ìƒˆë¡œìš´ ë…¸ë“œë“¤ ì¶”ê°€
    query_builder.add_node("multi_classify_question", multi_classify_question_node)
    query_builder.add_node("process_topics_and_retrieve_content", process_topics_and_retrieve_content_node)
    query_builder.add_node("generate_final_answer", generate_final_answer_node)

    # ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ìˆœí•˜ê²Œ ì—°ê²°
    query_builder.set_entry_point("multi_classify_question")
    query_builder.add_edge("multi_classify_question", "process_topics_and_retrieve_content")
    query_builder.add_edge("process_topics_and_retrieve_content", "generate_final_answer")
    query_builder.add_edge("generate_final_answer", END)
    
    return query_builder.compile()

# --- 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    print("ğŸŒ± ë†ì‘ë¬¼ ì±—ë´‡ ì—ì´ì „íŠ¸ ì‹œì‘...")
    print("--------------------------------------------------")
    
    print("ì±—ë´‡ ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... (Milvus EnsembleRetriever ìƒì„±)")
    setup_graph = build_initial_setup_graph()
    initial_state = {"question": "setup"}
    try:
        setup_result = setup_graph.invoke(initial_state)
        retriever = setup_result.get("retriever")
    except Exception as e:
        print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        exit()
        
    print("ì±—ë´‡ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")
    
    rag_app = build_query_graph()

    print("ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥)")
    print("--------------------------------------------------")

    while True:
        prompt = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if prompt.lower() in ["exit", "quit"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        print("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
        try:
            # 'retriever' ê°ì²´ë¥¼ ìƒíƒœì— ì „ë‹¬í•©ë‹ˆë‹¤.
            final_state = rag_app.invoke({"question": prompt, "retriever": retriever})
            response = final_state.get('answer', "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # --- ì°¸ê³  ìë£Œ ì¶œë ¥ ë¡œì§ ì¶”ê°€ ---
            db_sources = final_state.get('db_sources', [])
            web_sources = final_state.get('web_sources', [])

            print("\n------------------- ë‹µë³€ -------------------")
            print(response)
            print("-------------------------------------------\n")

            if db_sources:
                print("--- ì°¸ê³ í•œ DB ë‚´ìš© ---")
                for i, source in enumerate(db_sources, 1):
                    file_name = os.path.basename(source.get('source', '')).rsplit('.', 1)[0]
                    page_num = source.get('page')
                    print(f"**[{i}]** ì¶œì²˜: {file_name}", end="")
                    if page_num is not None:
                        print(f", í˜ì´ì§€: {page_num + 1}", end="")
                    print(f"\në‚´ìš©: {source.get('content', 'ë‚´ìš© ì—†ìŒ')[:100]}...\n")
            
            if web_sources:
                print("--- ì°¸ê³ í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ---")
                for i, source in enumerate(web_sources, 1):
                    print(f"**[{i}]** URL: {source.get('url', 'URL ì—†ìŒ')}")
                    print(f"ë‚´ìš©: {source.get('content', 'ë‚´ìš© ì—†ìŒ')[:100]}...\n")

            print("-------------------------------------------\n")

        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")