# -*- coding: utf-8 -*-
import os
import re
import json
from typing import TypedDict, Optional, Any, Dict, List
from datetime import datetime, timedelta
from operator import itemgetter
from argparse import ArgumentParser

import numpy as np
import pandas as pd
import faiss
import requests

from dotenv import load_dotenv
load_dotenv()

# =========[ ÌôòÍ≤ΩÏÑ§Ï†ï ]=========
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
VECTOR_DB_PATH   = os.getenv("VECTOR_DB_PATH", "faiss_kma_db")   # LangChain-FAISS ÎîîÎ†âÌÜ†Î¶¨ ÌïÑÏàò
GROQ_API_KEY     = os.getenv("GROQ_API_KEY")                     # ÌïÑÏàò
GROQ_MODEL       = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE      = float(os.getenv("TEMPERATURE", "0.2"))

# KMA
WHEATHER_API_KEY_HUB = os.getenv("WHEATHER_API_KEY_HUB")
KMA_TIMEOUT = int(os.getenv("KMA_TIMEOUT", "10"))

# ‚úÖ ÎùºÏù¥Î∏å ÌäπÎ≥¥ Ìò∏Ï∂ú ÌÜ†Í∏Ä (.env: USE_KMA_LIVE=true/false)
USE_KMA_LIVE = os.getenv("USE_KMA_LIVE", "true").lower() in ("1", "true", "yes")
FORCE_DISABLE_KMA_LIVE = False  # Í∞úÎ∞ú Ï§ë Í∞ïÏ†ú OFFÌïòÎ†§Î©¥ True

# =========[ LangChain/LangGraph/LLM ]=========
from langchain_huggingface import HuggingFaceEmbeddings   # ‚úÖ ÎîîÌîÑÎ¶¨ÏºÄÏù¥ÏÖò ÎåÄÏùë
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from sentence_transformers import SentenceTransformer

# =========[ Îß§Ìïë/Ïú†Ìã∏ ]=========
WRN_MAP = {"T":"ÌÉúÌíç","W":"Í∞ïÌíç","R":"Ìò∏Ïö∞","C":"ÌïúÌåå","D":"Í±¥Ï°∞","O":"Ìï¥Ïùº","N":"ÏßÄÏßÑÌï¥Ïùº","V":"ÌíçÎûë","S":"ÎåÄÏÑ§","Y":"Ìô©ÏÇ¨","H":"Ìè≠Ïóº","F":"ÏïàÍ∞ú"}
LVL_MAP = {"1":"ÏòàÎπÑÌäπÎ≥¥","2":"Ï£ºÏùòÎ≥¥","3":"Í≤ΩÎ≥¥"}
CMD_MAP = {"1":"Î∞úÌëú","2":"ÎåÄÏπò","3":"Ìï¥Ï†ú","4":"ÎåÄÏπòÌï¥Ï†ú","5":"Ïó∞Ïû•","6":"Î≥ÄÍ≤Ω","7":"Î≥ÄÍ≤ΩÌï¥Ï†ú"}
REGION_CODE_RE = re.compile(r"^[A-Z]\d{7}$")

# ‚¨áÔ∏è ÏµúÏÜå Í∏∞Î≥∏Í∞í. ÌïÑÏöî Ïãú CSVÎ°ú Ïò§Î≤ÑÎùºÏù¥Îìú
REGION_MAP = {}

def _load_region_map_from_csv():
    path = os.getenv("REGION_MAP_CSV")
    if not path or not os.path.exists(path):
        return
    try:
        df = pd.read_csv(path)
        code_col = next((c for c in df.columns if c.lower() in ("code","region_code","ÏßÄÏó≠ÏΩîÎìú")), None)
        name_col = next((c for c in df.columns if c.lower() in ("name","region_name","ÏßÄÏó≠Î™Ö")), None)
        if not code_col or not name_col:
            return
        for _, r in df.iterrows():
            code = str(r[code_col]).strip()
            name = str(r[name_col]).strip()
            if code and name:
                REGION_MAP[code] = name
    except Exception:
        pass

_load_region_map_from_csv()

def resolve_region(token: str) -> str:
    if not token: return "N/A"
    t = token.strip()
    return REGION_MAP.get(t, t)

def fmt_kst(yyyymmddHHMM: str) -> str:
    try:
        dt = datetime.strptime(yyyymmddHHMM, "%Y%m%d%H%M")
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return yyyymmddHHMM

def l2_normalize(vec: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(vec)
    return vec / n if n > 0 else vec

def minmax_norm(scores: List[float]) -> List[float]:
    if not scores: return []
    lo, hi = min(scores), max(scores)
    if hi - lo < 1e-8: return [0.0 for _ in scores]
    return [(s - lo) / (hi - lo) for s in scores]

# =========[ KMA Í∞ÄÏ†∏Ïò§Í∏∞/ÏöîÏïΩ ]=========
def _format_kma_record(raw: List[str]) -> Dict[str, str]:
    tm_st = raw[0] if len(raw)>0 else "N/A"
    tm_ed = raw[1] if len(raw)>1 else "N/A"
    reg_token = raw[4].strip() if len(raw)>4 else "N/A"
    wrn = (raw[5].strip() if len(raw)>5 else "")
    lvl = (raw[6].strip() if len(raw)>6 else "")
    cmd = (raw[7].strip() if len(raw)>7 else "")
    grd = (raw[8].strip() if len(raw)>8 else "")

    region_name = resolve_region(reg_token)
    payload = {
        "source":"KMA",
        "region_raw": reg_token,
        "region_name": region_name,
        "region_type":"code" if REGION_CODE_RE.match(reg_token or "") else "name",
        "hazard_code": wrn,
        "hazard_name": WRN_MAP.get(wrn, "ÏïåÏàòÏóÜÏùå"),
        "level_code": lvl,
        "level_name": LVL_MAP.get(lvl, "N/A"),
        "command_code": cmd,
        "command_name": CMD_MAP.get(cmd, cmd),
        "window_start": tm_st,
        "window_end": tm_ed,
        "window_start_kst": fmt_kst(tm_st) if tm_st!="N/A" else "N/A",
        "window_end_kst": fmt_kst(tm_ed) if tm_ed!="N/A" else "N/A",
        "announce_time_kst": fmt_kst(tm_st) if cmd=="1" and tm_st!="N/A" else None,
    }
    if wrn=="T" and grd:
        payload["typhoon_grade"] = grd

    time_bits = []
    if payload["window_start_kst"]!="N/A" and payload["window_end_kst"]!="N/A":
        time_bits.append(f"Í∏∞Í∞Ñ: {payload['window_start_kst']} ~ {payload['window_end_kst']}")
    elif payload["window_start_kst"]!="N/A":
        time_bits.append(f"ÏãúÍ∞Å: {payload['window_start_kst']}")
    if payload["announce_time_kst"]:
        time_bits.append(f"Î∞úÌëúÏãúÍ∞Å: {payload['announce_time_kst']}")

    parts = [
        f"ÏßÄÏó≠: {region_name} ({reg_token})",
        *time_bits,
        f"Ï¢ÖÎ•ò: {payload['hazard_name']}({payload['hazard_code']})",
        f"ÏàòÏ§Ä: {payload['level_name']}({payload['level_code']})",
        f"Î™ÖÎ†π: {payload['command_name']}({payload['command_code']})"
    ]
    if "typhoon_grade" in payload:
        parts.append(f"ÌÉúÌíç Îì±Í∏â: {payload['typhoon_grade']}")

    return {
        "json": json.dumps(payload, ensure_ascii=False, separators=(",", ":")),
        "human": " | ".join(parts)
    }

def fetch_kma_alerts(start_time: str, end_time: str, disp: str="1") -> List[Dict[str, str]]:
    if not WHEATHER_API_KEY_HUB:
        return []
    base = "https://apihub.kma.go.kr/api/typ01/url/wrn_met_data.php"
    params = {"authKey":WHEATHER_API_KEY_HUB, "wrn":"", "tmfc1":start_time, "tmfc2":end_time, "disp":disp}
    r = requests.get(base, params=params, timeout=KMA_TIMEOUT)
    r.raise_for_status()
    text = r.content.decode("euc-kr", errors="ignore")

    docs, seen = [], set()
    for line in [ln for ln in text.strip().split("\n") if ln.strip() and not ln.startswith("#") and not ln.startswith("7777END")]:
        raw = line.strip().rstrip("=").split(",")
        if len(raw) < 9:
            continue
        rec = _format_kma_record(raw)
        key = re.sub(r"\s+", " ", rec["json"]).strip()
        if key in seen:
            continue
        seen.add(key)
        docs.append(rec)
    return docs

# =========[ ÏßÄÏó≠ ÌååÏã±/ÏöîÏïΩ Ìó¨Ìçº ]=========
def _reverse_region_map() -> Dict[str, List[str]]:
    rev: Dict[str, List[str]] = {}
    for code, name in REGION_MAP.items():
        rev.setdefault(name, []).append(code)
        alias = name.replace(" ", "")
        rev.setdefault(alias, []).append(code)
    return rev

def extract_region_from_question(q: str) -> Optional[str]:
    if not q:
        return None
    q_norm = re.sub(r"\s+", "", q)
    cand_names = set(REGION_MAP.values())
    for nm in cand_names:
        if nm in q or nm.replace(" ", "") in q_norm:
            return nm
    commons = ["Ïö∏ÏßÑÍµ∞", "Ïö∏ÏßÑ", "ÏòÅÎçïÍµ∞", "ÏòÅÎçï", "Î∂ÄÏÇ∞ÎèôÎ∂Ä", "Ïö∏ÏÇ∞ÎèôÎ∂Ä", "Í±∞Ï†úÏãú", "ÎÇ®Ìï¥Íµ∞", "Ï†úÏ£ºÎèÑÎ∂ÅÎ∂Ä", "Ï†úÏ£ºÎ∂ÅÎ∂Ä", "ÏÇ¨Ï≤úÏãú", "ÌååÏ£ºÏãú"]
    for token in commons:
        if token in q or token.replace(" ", "") in q_norm:
            for code, name in REGION_MAP.items():
                if token.replace(" ", "") in name.replace(" ", ""):
                    return name
    return None

def summarize_region_alert(q: str, live_docs: List[Dict[str, str]]) -> str:
    """ÏßàÎ¨∏ÏóêÏÑú ÏßÄÏó≠ÏùÑ ÎΩëÏïÑ Îß§Ìïë Ïã§Ìå®/ÌäπÎ≥¥ ÏóÜÏùå/ÌäπÎ≥¥ ÏûàÏùå Î¨∏Íµ¨Î•º Í≤∞Ï†ïÏ†ÅÏúºÎ°ú ÎßåÎì§Ïñ¥Ï§å."""
    region_name = extract_region_from_question(q)
    if not region_name:
        return ""  # ÏßÄÏó≠ Ïñ∏Í∏â ÏóÜÏúºÎ©¥ ÌûåÌä∏ ÏÉùÎûµ

    rev = _reverse_region_map()
    target_codes = rev.get(region_name, [])
    if not target_codes:
        return f"[LIVE_STATUS] {region_name}Ïùò ÌäπÎ≥¥ Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÏßÄÏó≠ ÏΩîÎìú Îß§ÌïëÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."

    matched = []
    for d in live_docs or []:
        try:
            payload = json.loads(d["json"])
            if payload.get("region_raw") in target_codes:
                matched.append(payload)
        except Exception:
            continue

    if not matched:
        return f"[LIVE_STATUS] Ïò§Îäò {region_name}ÏóêÎäî Î∞úÌö® Ï§ëÏù∏ ÌäπÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§. ÌèâÏÜåÏôÄ Í∞ôÏù¥ Í¥ÄÎ¶¨ÌïòÏãúÎ©¥ Îê©ÎãàÎã§."

    matched.sort(key=lambda x: x.get("window_start", ""), reverse=True)
    p = matched[0]
    region = p.get("region_name", region_name)
    hz = p.get("hazard_name", "ÌäπÎ≥¥")
    lvl = p.get("level_name", "")
    cmd = p.get("command_name", "")
    st  = p.get("window_start_kst", "")
    ed  = p.get("window_end_kst", "")
    bits = [f"{region}ÏóêÎäî {hz} {lvl}Í∞Ä"]
    if st and ed:
        bits.append(f"{st}Ïóê Î∞úÌö®ÎêòÏñ¥ {ed}Ïóê Ìï¥Ï†úÎê©ÎãàÎã§.")
    elif st:
        bits.append(f"{st}Ïóê Î∞úÌö®ÎêòÏóàÏäµÎãàÎã§.")
    else:
        bits.append("Î∞úÌö® Ï§ëÏûÖÎãàÎã§.")
    if cmd:
        bits.append(f"(Î™ÖÎ†π: {cmd})")
    return "[LIVE_STATUS] " + " ".join(bits)

# =========[ ÏûÑÎ≤†Îî© (ÎùºÏù¥Î∏å Í≤ÄÏÉâÏö©) ]=========
_text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
def embed_texts(texts: List[str]) -> np.ndarray:
    embs = _text_embedder.encode(texts, show_progress_bar=False)
    embs = np.array([l2_normalize(e) for e in embs], dtype="float32")
    return embs

# =========[ ÏÉÅÌÉú/ÌîÑÎ°¨ÌîÑÌä∏/LLM ]=========
class GraphState(TypedDict):
    question: Optional[str]
    context: Optional[str]
    answer: Optional[str]
    store_obj: Optional[Any]

def make_llm() -> ChatGroq:
    if not GROQ_API_KEY:
        raise ValueError("GROQ_API_KEYÍ∞Ä .envÏóê ÏóÜÏäµÎãàÎã§.")
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

# =========[ ÏßàÏùò ÏùòÎèÑ Í∞êÏßÄ ]=========
def _should_use_live(q: str) -> bool:
    if FORCE_DISABLE_KMA_LIVE or not USE_KMA_LIVE:
        return False
    q = (q or "")
    live_kw = ["ÌòÑÏû¨", "ÏßÄÍ∏à", "Ïò§Îäò", "ÌäπÎ≥¥", "Ïã§ÏãúÍ∞Ñ", "Í≤ΩÎ≥¥", "Ï£ºÏùòÎ≥¥", "Ìï¥Ï†ú", "Î∞úÌëú"]
    return any(k in q for k in live_kw)

# =========[ LangGraph ÎÖ∏Îìú ]=========
def load_store_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú (LangChain-FAISS)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True}
    )
    vs = LCFAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return {**state, "store_obj": vs}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: Í≤ÄÏÉâ(ÏßÄÏÜç LC + KMA ÎùºÏù¥Î∏å Î≥ëÌï©)")
    q = state["question"] or ""
    live_enabled = _should_use_live(q)

    # 1) Persist Í≤ÄÏÉâ (ÌõÑÏàúÏúÑ Í∞ÄÏ§ëÏπò)
    retriever = state["store_obj"].as_retriever(search_type="similarity", search_kwargs={"k": 5})
    docs = retriever.invoke(q)
    persist_scored = [(0.2, d.page_content, "persist") for d in docs]

    # 2) Live(KMA) Í≤ÄÏÉâ ‚Äî Ïã§ÏãúÍ∞Ñ ÏùòÎèÑÏùº ÎïåÎßå Ìò∏Ï∂ú
    live_scored: List[tuple] = []
    live_docs: List[Dict[str, str]] = []
    live_status_msg = ""
    if live_enabled:
        try:
            now = datetime.now()
            tm1 = (now - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0).strftime("%Y%m%d%H%M")
            tm2 = now.strftime("%Y%m%d%H%M")
            live_docs = fetch_kma_alerts(tm1, tm2) if WHEATHER_API_KEY_HUB else []

            # üü¢ ÏßàÎ¨∏ ÏÜç ÏßÄÏó≠ Í∏∞Ï§ÄÏúºÎ°ú: Îß§Ìïë Ïã§Ìå®/ÌäπÎ≥¥ ÏóÜÏùå/ÌäπÎ≥¥ ÏûàÏùå Íµ¨Î∂Ñ Î©îÏãúÏßÄ ÏÉùÏÑ±
            live_status_msg = summarize_region_alert(q, live_docs)

            if live_docs:
                human_texts = [d["human"] for d in live_docs]
                embs = embed_texts(human_texts)
                live_idx = faiss.IndexFlatIP(embs.shape[1])
                live_idx.add(embs)
                qv = embed_texts([q])[0]
                topk = min(5, len(live_docs))
                D, I = live_idx.search(np.array([qv], dtype="float32"), topk)
                for s, i in zip(D[0], I[0]):
                    if i == -1:
                        continue
                    ctx = f"JSON: {live_docs[i]['json']}\nÏöîÏïΩ: {live_docs[i]['human']}"
                    live_scored.append((float(s) + 1.0, ctx, "live"))  # ÎùºÏù¥Î∏å Í∞ÄÏÇ∞Ï†ê
        except Exception:
            pass
    else:
        print("‚ÑπÔ∏è KMA ÎùºÏù¥Î∏å Ìò∏Ï∂ú Í±¥ÎÑàÎúÄ (Ïã§ÏãúÍ∞Ñ ÏùòÎèÑ ÏïÑÎãò ÎòêÎäî USE_KMA_LIVE=False)")

    # Î≥ëÌï© ‚Üí ÏÜåÏä§Î≥Ñ Ï†ïÍ∑úÌôî
    merged = live_scored + persist_scored
    normalized: List[tuple] = []
    bysrc = {"persist": [h for h in merged if h[2]=="persist"],
             "live":    [h for h in merged if h[2]=="live"]}
    for src, hits in bysrc.items():
        if not hits:
            continue
        scores = [h[0] for h in hits]
        normed = minmax_norm(scores)
        for (orig_s, text, sname), ns in zip(hits, normed):
            normalized.append((ns, text, sname, orig_s))

    # dedup/sort
    seen = set(); dedup = []
    for s, t, src, o in normalized:
        key = re.sub(r"\s+", " ", t.strip())
        if key in seen:
            continue
        seen.add(key); dedup.append((s, t, src, o))
    dedup.sort(key=lambda x: x[0], reverse=True)
    top = dedup[:5]

    # üü¢ Ïª®ÌÖçÏä§Ìä∏ ÏµúÏÉÅÎã®Ïóê LIVE_STATUSÎ•º Î∂ôÏó¨ LLMÏù¥ Ï†ïÌôïÌûà ÎßêÌïòÍ≤å Ïú†ÎèÑ
    ctx_parts = []
    if live_status_msg:
        ctx_parts.append(live_status_msg)
    ctx_parts += [f"[Ïú†ÏÇ¨ÎèÑ:{s:.4f}][{src}]\n{txt}" for s, txt, src, _ in top]
    context = "\n\n".join(ctx_parts) or "Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    return {**state, "context": context}

def generate_node(state: GraphState) -> Dict[str, Any]:
    print("üß© ÎÖ∏Îìú: ÏÉùÏÑ±")
    if not state.get("question"): raise ValueError("question ÎàÑÎùΩ")
    if not state.get("context"):  raise ValueError("context ÎàÑÎùΩ")

    # ‚úÖ ÏûêÏó∞Ïñ¥ ÏÑúÏà†Ìòï Í∞ïÏ†ú
    prompt = ChatPromptTemplate.from_template(
        """ÎÑàÎäî ÎÜçÏûëÎ¨º Ïû¨Ìï¥ ÎåÄÏùë Ï†ÑÎ¨∏Í∞ÄÏïº.
ÏïÑÎûò Î¨∏Îß•ÏóêÎäî JSON ÌäπÎ≥¥ÏôÄ ÏùºÎ∞ò Î¨∏ÏÑúÍ∞Ä ÏÑûÏó¨ ÏûàÏñ¥. JSONÏóêÏÑú ÏÇ¨Ïã§Îßå ÏÑ†Î≥ÑÌïòÎêò, ÏµúÏ¢Ö Ï∂úÎ†•ÏùÄ ÌïúÍµ≠Ïñ¥ ÏûêÏó∞Ïñ¥ Î¨∏Ïû•ÏúºÎ°úÎßå ÏûëÏÑ±Ìï¥.
Î∂àÎ¶ø, Î≤àÌò∏ Î™©Î°ù, ÏΩîÎìúÎ∏îÎ°ù, JSON, ÌëúÎäî ÏÇ¨Ïö©ÌïòÏßÄ Îßà. Ï§ëÎ≥µ/ÏÉÅÏ∂©ÏùÄ Ï†ïÎ¶¨ÌïòÍ≥†,
ÌäπÎ≥¥Îäî ÏßÄÏó≠¬∑Ï¢ÖÎ•ò¬∑ÏàòÏ§Ä¬∑Î™ÖÎ†π¬∑Î∞úÌëúÏãúÍ∞Å¬∑Í∏∞Í∞ÑÏùÑ ÏûêÏó∞Ïä§Îü¨Ïö¥ Î¨∏Ïû•ÏúºÎ°ú ÏÑ§Î™ÖÌï¥.

[Î¨∏Îß•]
{context}

ÏßàÎ¨∏: {question}
ÎãµÎ≥Ä:"""
    )
    chain = (
        {"context": itemgetter("context"), "question": itemgetter("question")}
        | prompt
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"]})

    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer": txt}

# =========[ Í∑∏ÎûòÌîÑ ÎπåÎìú ]=========
def build_graph():
    g = StateGraph(GraphState)
    g.add_node("load_store", load_store_node)
    g.add_node("retrieve",   retrieve_node)
    g.add_node("generate",   generate_node)

    g.add_edge("load_store", "retrieve")
    g.add_edge("retrieve",   "generate")
    g.add_edge("generate",   END)
    g.set_entry_point("load_store")
    return g.compile()

# =========[ ÌèâÍ∞Ä Ïú†Ìã∏ ]=========
def _ensure_embedder():
    global _text_embedder
    if _text_embedder is None:
        _text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
    return _text_embedder

def evaluate_goldenset(app, csv_path: str, limit: int = 50, out_path: str = "evaluation_results.csv"):
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Í≥®Îì†ÏÖã CSVÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}")

    df = pd.read_csv(csv_path)

    def _find_col(cands):
        lc = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in lc: return lc[k]
        for c in df.columns:
            if any(k in c for k in ["ÏßàÎ¨∏","question"]): return c
        return None

    q_col = _find_col(["question"])
    a_col = _find_col(["answer","ground_truth","gt"])
    if not q_col or not a_col:
        raise ValueError(f"CSVÏóê question/answer Ïª¨ÎüºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. (Î∞úÍ≤¨Îêú Ïª¨Îüº: {list(df.columns)})")

    eval_df = df.head(limit).copy()
    preds, scores = [], []
    emb_model = _ensure_embedder()

    for idx, row in eval_df.iterrows():
        q = str(row[q_col]).strip()
        gt = str(row[a_col]).strip()

        try:
            out = app.invoke({"question": q})
            pred = (out.get("answer") or "").strip()
        except Exception as e:
            pred = f"[Ïò§Î•ò] {e}"

        # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        if gt and pred and not pred.startswith("[Ïò§Î•ò]"):
            vecs = emb_model.encode([gt, pred], show_progress_bar=False)
            vecs = np.array([l2_normalize(v) for v in vecs], dtype="float32")
            sim = float((vecs[0] * vecs[1]).sum())
        else:
            sim = 0.0

        preds.append(pred)
        scores.append(sim)

        # üü¢ Ï¶âÏãú Ï∂úÎ†•
        print(f"\n[{idx+1}/{limit}]")
        print(f"ÏßàÎ¨∏: {q}")
        print(f"Ï†ïÎãµ: {gt}")
        print(f"ÎãµÎ≥Ä: {pred}")
        print(f"Ïú†ÏÇ¨ÎèÑ: {sim:.4f}")
        print("-"*50)

    eval_df["prediction"] = preds
    eval_df["cosine_similarity"] = scores
    eval_df["passed@0.75"] = eval_df["cosine_similarity"] >= 0.75

    eval_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\nÏ†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû•: {out_path}")

# =========[ Ïã§ÌñâÎ∂Ä ]=========
if __name__ == "__main__":
    from argparse import ArgumentParser
    import sys

    parser = ArgumentParser(description="Live+Persist RAG: ÎåÄÌôîÌòï Í∏∞Î≥∏ / --eval Ïãú ÌèâÍ∞Ä")
    # ‚úÖ QAÏö© ÏòµÏÖò(ÏÑúÎ∏åÏª§Îß®Îìú ÏóÜÏù¥)
    parser.add_argument("-q", "--question", default=None, help="Ìïú Î≤àÎßå ÏßàÎ¨∏ÌïòÍ≥† Ï¢ÖÎ£å")
    parser.add_argument("--show-context", action="store_true", help="Ïª®ÌÖçÏä§Ìä∏(Í∑ºÍ±∞)ÎèÑ Ìï®Íªò Ï∂úÎ†•")
    # ‚úÖ ÌèâÍ∞ÄÏö© ÏòµÏÖò
    parser.add_argument("--eval", dest="eval_csv", default=None, help="Í≥®Îì†ÏÖã CSV Í≤ΩÎ°ú(ÏßÄÏ†ï Ïãú ÌèâÍ∞Ä Î™®Îìú)")
    parser.add_argument("--n", dest="limit", type=int, default=50, help="ÌèâÍ∞Ä ÌëúÎ≥∏ Í∞úÏàò(ÏµúÎåÄ)")
    parser.add_argument("--out", dest="out_path", default="evaluation_results.csv", help="ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú")
    args = parser.parse_args()

    print("üí¨ LangGraph Live+Persist (LC-FAISS only)")
    app = build_graph()

    # --- ÌèâÍ∞Ä Î™®Îìú ---
    if args.eval_csv:
        evaluate_goldenset(app, csv_path=args.eval_csv, limit=args.limit, out_path=args.out_path)
        sys.exit(0)

    # --- QA Î™®Îìú(Í∏∞Î≥∏) ---
    if args.question:
        q = args.question.strip()
        if not q:
            raise ValueError("ÏßàÎ¨∏Ïù¥ ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.")
        try:
            out = app.invoke({"question": q})
            if args.show_context:
                print("\n=== Ïª®ÌÖçÏä§Ìä∏ ===")
                print(out.get("context", ""))
            print("\n=== ÎãµÎ≥Ä ===")
            print(out.get("answer", ""))
            print()
        except Exception as e:
            print(f"‚ùå Ïò§Î•ò: {e}\n")
    else:
        # REPL
        print("ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî. (Ï¢ÖÎ£å: exit/quit)")
        while True:
            q = input("ÏßàÎ¨∏> ").strip()
            if q.lower() in ("exit", "quit"):
                break
            if not q:
                continue
            try:
                out = app.invoke({"question": q})
                if args.show_context:
                    print("\n=== Ïª®ÌÖçÏä§Ìä∏ ===")
                    print(out.get("context", ""))
                print("\n=== ÎãµÎ≥Ä ===")
                print(out.get("answer", ""))
                print()
            except Exception as e:
                print(f"‚ùå Ïò§Î•ò: {e}\n")
