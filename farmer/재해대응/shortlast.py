# -*- coding: utf-8 -*-
import os
import re
import json
from typing import TypedDict, Optional, Any, Dict, List
from datetime import datetime, timedelta
from operator import itemgetter
from argparse import ArgumentParser

import numpy as np
import pandas as pd
import faiss
import requests
from urllib.parse import urlencode

from dotenv import load_dotenv
from tavily import TavilyClient
from pydantic import BaseModel
from langchain_core.output_parsers import JsonOutputParser

load_dotenv()

# =========[ í™˜ê²½ì„¤ì • ]=========
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_kma_db")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
KMA_TIMEOUT = int(os.getenv("KMA_TIMEOUT", "10"))
WHEATHER_API_KEY_HUB = os.getenv("WHEATHER_API_KEY_HUB")
USE_KMA_LIVE = os.getenv("USE_KMA_LIVE", "true").lower() in ("1", "true", "yes")
FORCE_DISABLE_KMA_LIVE = False

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
TAVILY_MAX_RESULTS = int(os.getenv("TAVILY_MAX_RESULTS", "5"))

# =========[ LangChain/LangGraph/LLM ]=========
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from sentence_transformers import SentenceTransformer

# =========[ ë§¤í•‘/ìœ í‹¸ ]=========
WRN_MAP = {"T":"íƒœí’","W":"ê°•í’","R":"í˜¸ìš°","C":"í•œíŒŒ","D":"ê±´ì¡°","O":"í•´ì¼","N":"ì§€ì§„í•´ì¼","V":"í’ë‘","S":"ëŒ€ì„¤","Y":"í™©ì‚¬","H":"í­ì—¼","F":"ì•ˆê°œ"}
LVL_MAP = {"1":"ì˜ˆë¹„íŠ¹ë³´","2":"ì£¼ì˜ë³´","3":"ê²½ë³´"}
CMD_MAP = {"1":"ë°œí‘œ","2":"ëŒ€ì¹˜","3":"í•´ì œ","4":"ëŒ€ì¹˜í•´ì œ","5":"ì—°ì¥","6":"ë³€ê²½","7":"ë³€ê²½í•´ì œ"}
REGION_CODE_RE = re.compile(r"^[A-Z]\d{7}$")
REGION_MAP = {}
_text_embedder = None

# ë‹¨ê¸°ì˜ˆë³´ ë§¤í•‘
SKY_MAP = {"DB01": "ë§‘ìŒ", "DB02": "êµ¬ë¦„ì¡°ê¸ˆ", "DB03": "êµ¬ë¦„ë§ìŒ", "DB04": "íë¦¼"}
PREP_MAP = {"0": "ì—†ìŒ", "1": "ë¹„", "2": "ë¹„/ëˆˆ", "3": "ëˆˆ", "4": "ëˆˆ/ë¹„"}
WIND_KO = {
    "N":"ë¶","NNE":"ë¶ë¶ë™","NE":"ë¶ë™","ENE":"ë™ë¶ë™","E":"ë™","ESE":"ë™ë‚¨ë™","SE":"ë‚¨ë™","SSE":"ë‚¨ë‚¨ë™",
    "S":"ë‚¨","SSW":"ë‚¨ë‚¨ì„œ","SW":"ë‚¨ì„œ","WSW":"ì„œë‚¨ì„œ","W":"ì„œ","WNW":"ì„œë¶ì„œ","NW":"ë¶ì„œ","NNW":"ë¶ë¶ì„œ"
}

# =========[ ì§€ì—­ ì½”ë“œ ë§¤í•‘: ë‘ CSV ë³‘í•© ]=========
# .env ì˜ˆì‹œ:
# KMA_ADVISORY_MAP_CSV=./warn_regions.csv   # ê¸°ìƒíŠ¹ë³´ìš©
# KMA_FCT_MAP_CSV=./regions.csv             # ë‹¨ê¸°ì˜ˆë³´ìš©

REGION_MAP: Dict[str, str] = {}          # code -> name
REGION_NAME_INDEX: Dict[str, List[str]] = {}  # normalized_name -> [codes]

def _norm_name(s: str) -> str:
    # ê³µë°±/ã†/Â·/ê´„í˜¸ ì œê±° í›„ ì†Œë¬¸ìí™”
    return re.sub(r"[()\sÂ·ã†]", "", str(s or "")).lower()

def _pick_cols(df: pd.DataFrame) -> tuple:
    cols = {c.lower(): c for c in df.columns}
    code_col = next((cols[k] for k in ("code","region_code","ì§€ì—­ì½”ë“œ","reg_id","regid","id") if k in cols), None)
    name_col = next((cols[k] for k in ("name","region_name","ì§€ì—­ëª…","reg_name","regname","ko_name","í•œê¸€ëª…") if k in cols), None)

    # ëª» ì°¾ìœ¼ë©´ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
    if not code_col:
        best, hits = None, -1
        for c in df.columns:
            cnt = sum(bool(re.match(r"^[A-Za-z]?\d{5,}$", str(v).strip())) for v in df[c].astype(str))
            if cnt > hits: best, hits = c, cnt
        code_col = best
    if not name_col:
        best, hits = None, -1
        for c in df.columns:
            cnt = sum(bool(re.search(r"[ê°€-í£]", str(v))) for v in df[c].astype(str))
            if cnt > hits: best, hits = c, cnt
        name_col = best
    return code_col, name_col

def _read_map_csv(path: Optional[str]) -> List[tuple]:
    if not path or not os.path.exists(path):
        return []
    df = pd.read_csv(path)
    code_col, name_col = _pick_cols(df)
    pairs: List[tuple] = []
    for _, r in df.iterrows():
        code = str(r[code_col]).strip() if code_col else ""
        name = str(r[name_col]).strip() if name_col else ""
        if not code or not name or code.lower() == "nan" or name.lower() == "nan":
            continue
        pairs.append((code, name))
    return pairs

def _load_region_maps_from_two_csvs():
    REGION_MAP.clear()
    REGION_NAME_INDEX.clear()

    paths = [os.getenv("KMA_ADVISORY_MAP_CSV"), os.getenv("KMA_FCT_MAP_CSV")]
    total = 0
    for p in paths:
        for code, name in _read_map_csv(p):
            REGION_MAP[code] = name   # ì¤‘ë³µ ì½”ë“œëŠ” ë§ˆì§€ë§‰ í•­ëª©ìœ¼ë¡œ ë®ì–´ì”€ (ìµœì‹ /ì •í™• CSVê°€ ìš°ì„ )
            REGION_NAME_INDEX.setdefault(_norm_name(name), []).append(code)
            total += 1

    used = ", ".join([p for p in paths if p]) or "N/A"
    print(f"âœ… ì§€ì—­ì½”ë“œ ë§¤í•‘ ë¡œë“œ: {len(REGION_MAP)}ê°œ (from: {used})")

_load_region_maps_from_two_csvs()  # <-- ê¸°ì¡´ _load_region_map_from_csv() í˜¸ì¶œì„ ì´ê±¸ë¡œ êµì²´

def resolve_region(token: str) -> str:
    if not token: return "N/A"
    t = token.strip()
    return REGION_MAP.get(t, t)

def fmt_kst(yyyymmddHHMM: str) -> str:
    try:
        dt = datetime.strptime(yyyymmddHHMM, "%Y%m%d%H%M")
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return yyyymmddHHMM

def l2_normalize(vec: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(vec)
    return vec / n if n > 0 else vec

def minmax_norm(scores: List[float]) -> List[float]:
    if not scores: return []
    lo, hi = min(scores), max(scores)
    if hi - lo < 1e-8: return [0.0 for _ in scores]
    return [(s - lo) / (hi - lo) for s in scores]

# =========[ KMA íŠ¹ë³´/ì˜ˆë³´ ê°€ì ¸ì˜¤ê¸° ]=========
def _format_kma_record(raw: List[str]) -> Dict[str, str]:
    tm_st = raw[0] if len(raw)>0 else "N/A"
    tm_ed = raw[1] if len(raw)>1 else "N/A"
    reg_token = raw[4].strip() if len(raw)>4 else "N/A"
    wrn = (raw[5].strip() if len(raw)>5 else "")
    lvl = (raw[6].strip() if len(raw)>6 else "")
    cmd = (raw[7].strip() if len(raw)>7 else "")
    grd = (raw[8].strip() if len(raw)>8 else "")
    region_name = resolve_region(reg_token)
    payload = {
        "source":"KMA_ADVISORY",
        "region_raw": reg_token, "region_name": region_name,
        "region_type":"code" if REGION_CODE_RE.match(reg_token or "") else "name",
        "hazard_code": wrn, "hazard_name": WRN_MAP.get(wrn, "ì•Œìˆ˜ì—†ìŒ"),
        "level_code": lvl, "level_name": LVL_MAP.get(lvl, "N/A"),
        "command_code": cmd, "command_name": CMD_MAP.get(cmd, cmd),
        "window_start": tm_st, "window_end": tm_ed,
        "window_start_kst": fmt_kst(tm_st) if tm_st!="N/A" else "N/A",
        "window_end_kst": fmt_kst(tm_ed) if tm_ed!="N/A" else "N/A",
        "announce_time_kst": fmt_kst(tm_st) if cmd=="1" and tm_st!="N/A" else None,
    }
    if wrn=="T" and grd: payload["typhoon_grade"] = grd
    time_bits = []
    if payload["window_start_kst"]!="N/A" and payload["window_end_kst"]!="N/A":
        time_bits.append(f"ê¸°ê°„: {payload['window_start_kst']} ~ {payload['window_end_kst']}")
    elif payload["window_start_kst"]!="N/A":
        time_bits.append(f"ì‹œê°: {payload['window_start_kst']}")
    if payload["announce_time_kst"]:
        time_bits.append(f"ë°œí‘œì‹œê°: {payload['announce_time_kst']}")
    parts = [
        f"ì§€ì—­: {region_name} ({reg_token})", *time_bits,
        f"ì¢…ë¥˜: {payload['hazard_name']}({payload['hazard_code']})",
        f"ìˆ˜ì¤€: {payload['level_name']}({payload['level_code']})",
        f"ëª…ë ¹: {payload['command_name']}({payload['command_code']})"
    ]
    if "typhoon_grade" in payload: parts.append(f"íƒœí’ ë“±ê¸‰: {payload['typhoon_grade']}")
    return {
        "json": json.dumps(payload, ensure_ascii=False, separators=(",", ":")),
        "human": " | ".join(parts)
    }

def fetch_kma_advisories(start_time: str, end_time: str, disp: str="1") -> List[Dict[str, str]]:
    if not WHEATHER_API_KEY_HUB: return []
    base = "https://apihub.kma.go.kr/api/typ01/url/wrn_met_data.php"
    params = {"authKey":WHEATHER_API_KEY_HUB, "wrn":"", "tmfc1":start_time, "tmfc2":end_time, "disp":disp}
    try:
        r = requests.get(base, params=params, timeout=KMA_TIMEOUT)
        r.raise_for_status()
        text = r.content.decode("euc-kr", errors="ignore")
        docs, seen = [], set()
        for line in [ln for ln in text.strip().split("\n") if ln.strip() and not ln.startswith("#") and not ln.startswith("7777END")]:
            raw = line.strip().rstrip("=").split(",")
            if len(raw) < 9: continue
            rec = _format_kma_record(raw)
            key = re.sub(r"\s+", " ", rec["json"]).strip()
            if key in seen: continue
            seen.add(key)
            docs.append(rec)
        return docs
    except Exception as e:
        print(f"âŒ ê¸°ìƒíŠ¹ë³´ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def _format_short_land_record(raw: list) -> Dict[str, str]:
    def g(i): return raw[i] if i < len(raw) else ""
    reg_id   = g(0)
    tm_fc    = g(1)
    tm_ef    = g(2)
    mod      = g(3)
    ne       = g(4)
    w1, w2   = g(9), g(11)
    ta, st   = g(12), g(13)
    sky, prep, wf = g(14), g(15), g(16)
    reg_name = resolve_region(reg_id)
    
    payload = {
        "source": "KMA_SHORT_LAND",
        "region_id": reg_id,
        "region_name": reg_name,
        "forecast_time": fmt_kst(tm_fc) if tm_fc else "N/A",
        "effective_time": fmt_kst(tm_ef) if tm_ef else "N/A",
        "temp": f"{ta}Â°C" if ta and ta != "-99" else "N/A",
        "precip_prob": f"{st}%" if st else "N/A",
        "sky_status": SKY_MAP.get(sky, sky),
        "precip_status": PREP_MAP.get(prep, prep),
        "wind_direction": f"{WIND_KO.get(w1, w1)}~{WIND_KO.get(w2, w2)}" if w1 and w2 else WIND_KO.get(w1, w1)
    }

    human_summary = f"{reg_name} ì§€ì—­ì˜ ë‚ ì”¨ëŠ” {payload['sky_status']} ìƒíƒœì´ë©°, ê¸°ì˜¨ì€ {payload['temp']}ì´ê³  ê°•ìˆ˜í™•ë¥ ì€ {payload['precip_prob']}ì…ë‹ˆë‹¤."
    if payload['wind_direction']:
        human_summary += f" ë°”ëŒì€ {payload['wind_direction']}ìœ¼ë¡œ ë¶‘ë‹ˆë‹¤."
    
    return {
        "json": json.dumps(payload, ensure_ascii=False, separators=(",", ":")),
        "human": human_summary
    }

def fetch_short_land_records() -> list:
    if not WHEATHER_API_KEY_HUB: return []
    BASE = "https://apihub.kma.go.kr/api/typ01/url/fct_afs_dl.php"
    params = {"reg": "", "tmfc": "0", "disp": "1", "authKey": WHEATHER_API_KEY_HUB}
    try:
        r = requests.get(f"{BASE}?{urlencode(params)}", timeout=KMA_TIMEOUT)
        r.raise_for_status()
        text = r.content.decode("euc-kr", errors="replace")
        
        docs = []
        for line in text.splitlines():
            s = line.strip()
            if not s or s.startswith("#") or s.startswith("7777END"):
                continue
            if s.endswith("="):
                s = s[:-1]
            raw_row = [c.strip() for c in s.split(",")]
            if len(raw_row) < 17: continue
            formatted_record = _format_short_land_record(raw_row)
            docs.append(formatted_record)
        return docs
    except Exception as e:
        print(f"âŒ ë‹¨ê¸°ì˜ˆë³´ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def _reverse_region_map() -> Dict[str, List[str]]:
    """ì •ê·œí™”ëœ ì§€ì—­ëª… -> ì½”ë“œëª©ë¡ (ë‘ CSV í•©ë³¸ ê¸°ë°˜)"""
    return {k: list(dict.fromkeys(v)) for k, v in REGION_NAME_INDEX.items()}

def extract_region_from_question(q: str) -> Optional[str]:
    """ì§ˆë¬¸ì—ì„œ ì§€ì—­ëª…ì„ ì°¾ì•„ 'ì •ì‹ ì§€ì—­ëª…'ìœ¼ë¡œ ë°˜í™˜ (ì½”ë“œ/í•œê¸€ ëª¨ë‘ ì¸ì‹)"""
    if not q:
        return None

    # ì½”ë“œ ì§ì ‘ ì…ë ¥ ì²˜ë¦¬ (ì˜ˆ: 11B00000)
    m = re.search(r"[A-Z]\d{7}", q)
    if m:
        code = m.group(0)
        name = REGION_MAP.get(code)
        if name:
            return name

    # í•œê¸€ ì§€ì—­ëª…(ì •ê·œí™”) ë§¤ì¹­
    qn  = _norm_name(q)
    rev = _reverse_region_map()
    for norm_name in sorted(rev.keys(), key=len, reverse=True):
        if norm_name and norm_name in qn:
            for c in rev[norm_name]:
                nm = REGION_MAP.get(c)
                if nm:
                    return nm

    # ê´€ìš©/ì•½ì¹­ ë³´ì •
    commons = ["ìš¸ì§„êµ°", "ìš¸ì§„", "ì˜ë•êµ°", "ì˜ë•", "ë¶€ì‚°ë™ë¶€", "ìš¸ì‚°ë™ë¶€", "ê±°ì œì‹œ",
               "ë‚¨í•´êµ°", "ì œì£¼ë„ë¶ë¶€", "ì œì£¼ë¶ë¶€", "ì‚¬ì²œì‹œ", "íŒŒì£¼ì‹œ"]
    for token in commons:
        tn = _norm_name(token)
        if tn in qn:
            for code, name in REGION_MAP.items():
                if tn in _norm_name(name):
                    return name
    return None


def summarize_region_alert(q: str, live_docs: List[Dict[str, str]]) -> str:
    region_name = extract_region_from_question(q)
    if not region_name: return "" 
    rev = _reverse_region_map()
    target_codes = rev.get(region_name, [])
    if not target_codes:
        return f"[LIVE_STATUS] {region_name}ì˜ íŠ¹ë³´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì—­ ì½”ë“œ ë§¤í•‘ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    matched = []
    for d in live_docs or []:
        try:
            payload = json.loads(d["json"])
            if payload.get("region_raw") in target_codes: matched.append(payload)
        except Exception: continue
    if not matched:
        return f"[LIVE_STATUS] ì˜¤ëŠ˜ {region_name}ì—ëŠ” ë°œíš¨ ì¤‘ì¸ íŠ¹ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í‰ì†Œì™€ ê°™ì´ ê´€ë¦¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
    matched.sort(key=lambda x: x.get("window_start", ""), reverse=True)
    p = matched[0]
    region, hz = p.get("region_name", region_name), p.get("hazard_name", "íŠ¹ë³´")
    lvl, cmd = p.get("level_name", ""), p.get("command_name", "")
    st, ed = p.get("window_start_kst", ""), p.get("window_end_kst", "")
    bits = [f"{region}ì—ëŠ” {hz} {lvl}ê°€"]
    if st and ed: bits.append(f"{st}ì— ë°œíš¨ë˜ì–´ {ed}ì— í•´ì œë©ë‹ˆë‹¤.")
    elif st: bits.append(f"{st}ì— ë°œíš¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else: bits.append("ë°œíš¨ ì¤‘ì…ë‹ˆë‹¤.")
    if cmd: bits.append(f"(ëª…ë ¹: {cmd})")
    return "[LIVE_STATUS] " + " ".join(bits)

# =========[ ì„ë² ë”© (ë¼ì´ë¸Œ ê²€ìƒ‰ìš©) ]=========
_text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
def embed_texts(texts: List[str]) -> np.ndarray:
    embs = _text_embedder.encode(texts, show_progress_bar=False)
    embs = np.array([l2_normalize(e) for e in embs], dtype="float32")
    return embs

# =========[ ìƒíƒœ/í”„ë¡¬í”„íŠ¸/LLM ]=========
class GraphState(TypedDict):
    question: Optional[str]
    context: Optional[str]
    answer: Optional[str]
    answer_draft: Optional[str]
    store_obj: Optional[Any]
    retry_count: int
    is_context_valid: Optional[bool]
    next_action: Optional[str]

class ValidationResult(BaseModel):
    is_valid: bool
    is_grounded: bool
    reasoning: str

def make_llm() -> ChatGroq:
    if not GROQ_API_KEY: raise ValueError("GROQ_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

def _should_use_live(q: str) -> bool:
    if FORCE_DISABLE_KMA_LIVE or not USE_KMA_LIVE: return False
    q = (q or "")
    live_kw = ["í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "íŠ¹ë³´", "ì‹¤ì‹œê°„", "ê²½ë³´", "ì£¼ì˜ë³´", "í•´ì œ", "ë°œí‘œ", "ë‚ ì”¨", "ê¸°ì˜¨", "ê°•ìˆ˜"]
    return any(k in q for k in live_kw)

# =========[ LangGraph ë…¸ë“œ ]=========
def load_store_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (LangChain-FAISS)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True}
    )
    vs = LCFAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return {**state, "store_obj": vs, "retry_count": 0}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: ê²€ìƒ‰(MMR + KMA ë¼ì´ë¸Œ ë³‘í•©) | ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    q = state["question"] or ""
    live_enabled = _should_use_live(q)
    
    retriever = state["store_obj"].as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 20})
    docs = retriever.invoke(q)
    persist_scored = [(0.2, d.page_content, "persist") for d in docs]
    
    live_scored: List[tuple] = []
    live_status_msg = ""

    if live_enabled:
        # ê¸°ìƒíŠ¹ë³´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì„ë² ë”©
        try:
            now = datetime.now()
            tm1 = now.replace(hour=0, minute=0, second=0, microsecond=0).strftime("%Y%m%d%H%M")
            tm2 = now.strftime("%Y%m%d%H%M")
            advisories = fetch_kma_advisories(tm1, tm2) if WHEATHER_API_KEY_HUB else []
            live_status_msg = summarize_region_alert(q, advisories)
            if advisories:
                human_texts = [d["human"] for d in advisories]
                embs = embed_texts(human_texts)
                live_idx = faiss.IndexFlatIP(embs.shape[1])
                live_idx.add(embs)
                qv = embed_texts([q])[0]
                topk = min(5, len(advisories))
                D, I = live_idx.search(np.array([qv], dtype="float32"), topk)
                for s, i in zip(D[0], I[0]):
                    if i == -1: continue
                    ctx = f"JSON: {advisories[i]['json']}\nìš”ì•½: {advisories[i]['human']}"
                    live_scored.append((float(s) + 1.0, ctx, "live_advisory"))
        except Exception as e:
            print(f"âŒ ê¸°ìƒíŠ¹ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë‹¨ê¸°ì˜ˆë³´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì„ë² ë”©
        try:
            forecasts = fetch_short_land_records()
            if forecasts:
                human_texts = [d["human"] for d in forecasts]
                embs = embed_texts(human_texts)
                live_idx = faiss.IndexFlatIP(embs.shape[1])
                live_idx.add(embs)
                qv = embed_texts([q])[0]
                topk = min(5, len(forecasts))
                D, I = live_idx.search(np.array([qv], dtype="float32"), topk)
                for s, i in zip(D[0], I[0]):
                    if i == -1: continue
                    ctx = f"JSON: {forecasts[i]['json']}\nìš”ì•½: {forecasts[i]['human']}"
                    live_scored.append((float(s) + 1.0, ctx, "live_forecast"))
        except Exception as e:
            print(f"âŒ ë‹¨ê¸°ì˜ˆë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    else:
        print("â„¹ï¸ KMA ë¼ì´ë¸Œ í˜¸ì¶œ ê±´ë„ˆëœ€ (ì‹¤ì‹œê°„ ì˜ë„ ì•„ë‹˜ ë˜ëŠ” USE_KMA_LIVE=False)")
    
    merged = live_scored + persist_scored
    normalized: List[tuple] = []
    by_src = {"persist": [h for h in merged if h[2]=="persist"], 
              "live_advisory": [h for h in merged if h[2]=="live_advisory"],
              "live_forecast": [h for h in merged if h[2]=="live_forecast"]}
    
    for src, hits in by_src.items():
        if not hits: continue
        scores = [h[0] for h in hits]
        normed = minmax_norm(scores)
        for (orig_s, text, sname), ns in zip(hits, normed):
            normalized.append((ns, text, sname, orig_s))
            
    seen = set(); dedup = []
    for s, t, src, o in normalized:
        key = re.sub(r"\s+", " ", t.strip())
        if key in seen: continue
        seen.add(key); dedup.append((s, t, src, o))
        
    dedup.sort(key=lambda x: x[0], reverse=True)
    top = dedup[:5]
    
    ctx_parts = []
    if live_status_msg: ctx_parts.append(live_status_msg)
    ctx_parts += [f"[ìœ ì‚¬ë„:{s:.4f}][{src}]\n{txt}" for s, txt, src, _ in top]
    context = "\n\n".join(ctx_parts) or "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return {**state, "context": context}

def validate_context_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: 1ì°¨ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦")
    context = state["context"] or ""
    is_valid = not (
        "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in context or
        (state.get("is_context_valid") is None and "LIVE_STATUS" in context and len(context.split('\n')) < 3)
    )
    if not is_valid:
        print("âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆì¶©ë¶„í•˜ì—¬ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return {**state, "is_context_valid": False, "retry_count": state["retry_count"] + 1}
    print("âœ… ì»¨í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.")
    return {**state, "is_context_valid": True}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: ì›¹ ê²€ìƒ‰ (Tavily Search API ì‚¬ìš©) | í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    if state["retry_count"] >= 2:
        print("âš ï¸ ì›¹ ê²€ìƒ‰ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}
    
    question = state["question"] or ""
    if not TAVILY_API_KEY:
        print("âš ï¸ TAVILY_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "context": state["context"] + "\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼] Tavily API í‚¤ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    client = TavilyClient(api_key=TAVILY_API_KEY)
    try:
        results = client.search(query=question, max_results=TAVILY_MAX_RESULTS)
        web_search_results = "\n".join([
            f"- ì¶œì²˜: {res['url']}\n ë‚´ìš©: {res['content']}"
            for res in results['results']
        ])
        web_result = f"""[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
ì§ˆë¬¸ "{question}"ì— ëŒ€í•œ ìµœì‹  ì •ë³´ì…ë‹ˆë‹¤.
{web_search_results}
"""
        return {**state, "context": state["context"] + "\n\n" + web_result}
    except Exception as e:
        print(f"âŒ Tavily ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {**state, "context": state["context"] + "\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼] ì›¹ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”."}

DRAFT_PROMPT = ChatPromptTemplate.from_template(
    """ë„ˆëŠ” ë†ì‘ë¬¼ ì¬í•´ ëŒ€ì‘ ì „ë¬¸ê°€ì•¼.
ì•„ë˜ ë¬¸ë§¥ì—ëŠ” JSON íŠ¹ë³´ì™€ ì¼ë°˜ ë¬¸ì„œê°€ ì„ì—¬ ìˆì–´. ë¬¸ë§¥ì—ì„œ ì‚¬ì‹¤ë§Œ ì„ ë³„í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆì•ˆ ë‹µë³€ì„ ì‘ì„±í•´.
ë¶ˆë¦¿, ë²ˆí˜¸ ëª©ë¡, ì½”ë“œë¸”ë¡, JSON, í‘œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆ. ì¤‘ë³µë˜ê±°ë‚˜ ìƒì¶©ë˜ëŠ” ë‚´ìš©ì€ ì •ë¦¬í•˜ê³ , íŠ¹ë³´ëŠ” ì§€ì—­, ì¢…ë¥˜, ìˆ˜ì¤€, ë°œí‘œì‹œê°, ê¸°ê°„ì„ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´. ë‹¨ê¸°ì˜ˆë³´ëŠ” ì§€ì—­, ê¸°ì˜¨, í•˜ëŠ˜ ìƒíƒœ, ê°•ìˆ˜ í™•ë¥ ì„ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ì´ˆì•ˆ ë‹µë³€:"""
)

def generate_draft_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì´ˆì•ˆ ìƒì„±")
    if not state.get("question"): raise ValueError("question ëˆ„ë½")
    if not state.get("context"): raise ValueError("context ëˆ„ë½")

    chain = (
        {"context": itemgetter("context"), "question": itemgetter("question")}
        | DRAFT_PROMPT
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"]})
    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer_draft": txt}

REFINE_PROMPT = ChatPromptTemplate.from_template(
    """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼, ê·¸ì— ëŒ€í•œ ì´ˆì•ˆ ë‹µë³€, ê·¸ë¦¬ê³  ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ë§¥ì´ì•¼.
ë„ˆëŠ” ë†ì‘ë¬¼ ì¬í•´ ëŒ€ì‘ ì „ë¬¸ê°€ë¡œì„œ, ì´ˆì•ˆ ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì™„ì „íˆ ì¶©ì¡±í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©(í™˜ê°)ì„ í¬í•¨í•˜ê³  ìˆì§€ëŠ” ì•Šì€ì§€ ì—„ê²©í•˜ê²Œ ê²€í† í•˜ê³  ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì¤˜.
ë§Œì•½ ì´ˆì•ˆ ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë¬¸ë§¥ê³¼ ë§ì§€ ì•ŠëŠ”ë‹¤ë©´, ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•´.
ìµœì¢… ë‹µë³€ì€ í•œêµ­ì–´ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„œì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆ.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ì´ˆì•ˆ ë‹µë³€: {answer_draft}

ìµœì¢… ë‹µë³€:"""
)

def refine_answer_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë‹µë³€ ê°œì„  ë° ìµœì¢… ìƒì„±")
    if not state.get("question"): raise ValueError("question ëˆ„ë½")
    if not state.get("context"): raise ValueError("context ëˆ„ë½")
    if not state.get("answer_draft"): raise ValueError("answer_draft ëˆ„ë½")

    chain = (
        {"context": itemgetter("context"), "question": itemgetter("question"), "answer_draft": itemgetter("answer_draft")}
        | REFINE_PROMPT
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"], "answer_draft": state["answer_draft"]})
    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer": txt}

VALIDATION_PROMPT = ChatPromptTemplate.from_template(
    """ì£¼ì–´ì§„ ì§ˆë¬¸, ë‹µë³€, ê·¸ë¦¬ê³  ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ ì—„ê²©í•˜ê²Œ í‰ê°€í•´ì¤˜.
ë„ˆëŠ” ì˜¤ì§ **JSON ê°ì²´**ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•˜ë©°, ì–´ë– í•œ ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë¼. ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¼ì•¼ í•´:

```json
{{
    "is_valid": boolean,
    "is_grounded": boolean,
    "reasoning": string
}}

ì§ˆë¬¸: {question}
ë¬¸ë§¥: {context}
ë‹µë³€: {answer}
ì‘ë‹µ:"""
)

def post_generate_validate_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: 2ì°¨ ë‹µë³€ ê²€ì¦ (LLM í™œìš©) | ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    q = state["question"] or ""
    a = state["answer"] or ""
    ctx = state["context"] or ""

    if state["retry_count"] >= 3:
        print("âš ï¸ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}

    validation_chain = (
        VALIDATION_PROMPT
        | make_llm()
        | StrOutputParser()
    )

    is_valid_answer, is_grounded = False, False

    try:
        raw_output = validation_chain.invoke({"question": q, "context": ctx, "answer": a})
        print(f"LLM ì›ë³¸ ì‘ë‹µ: {raw_output[:100]}...")

        match = re.search(r'```json\s*(\{.*\})\s*```', raw_output, re.DOTALL)
        if not match:
            match = re.search(r'(\{.*\})', raw_output, re.DOTALL)
        
        if not match:
            raise ValueError("LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        json_str = match.group(1)
        json_obj = json.loads(json_str)

        validation_result = ValidationResult(**json_obj)
        
        is_valid_answer = validation_result.is_valid
        is_grounded = validation_result.is_grounded
        print(f"â¡ï¸ LLM í‰ê°€ ê²°ê³¼: ìœ íš¨ì„±={is_valid_answer}, ê·¼ê±° ê¸°ë°˜={is_grounded}, ì´ìœ : {validation_result.reasoning}")

    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ë˜ëŠ” ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        is_valid_answer, is_grounded = False, False

    if is_valid_answer and is_grounded:
        print("âœ… ë‹µë³€ì´ ì¶©ë¶„í•˜ê³  ê·¼ê±°ê°€ ëª…í™•í•©ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return {**state, "next_action": "END"}
    elif not is_grounded and state["retry_count"] < 2:
        print("âš ï¸ ë‹µë³€ì´ ë¬¸ë§¥ì— ê·¼ê±°í•˜ì§€ ì•Šì•„ ì¬ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "re_search", "retry_count": state["retry_count"] + 1}
    elif not is_valid_answer:
        print("âš ï¸ ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ì—¬ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "refine_again", "retry_count": state["retry_count"] + 1}
    else:
        print("âš ï¸ ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•˜ì§€ë§Œ, ë” ì´ìƒ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}

def fallback_answer_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ìµœì¢… ì‹¤íŒ¨ ë‹µë³€")
    fallback_message = (
        "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì›¹ ê²€ìƒ‰ì„ ì‹œë„í–ˆìœ¼ë‚˜, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ í™•ë³´í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
        "ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    )
    return {**state, "answer": fallback_message}

# =========[ ê·¸ë˜í”„ ë¹Œë“œ ]=========
def build_graph():
    g = StateGraph(GraphState)
    
    g.add_node("load_store", load_store_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("validate_context", validate_context_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_draft", generate_draft_node)
    g.add_node("refine_answer", refine_answer_node)
    g.add_node("post_generate_validate", post_generate_validate_node)
    g.add_node("fallback_answer", fallback_answer_node)

    g.set_entry_point("load_store")
    g.add_edge("load_store", "retrieve")
    g.add_edge("retrieve", "validate_context")

    g.add_conditional_edges(
        "validate_context",
        lambda state: "continue" if state["is_context_valid"] else "fallback",
        {"continue": "generate_draft", "fallback": "web_search"}
    )
    
    g.add_edge("web_search", "generate_draft")
    g.add_edge("generate_draft", "refine_answer")
    g.add_edge("refine_answer", "post_generate_validate")

    g.add_conditional_edges(
        "post_generate_validate",
        lambda state: state["next_action"],
        {
            "re_search": "web_search",
            "refine_again": "refine_answer",
            "END": END,
            "fallback": "fallback_answer"
        }
    )
    
    g.add_edge("fallback_answer", END)
    
    app = g.compile()
    
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return app

# =========[ í‰ê°€ ìœ í‹¸ ]=========
def _ensure_embedder():
    global _text_embedder
    if _text_embedder is None: _text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
    return _text_embedder

def evaluate_goldenset(app, csv_path: str, limit: int = 50, out_path: str = "evaluation_results.csv"):
    if not os.path.exists(csv_path): raise FileNotFoundError(f"ê³¨ë“ ì…‹ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path)
    def _find_col(cands):
        lc = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in lc: return lc[k]
        for c in df.columns:
            if any(k in c for k in ["ì§ˆë¬¸","question"]): return c
        return None
    q_col = _find_col(["question"])
    a_col = _find_col(["answer","ground_truth","gt"])
    if not q_col or not a_col: raise ValueError(f"CSVì— question/answer ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°œê²¬ëœ ì»¬ëŸ¼: {list(df.columns)})")
    eval_df = df.head(limit).copy()
    preds, scores = [], []
    emb_model = _ensure_embedder()
    for idx, row in eval_df.iterrows():
        q = str(row[q_col]).strip()
        gt = str(row[a_col]).strip()
        try:
            out = app.invoke({"question": q})
            pred = (out.get("answer") or "").strip()
        except Exception as e:
            pred = f"[ì˜¤ë¥˜] {e}"
        if gt and pred and not pred.startswith("[ì˜¤ë¥˜]"):
            vecs = emb_model.encode([gt, pred], show_progress_bar=False)
            vecs = np.array([l2_normalize(v) for v in vecs], dtype="float32")
            sim = float((vecs[0] * vecs[1]).sum())
        else: sim = 0.0
        preds.append(pred)
        scores.append(sim)
        print(f"\n[{idx+1}/{limit}]")
        print(f"ì§ˆë¬¸: {q}")
        print(f"ì •ë‹µ: {gt}")
        print(f"ë‹µë³€: {pred}")
        print(f"ìœ ì‚¬ë„: {sim:.4f}")
        print("-"*50)
    eval_df["prediction"] = preds
    eval_df["cosine_similarity"] = scores
    eval_df["passed@0.75"] = eval_df["cosine_similarity"] >= 0.75
    eval_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\nì „ì²´ ê²°ê³¼ ì €ì¥: {out_path}")

# =========[ ì‹¤í–‰ë¶€ ]=========
if __name__ == "__main__":
    from argparse import ArgumentParser
    import sys
    parser = ArgumentParser(description="Live+Persist RAG: ëŒ€í™”í˜• ê¸°ë³¸ / --eval ì‹œ í‰ê°€")
    parser.add_argument("-q", "--question", default=None, help="í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ")
    parser.add_argument("--show-context", action="store_true", help="ì»¨í…ìŠ¤íŠ¸(ê·¼ê±°)ë„ í•¨ê»˜ ì¶œë ¥")
    parser.add_argument("--eval", dest="eval_csv", default=None, help="ê³¨ë“ ì…‹ CSV ê²½ë¡œ(ì§€ì • ì‹œ í‰ê°€ ëª¨ë“œ)")
    parser.add_argument("--n", dest="limit", type=int, default=50, help="í‰ê°€ í‘œë³¸ ê°œìˆ˜(ìµœëŒ€)")
    parser.add_argument("--out", dest="out_path", default="evaluation_results.csv", help="í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()
    print("ğŸ’¬ LangGraph Live+Persist (LC-FAISS only)")
    app = build_graph()
    if args.eval_csv:
        evaluate_goldenset(app, csv_path=args.eval_csv, limit=args.limit, out_path=args.out_path)
        sys.exit(0)
    if args.question:
        q = args.question.strip()
        if not q: raise ValueError("ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        try:
            out = app.invoke({"question": q})
            if args.show_context:
                print("\n=== ì»¨í…ìŠ¤íŠ¸ ===")
                print(out.get("context", ""))
            print("\n=== ë‹µë³€ ===")
            print(out.get("answer", ""))
            print()
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}\n")
    else:
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit/quit)")
        while True:
            q = input("ì§ˆë¬¸> ").strip()
            if q.lower() in ("exit", "quit"): break
            if not q: continue
            try:
                out = app.invoke({"question": q})
                if args.show_context:
                    print("\n=== ì»¨í…ìŠ¤íŠ¸ ===")
                    print(out.get("context", ""))
                print("\n=== ë‹µë³€ ===")
                print(out.get("answer", ""))
                print()
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")