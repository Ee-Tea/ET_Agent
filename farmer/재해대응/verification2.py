# -*- coding: utf-8 -*-
"""
Live + Persist RAG (LangChain-FAISS + KMA ë¼ì´ë¸Œ ë³‘í•©)
- fallback_answer ìœ ì§€
- web_search: 1íšŒìš©(web_once) + ë³´ê´€(web_keep) ë³‘í–‰
  * 1ì°¨ ê²€ìƒ‰ì—ì„œ ìœ íš¨í•œ ì¡°ê°ì€ web_keepì— 'ìŠ¹ê²©' ë³´ê´€
  * 2ì°¨ ê²€ìƒ‰ì—ì„œë„ web_keepê³¼ í•©ì³ì„œ ì‚¬ìš© (ì¤‘ë³µ ì œê±° + ê¸¸ì´ ì œí•œ)
- ì»¨í…ìŠ¤íŠ¸/ë‹µë³€ ê²€ì¦ ê°•í™” ê·¸ëŒ€ë¡œ ìœ ì§€
- ì¬ì‹œë„ í•œë„(MAX_RETRIES) ì ìš©
- í‰ê°€ ê²°ê³¼ì— ì‹¤ì œ ì‚¬ìš© ì»¨í…ìŠ¤íŠ¸(context_used) ê¸°ë¡
"""
import os
import re
import json
from typing import TypedDict, Optional, Any, Dict, List, Tuple
from datetime import datetime, timedelta
from operator import itemgetter

import numpy as np
import pandas as pd
import faiss
import requests

from dotenv import load_dotenv
load_dotenv()

# =========[ í™˜ê²½ì„¤ì • ]=========
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
VECTOR_DB_PATH   = os.getenv("VECTOR_DB_PATH", "faiss_kma_db")  # LangChain-FAISS ë””ë ‰í† ë¦¬ í•„ìˆ˜
GROQ_API_KEY     = os.getenv("GROQ_API_KEY")                    # í•„ìˆ˜
GROQ_MODEL       = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE      = float(os.getenv("TEMPERATURE", "0.2"))
KMA_TIMEOUT      = int(os.getenv("KMA_TIMEOUT", "10"))
WHEATHER_API_KEY_HUB = os.getenv("WHEATHER_API_KEY_HUB")
USE_KMA_LIVE     = os.getenv("USE_KMA_LIVE", "true").lower() in ("1", "true", "yes")
FORCE_DISABLE_KMA_LIVE = False  # REGION_MAP ë¹„ì–´ìˆì„ ë•Œ ê°•ì œ ë¹„í™œì„±í™” ê°€ëŠ¥
MAX_RETRIES      = int(os.getenv("MAX_RETRIES", "2"))  # ì¬ì‹œë„ í•œë„(ê¸°ë³¸ 2íšŒ)

# ì›¹ ê²€ìƒ‰ ë³´ê´€ ë²„í¼ ì œí•œ (í† í° ì ˆì•½)
WEB_KEEP_MAX_CHARS = int(os.getenv("WEB_KEEP_MAX_CHARS", "1500"))
WEB_KEEP_MAX_LINES = int(os.getenv("WEB_KEEP_MAX_LINES", "10"))

# =========[ LangChain/LangGraph/LLM ]=========
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from sentence_transformers import SentenceTransformer

# =========[ ë§¤í•‘/ìœ í‹¸ ]=========
WRN_MAP = {
    "T": "íƒœí’", "W": "ê°•í’", "R": "í˜¸ìš°", "C": "í•œíŒŒ", "D": "ê±´ì¡°",
    "O": "í•´ì¼", "N": "ì§€ì§„í•´ì¼", "V": "í’ë‘", "S": "ëŒ€ì„¤",
    "Y": "í™©ì‚¬", "H": "í­ì—¼", "F": "ì•ˆê°œ"
}
LVL_MAP = {"1": "ì˜ˆë¹„íŠ¹ë³´", "2": "ì£¼ì˜ë³´", "3": "ê²½ë³´"}
CMD_MAP = {"1": "ë°œí‘œ", "2": "ëŒ€ì¹˜", "3": "í•´ì œ", "4": "ëŒ€ì¹˜í•´ì œ", "5": "ì—°ì¥", "6": "ë³€ê²½", "7": "ë³€ê²½í•´ì œ"}
REGION_CODE_RE = re.compile(r"^[A-Z]\d{7}$")
REGION_MAP: Dict[str, str] = {}
_text_embedder: Optional[SentenceTransformer] = None

def _load_region_map_from_csv():
    path = os.getenv("REGION_MAP_CSV")
    if not path or not os.path.exists(path):
        return
    try:
        df = pd.read_csv(path)
        code_col = next((c for c in df.columns if c.lower() in ("code", "region_code", "ì§€ì—­ì½”ë“œ")), None)
        name_col = next((c for c in df.columns if c.lower() in ("name", "region_name", "ì§€ì—­ëª…")), None)
        if not code_col or not name_col:
            return
        for _, r in df.iterrows():
            code = str(r[code_col]).strip()
            name = str(r[name_col]).strip()
            if code and name:
                REGION_MAP[code] = name
    except Exception:
        pass

_load_region_map_from_csv()

# ê¸°ë³¸ REGION_MAP ìµœì†Œ ì…‹(ì—†ì„ ë•Œ ëŒ€ë¹„)
if not REGION_MAP:
    REGION_MAP.update({
        # ìœ¡ìƒ(ì¼ë¶€)
        "L1010000": "ê²½ê¸°ë„",
        "L1020000": "ê°•ì›ë„",
        "L1030000": "ì¶©ì²­ë‚¨ë„",
        "L1040000": "ì¶©ì²­ë¶ë„",
        "L1050000": "ì „ë¼ë‚¨ë„",
        "L1060000": "ì „ë¼ë¶ë„",
        "L1070000": "ê²½ìƒë¶ë„",
        "L1080000": "ê²½ìƒë‚¨ë„",
        "L1090000": "ì œì£¼ë„ (ìœ¡ìƒ)",
        "L1100000": "ì„œìš¸íŠ¹ë³„ì‹œ",
        "L1110000": "ì¸ì²œê´‘ì—­ì‹œ",
        "L1120000": "ëŒ€ì „ê´‘ì—­ì‹œ",
        "L1130000": "ê´‘ì£¼ê´‘ì—­ì‹œ",
        "L1140000": "ëŒ€êµ¬ê´‘ì—­ì‹œ",
        "L1150000": "ë¶€ì‚°ê´‘ì—­ì‹œ",
        "L1160000": "ìš¸ì‚°ê´‘ì—­ì‹œ",
        "L1170000": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
        # í•´ìƒ(ì¼ë¶€)
        "S1330000": "ì œì£¼ë„ì „í•´ìƒ",
        "S1300000": "ë‚¨í•´ì „í•´ìƒ",
        "S1310000": "ë‚¨í•´ë™ë¶€ì „í•´ìƒ",
        "S1320000": "ë‚¨í•´ì„œë¶€ì „í•´ìƒ",
        "S1250000": "ì„œí•´ì¤‘ë¶€ì „í•´ìƒ",
        "S1230000": "ì„œí•´ë‚¨ë¶€ì „í•´ìƒ",
        "S1231000": "ì „ë‚¨ì„œë¶€í•´ìƒ",
        "S2000000": "ì—°ì•ˆ/í‰ìˆ˜êµ¬ì—­ ì „ì²´",
    })
    if not REGION_MAP:
        print("âš ï¸ REGION_MAP ë¹„ì–´ ìˆìŒ. USE_KMA_LIVEë¥¼ Falseë¡œ ê°•ì œí•©ë‹ˆë‹¤.")
        FORCE_DISABLE_KMA_LIVE = True

def resolve_region(token: str) -> str:
    if not token:
        return "N/A"
    t = token.strip()
    return REGION_MAP.get(t, t)

def fmt_kst(yyyymmddHHMM: str) -> str:
    try:
        dt = datetime.strptime(yyyymmddHHMM, "%Y%m%d%H%M")
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return yyyymmddHHMM

def l2_normalize(vec: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(vec)
    return vec / n if n > 0 else vec

def minmax_norm(scores: List[float]) -> List[float]:
    if not scores:
        return []
    lo, hi = min(scores), max(scores)
    if hi - lo < 1e-8:
        return [0.0 for _ in scores]
    return [(s - lo) / (hi - lo) for s in scores]

# =========[ KMA ê°€ì ¸ì˜¤ê¸°/ìš”ì•½ ]=========
def _format_kma_record(raw: List[str]) -> Dict[str, str]:
    tm_st = raw[0] if len(raw) > 0 else "N/A"
    tm_ed = raw[1] if len(raw) > 1 else "N/A"
    reg_token = raw[4].strip() if len(raw) > 4 else "N/A"
    wrn = (raw[5].strip() if len(raw) > 5 else "")
    lvl = (raw[6].strip() if len(raw) > 6 else "")
    cmd = (raw[7].strip() if len(raw) > 7 else "")
    grd = (raw[8].strip() if len(raw) > 8 else "")

    region_name = resolve_region(reg_token)
    payload = {
        "source": "KMA",
        "region_raw": reg_token,
        "region_name": region_name,
        "region_type": "code" if REGION_CODE_RE.match(reg_token or "") else "name",
        "hazard_code": wrn,
        "hazard_name": WRN_MAP.get(wrn, "ì•Œìˆ˜ì—†ìŒ"),
        "level_code": lvl,
        "level_name": LVL_MAP.get(lvl, "N/A"),
        "command_code": cmd,
        "command_name": CMD_MAP.get(cmd, cmd),
        "window_start": tm_st,
        "window_end": tm_ed,
        "window_start_kst": fmt_kst(tm_st) if tm_st != "N/A" else "N/A",
        "window_end_kst": fmt_kst(tm_ed) if tm_ed != "N/A" else "N/A",
        "announce_time_kst": fmt_kst(tm_st) if cmd == "1" and tm_st != "N/A" else None,
    }
    if wrn == "T" and grd:
        payload["typhoon_grade"] = grd

    time_bits = []
    if payload["window_start_kst"] != "N/A" and payload["window_end_kst"] != "N/A":
        time_bits.append(f"ê¸°ê°„: {payload['window_start_kst']} ~ {payload['window_end_kst']}")
    elif payload["window_start_kst"] != "N/A":
        time_bits.append(f"ì‹œê°: {payload['window_start_kst']}")
    if payload["announce_time_kst"]:
        time_bits.append(f"ë°œí‘œì‹œê°: {payload['announce_time_kst']}")

    parts = [
        f"ì§€ì—­: {region_name} ({reg_token})",
        *time_bits,
        f"ì¢…ë¥˜: {payload['hazard_name']}({payload['hazard_code']})",
        f"ìˆ˜ì¤€: {payload['level_name']}({payload['level_code']})",
        f"ëª…ë ¹: {payload['command_name']}({payload['command_code']})",
    ]
    if "typhoon_grade" in payload:
        parts.append(f"íƒœí’ ë“±ê¸‰: {payload['typhoon_grade']}")

    return {
        "json": json.dumps(payload, ensure_ascii=False, separators=(",", ":")),
        "human": " | ".join(parts),
    }

def fetch_kma_alerts(start_time: str, end_time: str, disp: str = "1") -> List[Dict[str, str]]:
    if not WHEATHER_API_KEY_HUB:
        return []
    base = "https://apihub.kma.go.kr/api/typ01/url/wrn_met_data.php"
    params = {"authKey": WHEATHER_API_KEY_HUB, "wrn": "", "tmfc1": start_time, "tmfc2": end_time, "disp": disp}
    r = requests.get(base, params=params, timeout=KMA_TIMEOUT)
    r.raise_for_status()
    text = r.content.decode("euc-kr", errors="ignore")
    docs, seen = [], set()
    for line in [ln for ln in text.strip().split("\n") if ln.strip() and not ln.startswith("#") and not ln.startswith("7777END")]:
        raw = line.strip().rstrip("=").split(",")
        if len(raw) < 9:
            continue
        rec = _format_kma_record(raw)
        key = re.sub(r"\s+", " ", rec["json"]).strip()
        if key in seen:
            continue
        seen.add(key)
        docs.append(rec)
    return docs

def _reverse_region_map() -> Dict[str, List[str]]:
    rev: Dict[str, List[str]] = {}
    for code, name in REGION_MAP.items():
        rev.setdefault(name, []).append(code)
        alias = name.replace(" ", "")
        rev.setdefault(alias, []).append(code)
    return rev

def extract_region_from_question(q: str) -> Optional[str]:
    if not q:
        return None
    q_norm = re.sub(r"\s+", "", q)
    cand_names = set(REGION_MAP.values())
    for nm in cand_names:
        if nm in q or nm.replace(" ", "") in q_norm:
            return nm
    commons = ["ìš¸ì§„êµ°", "ìš¸ì§„", "ì˜ë•êµ°", "ì˜ë•", "ë¶€ì‚°ë™ë¶€", "ìš¸ì‚°ë™ë¶€", "ê±°ì œì‹œ", "ë‚¨í•´êµ°", "ì œì£¼ë„ë¶ë¶€", "ì œì£¼ë¶ë¶€", "ì‚¬ì²œì‹œ", "íŒŒì£¼ì‹œ"]
    for token in commons:
        if token in q or token.replace(" ", "") in q_norm:
            for code, name in REGION_MAP.items():
                if token.replace(" ", "") in name.replace(" ", ""):
                    return name
    return None

def summarize_region_alert(q: str, live_docs: List[Dict[str, str]]) -> str:
    region_name = extract_region_from_question(q)
    if not region_name:
        return ""
    rev = _reverse_region_map()
    target_codes = rev.get(region_name, [])
    if not target_codes:
        return f"[LIVE_STATUS] {region_name}ì˜ íŠ¹ë³´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì—­ ì½”ë“œ ë§¤í•‘ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    matched = []
    for d in live_docs or []:
        try:
            payload = json.loads(d["json"])
            if payload.get("region_raw") in target_codes:
                matched.append(payload)
        except Exception:
            continue
    if not matched:
        return f"[LIVE_STATUS] ì˜¤ëŠ˜ {region_name}ì—ëŠ” ë°œíš¨ ì¤‘ì¸ íŠ¹ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í‰ì†Œì™€ ê°™ì´ ê´€ë¦¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
    matched.sort(key=lambda x: x.get("window_start", ""), reverse=True)
    p = matched[0]
    region, hz = p.get("region_name", region_name), p.get("hazard_name", "íŠ¹ë³´")
    lvl, cmd = p.get("level_name", ""), p.get("command_name", "")
    st, ed = p.get("window_start_kst", ""), p.get("window_end_kst", "")
    bits = [f"{region}ì—ëŠ” {hz} {lvl}ê°€"]
    if st and ed:
        bits.append(f"{st}ì— ë°œíš¨ë˜ì–´ {ed}ì— í•´ì œë©ë‹ˆë‹¤.")
    elif st:
        bits.append(f"{st}ì— ë°œíš¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        bits.append("ë°œíš¨ ì¤‘ì…ë‹ˆë‹¤.")
    if cmd:
        bits.append(f"(ëª…ë ¹: {cmd})")
    return "[LIVE_STATUS] " + " ".join(bits)

# =========[ ì„ë² ë”© (ë¼ì´ë¸Œ ê²€ìƒ‰ìš©) ]=========
_text_embedder = SentenceTransformer(EMBED_MODEL_NAME)

def embed_texts(texts: List[str]) -> np.ndarray:
    embs = _text_embedder.encode(texts, show_progress_bar=False)
    embs = np.array([l2_normalize(e) for e in embs], dtype="float32")
    return embs

# =========[ ì›¹ ê²€ìƒ‰ ë³‘í•©/ì •ë¦¬ ìœ í‹¸ ]=========
_DATE_RE = re.compile(r"\d{4}[.\-\/ë…„ ]?\d{1,2}[.\-\/ì›” ]?\d{1,2}|\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}")
_TIME_RE = re.compile(r"\d{1,2}:\d{2}")
def _hazard_words() -> List[str]:
    return ["íƒœí’","í­ì—¼","í˜¸ìš°","ê°•í’","í’ë‘","ëŒ€ì„¤","í™©ì‚¬","í•´ì¼","ì§€ì§„í•´ì¼","ì•ˆê°œ","í•œíŒŒ","ê±´ì¡°","ì£¼ì˜ë³´","ê²½ë³´","ì˜ˆë¹„íŠ¹ë³´"]

def _split_web_blocks(text: str) -> List[str]:
    if not text:
        return []
    # [WEB_ONESHOT] í—¤ë” ì œê±°í•˜ê³  ì¤„ ë‹¨ìœ„ë¡œ ë¶„í•´
    body = re.sub(r"^\[WEB_ONESHOT\]\s*", "", text.strip())
    lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
    return lines

def _score_line(line: str, region_hint: str) -> int:
    s = 0
    if region_hint and (region_hint in line or region_hint.replace(" ","") in line.replace(" ","")):
        s += 3
    if any(h in line for h in _hazard_words()):
        s += 2
    if _DATE_RE.search(line) or _TIME_RE.search(line):
        s += 1
    if any(k in line for k in ["ê¸°ìƒì²­","KMA","ë°œí‘œ","í•´ì œ","ë°œíš¨","íŠ¹ë³´"]):
        s += 1
    return s

def _dedup_lines(lines: List[str]) -> List[str]:
    seen = set()
    out = []
    for ln in lines:
        key = re.sub(r"\s+", " ", ln.lower()).strip()
        if key in seen:
            continue
        seen.add(key)
        out.append(ln)
    return out

def _merge_web_keep(web_keep: str, web_once: str, question: str) -> str:
    """1ì°¨/2ì°¨ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©: ì¤‘ìš”ë„ ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ Nê°œ ë³´ê´€, ê¸¸ì´ ì œí•œ."""
    region_hint = extract_region_from_question(question) or ""
    keep_lines = _split_web_blocks(web_keep)
    once_lines = _split_web_blocks(web_once)
    merged = _dedup_lines(keep_lines + once_lines)

    # ì ìˆ˜ ë§¤ê¸°ê¸°
    scored: List[Tuple[int,str]] = [( _score_line(ln, region_hint), ln ) for ln in merged]
    # ì ìˆ˜ ë‚®ì€ ì¤„ ì œê±°(0ì  ì œê±°)
    scored = [t for t in scored if t[0] > 0]
    # ì ìˆ˜ìˆœ ì •ë ¬
    scored.sort(key=lambda x: x[0], reverse=True)
    # ìƒìœ„ ì¤„ ì„ ì • ë° ê¸¸ì´ ì œí•œ
    acc, res = 0, []
    for _, ln in scored:
        if len(res) >= WEB_KEEP_MAX_LINES:
            break
        if acc + len(ln) + 1 > WEB_KEEP_MAX_CHARS:
            break
        res.append(ln); acc += len(ln) + 1

    if not res:
        return ""

    return "[WEB_KEEP]\n" + "\n".join(res)

def _compose_effective_context(base_ctx: str, web_keep: str, web_once: str) -> str:
    parts = [p for p in [base_ctx.strip(), web_keep.strip(), web_once.strip()] if p]
    return "\n\n".join(parts).strip() if parts else ""

# =========[ ìƒíƒœ/í”„ë¡¬í”„íŠ¸/LLM ]=========
class GraphState(TypedDict):
    question: Optional[str]
    context: Optional[str]
    answer: Optional[str]
    store_obj: Optional[Any]
    retry_count: int
    is_context_valid: Optional[bool]
    next_action: Optional[str]
    web_once: Optional[str]       # 1íšŒìš© ì›¹ê²€ìƒ‰ ê²°ê³¼
    web_keep: Optional[str]       # ìœ ì§€ë˜ëŠ”(ìŠ¹ê²©ëœ) ì›¹ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ ì œê±°/ê¸¸ì´ ì œí•œ)
    context_used: Optional[str]   # ì‹¤ì œ ì‚¬ìš© ì»¨í…ìŠ¤íŠ¸(ì¶”ì ìš©)

def make_llm() -> ChatGroq:
    if not GROQ_API_KEY:
        raise ValueError("GROQ_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

def _should_use_live(q: str) -> bool:
    if FORCE_DISABLE_KMA_LIVE or not USE_KMA_LIVE:
        return False
    q = (q or "")
    live_kw = ["í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "íŠ¹ë³´", "ì‹¤ì‹œê°„", "ê²½ë³´", "ì£¼ì˜ë³´", "í•´ì œ", "ë°œí‘œ"]
    return any(k in q for k in live_kw)

# =========[ LangGraph ë…¸ë“œ ]=========
def load_store_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (LangChain-FAISS)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True}
    )
    vs = LCFAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return {**state, "store_obj": vs, "retry_count": 0, "web_once": "", "web_keep": "", "context_used": ""}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: ê²€ìƒ‰(MMR + KMA ë¼ì´ë¸Œ ë³‘í•©) | ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    q = state["question"] or ""
    live_enabled = _should_use_live(q)

    retriever = state["store_obj"].as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 20})
    docs = retriever.invoke(q)
    persist_scored = [(0.2, d.page_content, "persist") for d in docs]

    live_scored: List[tuple] = []
    live_docs: List[Dict[str, str]] = []
    live_status_msg = ""

    if live_enabled:
        try:
            now = datetime.now()
            tm1 = (now - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0).strftime("%Y%m%d%H%M")
            tm2 = now.strftime("%Y%m%d%H%M")
            live_docs = fetch_kma_alerts(tm1, tm2) if WHEATHER_API_KEY_HUB else []
            live_status_msg = summarize_region_alert(q, live_docs)
            if live_docs:
                human_texts = [d["human"] for d in live_docs]
                embs = embed_texts(human_texts)
                live_idx = faiss.IndexFlatIP(embs.shape[1])
                live_idx.add(embs)
                qv = embed_texts([q])[0]
                topk = min(5, len(live_docs))
                D, I = live_idx.search(np.array([qv], dtype="float32"), topk)
                for s, i in zip(D[0], I[0]):
                    if i == -1:
                        continue
                    ctx = f"JSON: {live_docs[i]['json']}\nìš”ì•½: {live_docs[i]['human']}"
                    live_scored.append((float(s) + 1.0, ctx, "live"))
        except Exception:
            pass
    else:
        print("â„¹ï¸ KMA ë¼ì´ë¸Œ í˜¸ì¶œ ê±´ë„ˆëœ€ (ì‹¤ì‹œê°„ ì˜ë„ ì•„ë‹˜ ë˜ëŠ” USE_KMA_LIVE=False)")

    merged = live_scored + persist_scored

    # ì†ŒìŠ¤ë³„ ì •ê·œí™”
    normalized: List[tuple] = []
    bysrc = {"persist": [h for h in merged if h[2] == "persist"], "live": [h for h in merged if h[2] == "live"]}
    for src, hits in bysrc.items():
        if not hits:
            continue
        scores = [h[0] for h in hits]
        normed = minmax_norm(scores)
        for (orig_s, text, sname), ns in zip(hits, normed):
            normalized.append((ns, text, sname, orig_s))

    # ì¤‘ë³µ ì œê±°
    seen = set()
    dedup = []
    for s, t, src, o in normalized:
        key = re.sub(r"\s+", " ", t.strip())
        if key in seen:
            continue
        seen.add(key)
        dedup.append((s, t, src, o))
    dedup.sort(key=lambda x: x[0], reverse=True)
    top = dedup[:5]

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (persistedë§Œ ê³ ì • ì €ì¥, live ìƒíƒœ ë¬¸êµ¬ëŠ” persistedì— í•©ì¹¨)
    ctx_parts = []
    if live_status_msg:
        ctx_parts.append(live_status_msg)
    persisted_context = "\n\n".join(ctx_parts + [f"[ìœ ì‚¬ë„:{s:.4f}][{src}]\n{txt}" for s, txt, src, _ in top]) or "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return {**state, "context": persisted_context}

# ---- ê°•í™”ëœ 1ì°¨ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ ----
def _has_live_json(context: str) -> bool:
    return context.count("JSON: {") >= 1

def _has_persist(context: str) -> bool:
    return "[persist]" in context

def _extract_dates(text: str) -> List[str]:
    return re.findall(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}", text)

def _region_mentioned_in_ctx(q: str, ctx: str) -> bool:
    region = extract_region_from_question(q) or ""
    if not region:
        return True  # ì§€ì—­ ë¯¸ì§€ì • ì§ˆë¬¸ì€ í†µê³¼
    if region in ctx:
        return True
    if region.replace(" ", "") in re.sub(r"\s+", "", ctx):
        return True
    return False

def validate_context_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: 1ì°¨ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦(ê°•í™”)")
    q = state.get("question") or ""
    base_ctx = state.get("context") or ""
    web_keep = state.get("web_keep") or ""
    web_once = state.get("web_once") or ""
    ctx = _compose_effective_context(base_ctx, web_keep, web_once)
    live_needed = _should_use_live(q)

    ok = True
    if live_needed and not _has_live_json(ctx):
        ok = False
    if not _has_persist(ctx) and not live_needed:
        ok = False
    if not _region_mentioned_in_ctx(q, ctx):
        ok = False
    if "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in ctx:
        ok = False

    if not ok:
        print("âš ï¸ ì»¨í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„ â†’ ì›¹ ê²€ìƒ‰/ì¬ìˆ˜ì§‘ í•„ìš”")
        return {**state, "is_context_valid": False, "retry_count": state["retry_count"] + 1}
    print("âœ… ì»¨í…ìŠ¤íŠ¸ ì¶©ë¶„")
    return {**state, "is_context_valid": True}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì›¹ ê²€ìƒ‰ (ë”ë¯¸ ë°ì´í„°, 1íšŒìš© + ë³´ê´€ ë³‘í•©)")
    question = state["question"] or ""
    if state["retry_count"] > MAX_RETRIES:
        print("âš ï¸ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›¹ ê²€ìƒ‰ ìƒëµ.")
        return {**state, "web_once": ""}

    dummy_web_result = f"""[WEB_ONESHOT]
- ì§ˆë¬¸ "{question}"ì— ëŒ€í•œ ìµœì‹  ì •ë³´(ë”ë¯¸).
- 2025ë…„ 8ì›” 12ì¼ì ê¸°ì‚¬: ì „êµ­ í­ì—¼ íŠ¹ë³´ ë°œíš¨(ì˜ˆì‹œ ë”ë¯¸).
- ì •ë¶€ ì§€ì¹¨(ë”ë¯¸): ì˜ë†í˜„ì¥ ì¬í•´ì˜ˆë°© ê´€ë¦¬ ì§€ì¹¨ ì—…ë°ì´íŠ¸.
"""
    # ê¸°ì¡´ web_keepì€ ìœ ì§€, ì´ë²ˆ ê²€ìƒ‰ì€ web_onceì—ë§Œ
    return {**state, "web_once": dummy_web_result}

def generate_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ìƒì„±")
    if not state.get("question"):
        raise ValueError("question ëˆ„ë½")
    if not state.get("context") and not state.get("web_once") and not state.get("web_keep"):
        raise ValueError("context ëˆ„ë½")

    base_ctx = state.get("context") or ""
    web_keep = state.get("web_keep") or ""
    web_once = state.get("web_once") or ""
    context_eff = _compose_effective_context(base_ctx, web_keep, web_once)

    prompt = ChatPromptTemplate.from_template(
        """ë„ˆëŠ” ë†ì‘ë¬¼ ì¬í•´ ëŒ€ì‘ ì „ë¬¸ê°€ì•¼.
ì•„ë˜ ë¬¸ë§¥ì—ëŠ” JSON íŠ¹ë³´ì™€ ì¼ë°˜ ë¬¸ì„œê°€ ì„ì—¬ ìˆì–´. JSONì—ì„œ ì‚¬ì‹¤ë§Œ ì„ ë³„í•˜ë˜, ìµœì¢… ì¶œë ¥ì€ í•œêµ­ì–´ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•´.
ë¶ˆë¦¿, ë²ˆí˜¸ ëª©ë¡, ì½”ë“œë¸”ë¡, JSON, í‘œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆ. ì¤‘ë³µ/ìƒì¶©ì€ ì •ë¦¬í•˜ê³ ,
íŠ¹ë³´ëŠ” ì§€ì—­Â·ì¢…ë¥˜Â·ìˆ˜ì¤€Â·ëª…ë ¹Â·ë°œí‘œì‹œê°Â·ê¸°ê°„ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""
    )
    chain = (
        {"context": lambda st: context_eff, "question": itemgetter("question")}
        | prompt
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": context_eff, "question": state["question"]})
    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer": txt, "context_used": context_eff}

# ---- ê°•í™”ëœ 2ì°¨ ë‹µë³€ ê²€ì¦ + web_keep ìŠ¹ê²©/ì •ë¦¬ ----
def _mentions_any(s: str, keys: List[str]) -> bool:
    return any(k and k in s for k in keys)

def _extract_hazard(answer: str) -> List[str]:
    cand = ["íƒœí’", "í­ì—¼", "í˜¸ìš°", "ê°•í’", "í’ë‘", "ëŒ€ì„¤", "í™©ì‚¬", "í•´ì¼", "ì§€ì§„í•´ì¼", "ì•ˆê°œ", "í•œíŒŒ", "ê±´ì¡°"]
    return [v for v in cand if v in answer][:2]

def post_generate_validate_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: 2ì°¨ ë‹µë³€ ê²€ì¦(ê°•í™”) | ì¬ì‹œë„: {state['retry_count']}")
    q = state.get("question") or ""
    a = (state.get("answer") or "").strip()

    base_ctx = state.get("context") or ""
    web_keep = state.get("web_keep") or ""
    web_once = state.get("web_once") or ""
    ctx = _compose_effective_context(base_ctx, web_keep, web_once)

    # --- ê²€ì¦ ë¡œì§ ---
    if not a:
        new_retry = state["retry_count"] + 1
        next_action = "re_retrieve" if new_retry <= MAX_RETRIES else "fallback"
        # 1) í˜„ì¬ web_onceì—ì„œ ìœ íš¨ ì¡°ê°ì„ web_keepìœ¼ë¡œ ìŠ¹ê²© í›„ web_once ë¹„ì›€
        new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
        return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    if any(bad in a for bad in ["ì•Œìˆ˜ì—†ìŠµë‹ˆë‹¤", "ë¶€ì¡±í•©ë‹ˆë‹¤"]):
        new_retry = state["retry_count"] + 1
        next_action = "re_retrieve" if new_retry <= MAX_RETRIES else "fallback"
        new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
        return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    region = extract_region_from_question(q) or ""
    hazards = _extract_hazard(a)
    dates = _extract_dates(a)
    level_tokens = [lv for lv in ["ì£¼ì˜ë³´", "ê²½ë³´", "ì˜ˆë¹„íŠ¹ë³´"] if lv in a]

    if region and region not in ctx and region.replace(" ", "") not in re.sub(r"\s+", "", ctx):
        new_retry = state["retry_count"] + 1
        next_action = "re_retrieve" if new_retry <= MAX_RETRIES else "fallback"
        new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
        return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    if hazards and not _mentions_any(ctx, hazards):
        new_retry = state["retry_count"] + 1
        next_action = "re_retrieve" if new_retry <= MAX_RETRIES else "fallback"
        new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
        return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    if level_tokens and not _mentions_any(ctx, level_tokens):
        new_retry = state["retry_count"] + 1
        next_action = "re_retrieve" if new_retry <= MAX_RETRIES else "fallback"
        new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
        return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    for d in dates:
        if d not in ctx:
            new_retry = state["retry_count"] + 1
            next_action = "re_search" if new_retry <= MAX_RETRIES else "fallback"
            new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
            return {**state, "next_action": next_action, "retry_count": new_retry, "web_once": "", "web_keep": new_keep}

    print("âœ… ë‹µë³€ ì •í•©ì„± OK")
    # ì„±ê³µ ì‹œì—ë„ web_onceì—ì„œ ìœ íš¨ ì¡°ê°ì„ web_keepìœ¼ë¡œ ìŠ¹ê²©, ê·¸ë¦¬ê³  web_once ë¹„ì›€
    new_keep = _merge_web_keep(web_keep, web_once, q) if web_once else web_keep
    return {**state, "next_action": "END", "web_once": "", "web_keep": new_keep}

def fallback_answer_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ìµœì¢… ì‹¤íŒ¨ ë‹µë³€")
    # ë³´ê´€ ì¤‘ì¸ web_keepë„ í¬í•¨í•´ì„œ 'ì‚¬ì‹¤ ê¸°ë°˜'ì— ìµœëŒ€í•œ ê¸°ëŒ€
    base_ctx = state.get("context") or ""
    web_keep = state.get("web_keep") or ""
    ctx = _compose_effective_context(base_ctx, web_keep, "")
    fallback_message = (
        "ì£„ì†¡í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì°¨ë¡€ ì‹œë„í–ˆì§€ë§Œ ì •í™•í•œ ë‹µë³€ì„ í™•ì‹ í•˜ê¸° ì–´ë ¤ì›Œìš”. "
        "ë‹¤ìŒì€ í˜„ì¬ í™•ë³´ëœ ì‚¬ì‹¤ ê¸°ë°˜ ì •ë³´ì…ë‹ˆë‹¤.\n\n" + (ctx or "ì¶”ê°€ë¡œ ì œì‹œí•  ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    )
    return {**state, "answer": fallback_message}

# =========[ ê·¸ë˜í”„ ë¹Œë“œ ]=========
def build_graph():
    g = StateGraph(GraphState)

    g.add_node("load_store", load_store_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("validate_context", validate_context_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate", generate_node)
    g.add_node("post_generate_validate", post_generate_validate_node)
    g.add_node("fallback_answer", fallback_answer_node)

    g.set_entry_point("load_store")
    g.add_edge("load_store", "retrieve")
    g.add_edge("retrieve", "validate_context")

    # 1ì°¨ ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ íë¦„ ì œì–´
    g.add_conditional_edges(
        "validate_context",
        lambda state: "continue" if state["is_context_valid"] else "fallback",
        {"continue": "generate", "fallback": "web_search"}
    )

    # ì›¹ ê²€ìƒ‰ í›„ ë‹¤ì‹œ ìƒì„±ìœ¼ë¡œ ì´ë™
    g.add_edge("web_search", "generate")

    # ìƒì„± â†’ 2ì°¨ ê²€ì¦
    g.add_edge("generate", "post_generate_validate")

    # 2ì°¨ ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ì¬ì‹œë„/ì›¹ê²€ìƒ‰/ì‹¤íŒ¨/ì¢…ë£Œ
    g.add_conditional_edges(
        "post_generate_validate",
        lambda state: state["next_action"],  # "re_retrieve" | "re_search" | "fallback" | "END"
        {"re_retrieve": "retrieve", "re_search": "web_search", "fallback": "fallback_answer", "END": END}
    )

    app = g.compile()
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return app

# =========[ í‰ê°€ ìœ í‹¸ ]=========
def _ensure_embedder():
    global _text_embedder
    if _text_embedder is None:
        _text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
    return _text_embedder

def evaluate_goldenset(app, csv_path: str, limit: int = 50, out_path: str = "evaluation_results.csv"):
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"ê³¨ë“ ì…‹ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path)

    def _find_col(cands):
        lc = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in lc:
                return lc[k]
        for c in df.columns:
            if any(k in c for k in ["ì§ˆë¬¸", "question"]):
                return c
        return None

    q_col = _find_col(["question"])
    a_col = _find_col(["answer", "ground_truth", "gt"])
    if not q_col or not a_col:
        raise ValueError(f"CSVì— question/answer ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°œê²¬ëœ ì»¬ëŸ¼: {list(df.columns)})")

    eval_df = df.head(limit).copy()
    preds, scores, used_ctx = [], [], []
    emb_model = _ensure_embedder()

    for idx, row in eval_df.iterrows():
        q = str(row[q_col]).strip()
        gt = str(row[a_col]).strip()
        try:
            out = app.invoke({"question": q})
            pred = (out.get("answer") or "").strip()
            ctx = out.get("context_used", _compose_effective_context(out.get("context",""), out.get("web_keep",""), ""))
        except Exception as e:
            pred = f"[ì˜¤ë¥˜] {e}"
            ctx = ""
        used_ctx.append(ctx)

        if gt and pred and not pred.startswith("[ì˜¤ë¥˜]"):
            vecs = emb_model.encode([gt, pred], show_progress_bar=False)
            vecs = np.array([l2_normalize(v) for v in vecs], dtype="float32")
            sim = float((vecs[0] * vecs[1]).sum())
        else:
            sim = 0.0

        preds.append(pred)
        scores.append(sim)

        print(f"\n[{idx+1}/{limit}]")
        print(f"ì§ˆë¬¸: {q}")
        print(f"ì •ë‹µ: {gt}")
        print(f"ë‹µë³€: {pred}")
        print(f"ìœ ì‚¬ë„: {sim:.4f}")
        print("-" * 50)

    eval_df["prediction"] = preds
    eval_df["cosine_similarity"] = scores
    eval_df["passed@0.75"] = eval_df["cosine_similarity"] >= 0.75
    eval_df["context_used"] = used_ctx
    eval_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\nì „ì²´ ê²°ê³¼ ì €ì¥: {out_path}")

# =========[ ì‹¤í–‰ë¶€ ]=========
if __name__ == "__main__":
    from argparse import ArgumentParser
    import sys

    parser = ArgumentParser(description="Live+Persist RAG: ëŒ€í™”í˜• ê¸°ë³¸ / --eval ì‹œ í‰ê°€")
    parser.add_argument("-q", "--question", default=None, help="í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ")
    parser.add_argument("--show-context", action="store_true", help="ì»¨í…ìŠ¤íŠ¸(ê·¼ê±°)ë„ í•¨ê»˜ ì¶œë ¥")
    parser.add_argument("--eval", dest="eval_csv", default=None, help="ê³¨ë“ ì…‹ CSV ê²½ë¡œ(ì§€ì • ì‹œ í‰ê°€ ëª¨ë“œ)")
    parser.add_argument("--n", dest="limit", type=int, default=50, help="í‰ê°€ í‘œë³¸ ê°œìˆ˜(ìµœëŒ€)")
    parser.add_argument("--out", dest="out_path", default="evaluation_results.csv", help="í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()

    print("ğŸ’¬ LangGraph Live+Persist (LC-FAISS only)")
    app = build_graph()

    if args.eval_csv:
        evaluate_goldenset(app, csv_path=args.eval_csv, limit=args.limit, out_path=args.out_path)
        sys.exit(0)

    if args.question:
        q = args.question.strip()
        if not q:
            raise ValueError("ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        try:
            out = app.invoke({"question": q})
            if args.show_context:
                # ì‹¤ì œë¡œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ í•©ì„±
                used = out.get("context_used", _compose_effective_context(out.get("context",""), out.get("web_keep",""), ""))
                print("\n=== ì»¨í…ìŠ¤íŠ¸(ì‹¤ì œ ì‚¬ìš©/ë³´ê´€ í¬í•¨) ===")
                print(used)
            print("\n=== ë‹µë³€ ===")
            print(out.get("answer", ""))
            print()
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}\n")
    else:
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit/quit)")
        while True:
            q = input("ì§ˆë¬¸> ").strip()
            if q.lower() in ("exit", "quit"):
                break
            if not q:
                continue
            try:
                out = app.invoke({"question": q})
                if args.show_context:
                    used = out.get("context_used", _compose_effective_context(out.get("context",""), out.get("web_keep",""), ""))
                    print("\n=== ì»¨í…ìŠ¤íŠ¸(ì‹¤ì œ ì‚¬ìš©/ë³´ê´€ í¬í•¨) ===")
                    print(used)
                print("\n=== ë‹µë³€ ===")
                print(out.get("answer", ""))
                print()
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")
