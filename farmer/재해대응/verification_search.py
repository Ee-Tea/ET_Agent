# -*- coding: utf-8 -*-
import os
import re
import json
from typing import TypedDict, Optional, Any, Dict, List
from datetime import datetime, timedelta
from operator import itemgetter
from argparse import ArgumentParser

import numpy as np
import pandas as pd
import faiss
import requests

from dotenv import load_dotenv
from tavily import TavilyClient
from pydantic import BaseModel
from langchain_core.output_parsers import JsonOutputParser

load_dotenv()

# =========[ í™˜ê²½ì„¤ì • ]=========
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_kma_db") # LangChain-FAISS ë””ë ‰í† ë¦¬ í•„ìˆ˜
GROQ_API_KEY = os.getenv("GROQ_API_KEY") # í•„ìˆ˜
GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
KMA_TIMEOUT = int(os.getenv("KMA_TIMEOUT", "10"))
WHEATHER_API_KEY_HUB = os.getenv("WHEATHER_API_KEY_HUB")
USE_KMA_LIVE = os.getenv("USE_KMA_LIVE", "true").lower() in ("1", "true", "yes")
FORCE_DISABLE_KMA_LIVE = False

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
TAVILY_MAX_RESULTS = int(os.getenv("TAVILY_MAX_RESULTS", "5"))

# =========[ LangChain/LangGraph/LLM ]=========
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS as LCFAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from sentence_transformers import SentenceTransformer

# =========[ ë§¤í•‘/ìœ í‹¸ ]=========
WRN_MAP = {"T":"íƒœí’","W":"ê°•í’","R":"í˜¸ìš°","C":"í•œíŒŒ","D":"ê±´ì¡°","O":"í•´ì¼","N":"ì§€ì§„í•´ì¼","V":"í’ë‘","S":"ëŒ€ì„¤","Y":"í™©ì‚¬","H":"í­ì—¼","F":"ì•ˆê°œ"}
LVL_MAP = {"1":"ì˜ˆë¹„íŠ¹ë³´","2":"ì£¼ì˜ë³´","3":"ê²½ë³´"}
CMD_MAP = {"1":"ë°œí‘œ","2":"ëŒ€ì¹˜","3":"í•´ì œ","4":"ëŒ€ì¹˜í•´ì œ","5":"ì—°ì¥","6":"ë³€ê²½","7":"ë³€ê²½í•´ì œ"}
REGION_CODE_RE = re.compile(r"^[A-Z]\d{7}$")
REGION_MAP = {}
_text_embedder = None

def _load_region_map_from_csv():
    path = os.getenv("REGION_MAP_CSV")
    if not path or not os.path.exists(path):
        return
    try:
        df = pd.read_csv(path)
        code_col = next((c for c in df.columns if c.lower() in ("code","region_code","ì§€ì—­ì½”ë“œ")), None)
        name_col = next((c for c in df.columns if c.lower() in ("name","region_name","ì§€ì—­ëª…")), None)
        if not code_col or not name_col: return
        for _, r in df.iterrows():
            code = str(r[code_col]).strip()
            name = str(r[name_col]).strip()
            if code and name: REGION_MAP[code] = name
    except Exception:
        pass
_load_region_map_from_csv()

def resolve_region(token: str) -> str:
    if not token: return "N/A"
    t = token.strip()
    return REGION_MAP.get(t, t)

def fmt_kst(yyyymmddHHMM: str) -> str:
    try:
        dt = datetime.strptime(yyyymmddHHMM, "%Y%m%d%H%M")
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return yyyymmddHHMM

def l2_normalize(vec: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(vec)
    return vec / n if n > 0 else vec

def minmax_norm(scores: List[float]) -> List[float]:
    if not scores: return []
    lo, hi = min(scores), max(scores)
    if hi - lo < 1e-8: return [0.0 for _ in scores]
    return [(s - lo) / (hi - lo) for s in scores]

# =========[ KMA ê°€ì ¸ì˜¤ê¸°/ìš”ì•½ ]=========
def _format_kma_record(raw: List[str]) -> Dict[str, str]:
    tm_st = raw[0] if len(raw)>0 else "N/A"
    tm_ed = raw[1] if len(raw)>1 else "N/A"
    reg_token = raw[4].strip() if len(raw)>4 else "N/A"
    wrn = (raw[5].strip() if len(raw)>5 else "")
    lvl = (raw[6].strip() if len(raw)>6 else "")
    cmd = (raw[7].strip() if len(raw)>7 else "")
    grd = (raw[8].strip() if len(raw)>8 else "")
    region_name = resolve_region(reg_token)
    payload = {
        "source":"KMA",
        "region_raw": reg_token, "region_name": region_name,
        "region_type":"code" if REGION_CODE_RE.match(reg_token or "") else "name",
        "hazard_code": wrn, "hazard_name": WRN_MAP.get(wrn, "ì•Œìˆ˜ì—†ìŒ"),
        "level_code": lvl, "level_name": LVL_MAP.get(lvl, "N/A"),
        "command_code": cmd, "command_name": CMD_MAP.get(cmd, cmd),
        "window_start": tm_st, "window_end": tm_ed,
        "window_start_kst": fmt_kst(tm_st) if tm_st!="N/A" else "N/A",
        "window_end_kst": fmt_kst(tm_ed) if tm_ed!="N/A" else "N/A",
        "announce_time_kst": fmt_kst(tm_st) if cmd=="1" and tm_st!="N/A" else None,
    }
    if wrn=="T" and grd: payload["typhoon_grade"] = grd
    time_bits = []
    if payload["window_start_kst"]!="N/A" and payload["window_end_kst"]!="N/A":
        time_bits.append(f"ê¸°ê°„: {payload['window_start_kst']} ~ {payload['window_end_kst']}")
    elif payload["window_start_kst"]!="N/A":
        time_bits.append(f"ì‹œê°: {payload['window_start_kst']}")
    if payload["announce_time_kst"]:
        time_bits.append(f"ë°œí‘œì‹œê°: {payload['announce_time_kst']}")
    parts = [
        f"ì§€ì—­: {region_name} ({reg_token})", *time_bits,
        f"ì¢…ë¥˜: {payload['hazard_name']}({payload['hazard_code']})",
        f"ìˆ˜ì¤€: {payload['level_name']}({payload['level_code']})",
        f"ëª…ë ¹: {payload['command_name']}({payload['command_code']})"
    ]
    if "typhoon_grade" in payload: parts.append(f"íƒœí’ ë“±ê¸‰: {payload['typhoon_grade']}")
    return {
        "json": json.dumps(payload, ensure_ascii=False, separators=(",", ":")),
        "human": " | ".join(parts)
    }

def fetch_kma_alerts(start_time: str, end_time: str, disp: str="1") -> List[Dict[str, str]]:
    if not WHEATHER_API_KEY_HUB: return []
    base = "https://apihub.kma.go.kr/api/typ01/url/wrn_met_data.php"
    params = {"authKey":WHEATHER_API_KEY_HUB, "wrn":"", "tmfc1":start_time, "tmfc2":end_time, "disp":disp}
    r = requests.get(base, params=params, timeout=KMA_TIMEOUT)
    r.raise_for_status()
    text = r.content.decode("euc-kr", errors="ignore")
    docs, seen = [], set()
    for line in [ln for ln in text.strip().split("\n") if ln.strip() and not ln.startswith("#") and not ln.startswith("7777END")]:
        raw = line.strip().rstrip("=").split(",")
        if len(raw) < 9: continue
        rec = _format_kma_record(raw)
        key = re.sub(r"\s+", " ", rec["json"]).strip()
        if key in seen: continue
        seen.add(key)
        docs.append(rec)
    return docs

def _reverse_region_map() -> Dict[str, List[str]]:
    rev: Dict[str, List[str]] = {}
    for code, name in REGION_MAP.items():
        rev.setdefault(name, []).append(code)
        alias = name.replace(" ", "")
        rev.setdefault(alias, []).append(code)
    return rev

def extract_region_from_question(q: str) -> Optional[str]:
    if not q: return None
    q_norm = re.sub(r"\s+", "", q)
    cand_names = set(REGION_MAP.values())
    for nm in cand_names:
        if nm in q or nm.replace(" ", "") in q_norm: return nm
    commons = ["ìš¸ì§„êµ°", "ìš¸ì§„", "ì˜ë•êµ°", "ì˜ë•", "ë¶€ì‚°ë™ë¶€", "ìš¸ì‚°ë™ë¶€", "ê±°ì œì‹œ", "ë‚¨í•´êµ°", "ì œì£¼ë„ë¶ë¶€", "ì œì£¼ë¶ë¶€", "ì‚¬ì²œì‹œ", "íŒŒì£¼ì‹œ"]
    for token in commons:
        if token in q or token.replace(" ", "") in q_norm:
            for code, name in REGION_MAP.items():
                if token.replace(" ", "") in name.replace(" ", ""): return name
    return None

def summarize_region_alert(q: str, live_docs: List[Dict[str, str]]) -> str:
    """ì§ˆë¬¸ì—ì„œ ì§€ì—­ì„ ë½‘ì•„ ë§¤í•‘ ì‹¤íŒ¨/íŠ¹ë³´ ì—†ìŒ/íŠ¹ë³´ ìˆìŒ ë¬¸êµ¬ë¥¼ ê²°ì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ."""
    region_name = extract_region_from_question(q)
    if not region_name: return "" 
    rev = _reverse_region_map()
    target_codes = rev.get(region_name, [])
    if not target_codes:
        return f"[LIVE_STATUS] {region_name}ì˜ íŠ¹ë³´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì—­ ì½”ë“œ ë§¤í•‘ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    matched = []
    for d in live_docs or []:
        try:
            payload = json.loads(d["json"])
            if payload.get("region_raw") in target_codes: matched.append(payload)
        except Exception: continue
    if not matched:
        return f"[LIVE_STATUS] ì˜¤ëŠ˜ {region_name}ì—ëŠ” ë°œíš¨ ì¤‘ì¸ íŠ¹ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í‰ì†Œì™€ ê°™ì´ ê´€ë¦¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
    matched.sort(key=lambda x: x.get("window_start", ""), reverse=True)
    p = matched[0]
    region, hz = p.get("region_name", region_name), p.get("hazard_name", "íŠ¹ë³´")
    lvl, cmd = p.get("level_name", ""), p.get("command_name", "")
    st, ed = p.get("window_start_kst", ""), p.get("window_end_kst", "")
    bits = [f"{region}ì—ëŠ” {hz} {lvl}ê°€"]
    if st and ed: bits.append(f"{st}ì— ë°œíš¨ë˜ì–´ {ed}ì— í•´ì œë©ë‹ˆë‹¤.")
    elif st: bits.append(f"{st}ì— ë°œíš¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else: bits.append("ë°œíš¨ ì¤‘ì…ë‹ˆë‹¤.")
    if cmd: bits.append(f"(ëª…ë ¹: {cmd})")
    return "[LIVE_STATUS] " + " ".join(bits)

# =========[ ì„ë² ë”© (ë¼ì´ë¸Œ ê²€ìƒ‰ìš©) ]=========
_text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
def embed_texts(texts: List[str]) -> np.ndarray:
    embs = _text_embedder.encode(texts, show_progress_bar=False)
    embs = np.array([l2_normalize(e) for e in embs], dtype="float32")
    return embs

# =========[ ìƒíƒœ/í”„ë¡¬í”„íŠ¸/LLM ]=========
class GraphState(TypedDict):
    question: Optional[str]
    context: Optional[str]
    answer: Optional[str]
    answer_draft: Optional[str]
    store_obj: Optional[Any]
    retry_count: int
    is_context_valid: Optional[bool]
    next_action: Optional[str]

class ValidationResult(BaseModel):
    is_valid: bool
    is_grounded: bool
    reasoning: str

def make_llm() -> ChatGroq:
    if not GROQ_API_KEY: raise ValueError("GROQ_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
    return ChatGroq(model_name=GROQ_MODEL, temperature=TEMPERATURE, api_key=GROQ_API_KEY)

def _should_use_live(q: str) -> bool:
    if FORCE_DISABLE_KMA_LIVE or not USE_KMA_LIVE: return False
    q = (q or "")
    live_kw = ["í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "íŠ¹ë³´", "ì‹¤ì‹œê°„", "ê²½ë³´", "ì£¼ì˜ë³´", "í•´ì œ", "ë°œí‘œ"]
    return any(k in q for k in live_kw)

# =========[ LangGraph ë…¸ë“œ ]=========
def load_store_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (LangChain-FAISS)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True}
    )
    vs = LCFAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return {**state, "store_obj": vs, "retry_count": 0}

def retrieve_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: ê²€ìƒ‰(MMR + KMA ë¼ì´ë¸Œ ë³‘í•©) | ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    q = state["question"] or ""
    live_enabled = _should_use_live(q)
    retriever = state["store_obj"].as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 20})
    docs = retriever.invoke(q)
    persist_scored = [(0.2, d.page_content, "persist") for d in docs]
    live_scored: List[tuple] = []
    live_docs: List[Dict[str, str]] = []
    live_status_msg = ""
    if live_enabled:
        try:
            now = datetime.now()
            tm1 = now.replace(hour=0, minute=0, second=0, microsecond=0).strftime("%Y%m%d%H%M")
            tm2 = now.strftime("%Y%m%d%H%M")
            live_docs = fetch_kma_alerts(tm1, tm2) if WHEATHER_API_KEY_HUB else []
            live_status_msg = summarize_region_alert(q, live_docs)
            if live_docs:
                human_texts = [d["human"] for d in live_docs]
                embs = embed_texts(human_texts)
                live_idx = faiss.IndexFlatIP(embs.shape[1])
                live_idx.add(embs)
                qv = embed_texts([q])[0]
                topk = min(5, len(live_docs))
                D, I = live_idx.search(np.array([qv], dtype="float32"), topk)
                for s, i in zip(D[0], I[0]):
                    if i == -1: continue
                    ctx = f"JSON: {live_docs[i]['json']}\nìš”ì•½: {live_docs[i]['human']}"
                    live_scored.append((float(s) + 1.0, ctx, "live"))
        except Exception:
            pass
    else:
        print("â„¹ï¸ KMA ë¼ì´ë¸Œ í˜¸ì¶œ ê±´ë„ˆëœ€ (ì‹¤ì‹œê°„ ì˜ë„ ì•„ë‹˜ ë˜ëŠ” USE_KMA_LIVE=False)")
    merged = live_scored + persist_scored
    normalized: List[tuple] = []
    bysrc = {"persist": [h for h in merged if h[2]=="persist"], "live": [h for h in merged if h[2]=="live"]}
    for src, hits in bysrc.items():
        if not hits: continue
        scores = [h[0] for h in hits]
        normed = minmax_norm(scores)
        for (orig_s, text, sname), ns in zip(hits, normed):
            normalized.append((ns, text, sname, orig_s))
    seen = set(); dedup = []
    for s, t, src, o in normalized:
        key = re.sub(r"\s+", " ", t.strip())
        if key in seen: continue
        seen.add(key); dedup.append((s, t, src, o))
    dedup.sort(key=lambda x: x[0], reverse=True)
    top = dedup[:5]
    ctx_parts = []
    if live_status_msg: ctx_parts.append(live_status_msg)
    ctx_parts += [f"[ìœ ì‚¬ë„:{s:.4f}][{src}]\n{txt}" for s, txt, src, _ in top]
    context = "\n\n".join(ctx_parts) or "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return {**state, "context": context}

def validate_context_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: 1ì°¨ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦")
    context = state["context"] or ""
    is_valid = not (
        "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." in context or
        (state.get("is_context_valid") is None and "LIVE_STATUS" in context and len(context.split('\n')) < 3)
    )
    if not is_valid:
        print("âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆì¶©ë¶„í•˜ì—¬ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return {**state, "is_context_valid": False, "retry_count": state["retry_count"] + 1}
    print("âœ… ì»¨í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.")
    return {**state, "is_context_valid": True}

def web_search_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: ì›¹ ê²€ìƒ‰ (Tavily Search API ì‚¬ìš©) | í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    if state["retry_count"] >= 2: # ì›¹ ê²€ìƒ‰ì„ 2íšŒ ì´ìƒ ì‹œë„í–ˆë‹¤ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ê³  fallback
        print("âš ï¸ ì›¹ ê²€ìƒ‰ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}
    
    question = state["question"] or ""
    if not TAVILY_API_KEY:
        print("âš ï¸ TAVILY_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "context": state["context"] + "\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼] Tavily API í‚¤ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    client = TavilyClient(api_key=TAVILY_API_KEY)
    try:
        results = client.search(query=question, max_results=TAVILY_MAX_RESULTS)
        web_search_results = "\n".join([
            f"- ì¶œì²˜: {res['url']}\n ë‚´ìš©: {res['content']}"
            for res in results['results']
        ])
        web_result = f"""[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
ì§ˆë¬¸ "{question}"ì— ëŒ€í•œ ìµœì‹  ì •ë³´ì…ë‹ˆë‹¤.
{web_search_results}
"""
        return {**state, "context": state["context"] + "\n\n" + web_result}
    except Exception as e:
        print(f"âŒ Tavily ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {**state, "context": state["context"] + "\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼] ì›¹ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”."}

DRAFT_PROMPT = ChatPromptTemplate.from_template(
    """ë„ˆëŠ” ë†ì‘ë¬¼ ì¬í•´ ëŒ€ì‘ ì „ë¬¸ê°€ì•¼.
ì•„ë˜ ë¬¸ë§¥ì—ëŠ” JSON íŠ¹ë³´ì™€ ì¼ë°˜ ë¬¸ì„œê°€ ì„ì—¬ ìˆì–´. ë¬¸ë§¥ì—ì„œ ì‚¬ì‹¤ë§Œ ì„ ë³„í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆì•ˆ ë‹µë³€ì„ ì‘ì„±í•´.
ë¶ˆë¦¿, ë²ˆí˜¸ ëª©ë¡, ì½”ë“œë¸”ë¡, JSON, í‘œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆ. ì¤‘ë³µë˜ê±°ë‚˜ ìƒì¶©ë˜ëŠ” ë‚´ìš©ì€ ì •ë¦¬í•˜ê³ , íŠ¹ë³´ëŠ” ì§€ì—­, ì¢…ë¥˜, ìˆ˜ì¤€, ë°œí‘œì‹œê°, ê¸°ê°„ì„ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ì´ˆì•ˆ ë‹µë³€:"""
)

def generate_draft_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ì´ˆì•ˆ ìƒì„±")
    if not state.get("question"): raise ValueError("question ëˆ„ë½")
    if not state.get("context"): raise ValueError("context ëˆ„ë½")

    chain = (
        {"context": itemgetter("context"), "question": itemgetter("question")}
        | DRAFT_PROMPT
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"]})
    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer_draft": txt}

REFINE_PROMPT = ChatPromptTemplate.from_template(
    """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼, ê·¸ì— ëŒ€í•œ ì´ˆì•ˆ ë‹µë³€, ê·¸ë¦¬ê³  ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ë§¥ì´ì•¼.
ë„ˆëŠ” ë†ì‘ë¬¼ ì¬í•´ ëŒ€ì‘ ì „ë¬¸ê°€ë¡œì„œ, ì´ˆì•ˆ ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì™„ì „íˆ ì¶©ì¡±í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©(í™˜ê°)ì„ í¬í•¨í•˜ê³  ìˆì§€ëŠ” ì•Šì€ì§€ ì—„ê²©í•˜ê²Œ ê²€í† í•˜ê³  ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì¤˜.
ë§Œì•½ ì´ˆì•ˆ ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë¬¸ë§¥ê³¼ ë§ì§€ ì•ŠëŠ”ë‹¤ë©´, ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•´.
ìµœì¢… ë‹µë³€ì€ í•œêµ­ì–´ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„œì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆ.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ì´ˆì•ˆ ë‹µë³€: {answer_draft}

ìµœì¢… ë‹µë³€:"""
)

def refine_answer_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ë‹µë³€ ê°œì„  ë° ìµœì¢… ìƒì„±")
    if not state.get("question"): raise ValueError("question ëˆ„ë½")
    if not state.get("context"): raise ValueError("context ëˆ„ë½")
    if not state.get("answer_draft"): raise ValueError("answer_draft ëˆ„ë½")

    chain = (
        {"context": itemgetter("context"), "question": itemgetter("question"), "answer_draft": itemgetter("answer_draft")}
        | REFINE_PROMPT
        | make_llm()
        | StrOutputParser()
    )
    ans = chain.invoke({"context": state["context"], "question": state["question"], "answer_draft": state["answer_draft"]})
    txt = re.sub(r'\n{3,}', '\n\n', ans or "").strip()
    return {**state, "answer": txt}

VALIDATION_PROMPT = ChatPromptTemplate.from_template(
    """ì£¼ì–´ì§„ ì§ˆë¬¸, ë‹µë³€, ê·¸ë¦¬ê³  ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ ì—„ê²©í•˜ê²Œ í‰ê°€í•´ì¤˜.
ë„ˆëŠ” ì˜¤ì§ **JSON ê°ì²´**ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•˜ë©°, ì–´ë– í•œ ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë¼. ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¼ì•¼ í•´:

```json
{{
  "is_valid": boolean,
  "is_grounded": boolean,
  "reasoning": string
}}

ì§ˆë¬¸: {question}
ë¬¸ë§¥: {context}
ë‹µë³€: {answer}
ì‘ë‹µ:"""
)

def post_generate_validate_node(state: GraphState) -> Dict[str, Any]:
    print(f"ğŸ§© ë…¸ë“œ: 2ì°¨ ë‹µë³€ ê²€ì¦ (LLM í™œìš©) | ì¬ì‹œë„ íšŸìˆ˜: {state['retry_count']}")
    q = state["question"] or ""
    a = state["answer"] or ""
    ctx = state["context"] or ""

    if state["retry_count"] >= 3:
        print("âš ï¸ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}

    # JsonOutputParser ëŒ€ì‹  StrOutputParserë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ LLM ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë°›ìŒ
    validation_chain = (
        VALIDATION_PROMPT
        | make_llm()
        | StrOutputParser()
    )

    is_valid_answer, is_grounded = False, False

    try:
        raw_output = validation_chain.invoke({"question": q, "context": ctx, "answer": a})
        print(f"LLM ì›ë³¸ ì‘ë‹µ: {raw_output[:100]}...")

        # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì—ì„œ JSON ê°ì²´ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        match = re.search(r'```json\s*(\{.*\})\s*```', raw_output, re.DOTALL)
        if not match:
            match = re.search(r'(\{.*\})', raw_output, re.DOTALL)
        
        if not match:
            # ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì—ëŸ¬ ë°œìƒ
            raise ValueError("LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        json_str = match.group(1)
        json_obj = json.loads(json_str)

        # ì¶”ì¶œëœ JSON ê°ì²´ë¥¼ Pydantic ëª¨ë¸ì— ë„£ì–´ ìœ íš¨ì„± ê²€ì¦
        validation_result = ValidationResult(**json_obj)
        
        is_valid_answer = validation_result.is_valid
        is_grounded = validation_result.is_grounded
        print(f"â¡ï¸ LLM í‰ê°€ ê²°ê³¼: ìœ íš¨ì„±={is_valid_answer}, ê·¼ê±° ê¸°ë°˜={is_grounded}, ì´ìœ : {validation_result.reasoning}")

    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ë˜ëŠ” ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ, ë‹µë³€ì˜ ìœ íš¨ì„±ê³¼ ê·¼ê±° ëª¨ë‘ 'False'ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¬ì‹œë„ ìœ ë„
        is_valid_answer, is_grounded = False, False

    if is_valid_answer and is_grounded:
        print("âœ… ë‹µë³€ì´ ì¶©ë¶„í•˜ê³  ê·¼ê±°ê°€ ëª…í™•í•©ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return {**state, "next_action": "END"}
    elif not is_grounded and state["retry_count"] < 2:
        print("âš ï¸ ë‹µë³€ì´ ë¬¸ë§¥ì— ê·¼ê±°í•˜ì§€ ì•Šì•„ ì¬ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "re_search", "retry_count": state["retry_count"] + 1}
    elif not is_valid_answer:
        print("âš ï¸ ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ì—¬ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "refine_again", "retry_count": state["retry_count"] + 1}
    else:
        print("âš ï¸ ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•˜ì§€ë§Œ, ë” ì´ìƒ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ìµœì¢… ì‹¤íŒ¨ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {**state, "next_action": "fallback"}

def fallback_answer_node(state: GraphState) -> Dict[str, Any]:
    print("ğŸ§© ë…¸ë“œ: ìµœì¢… ì‹¤íŒ¨ ë‹µë³€")
    fallback_message = (
        "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì›¹ ê²€ìƒ‰ì„ ì‹œë„í–ˆìœ¼ë‚˜, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ í™•ë³´í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
        "ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    )
    return {**state, "answer": fallback_message}

# =========[ ê·¸ë˜í”„ ë¹Œë“œ ]=========
def build_graph():
    g = StateGraph(GraphState)
    
    g.add_node("load_store", load_store_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("validate_context", validate_context_node)
    g.add_node("web_search", web_search_node)
    g.add_node("generate_draft", generate_draft_node)
    g.add_node("refine_answer", refine_answer_node)
    g.add_node("post_generate_validate", post_generate_validate_node)
    g.add_node("fallback_answer", fallback_answer_node)

    g.set_entry_point("load_store")
    g.add_edge("load_store", "retrieve")
    g.add_edge("retrieve", "validate_context")

    g.add_conditional_edges(
        "validate_context",
        lambda state: "continue" if state["is_context_valid"] else "fallback",
        {"continue": "generate_draft", "fallback": "web_search"}
    )
    
    # ì›¹ ê²€ìƒ‰ í›„ ë‹µë³€ ì´ˆì•ˆ ìƒì„±ìœ¼ë¡œ
    g.add_edge("web_search", "generate_draft")
    
    # ì´ˆì•ˆ ìƒì„± í›„ ë‹µë³€ ê°œì„ ìœ¼ë¡œ
    g.add_edge("generate_draft", "refine_answer")
    
    # ë‹µë³€ ê°œì„  í›„ ê²€ì¦ìœ¼ë¡œ
    g.add_edge("refine_answer", "post_generate_validate")

    g.add_conditional_edges(
        "post_generate_validate",
        lambda state: state["next_action"],
        {
            "re_search": "web_search",       # ê·¼ê±° ë¶€ì¡± ì‹œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¬ì´ë™
            "refine_again": "refine_answer", # ë‹µë³€ ë¶ˆì™„ì „ ì‹œ ë‹¤ì‹œ ê°œì„ ìœ¼ë¡œ
            "END": END,
            "fallback": "fallback_answer"
        }
    )
    
    g.add_edge("fallback_answer", END)
    
    app = g.compile()
    
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return app

# =========[ í‰ê°€ ìœ í‹¸ ]=========
def _ensure_embedder():
    global _text_embedder
    if _text_embedder is None: _text_embedder = SentenceTransformer(EMBED_MODEL_NAME)
    return _text_embedder

def evaluate_goldenset(app, csv_path: str, limit: int = 50, out_path: str = "evaluation_results.csv"):
    if not os.path.exists(csv_path): raise FileNotFoundError(f"ê³¨ë“ ì…‹ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path)
    def _find_col(cands):
        lc = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in lc: return lc[k]
        for c in df.columns:
            if any(k in c for k in ["ì§ˆë¬¸","question"]): return c
        return None
    q_col = _find_col(["question"])
    a_col = _find_col(["answer","ground_truth","gt"])
    if not q_col or not a_col: raise ValueError(f"CSVì— question/answer ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°œê²¬ëœ ì»¬ëŸ¼: {list(df.columns)})")
    eval_df = df.head(limit).copy()
    preds, scores = [], []
    emb_model = _ensure_embedder()
    for idx, row in eval_df.iterrows():
        q = str(row[q_col]).strip()
        gt = str(row[a_col]).strip()
        try:
            out = app.invoke({"question": q})
            pred = (out.get("answer") or "").strip()
        except Exception as e:
            pred = f"[ì˜¤ë¥˜] {e}"
        if gt and pred and not pred.startswith("[ì˜¤ë¥˜]"):
            vecs = emb_model.encode([gt, pred], show_progress_bar=False)
            vecs = np.array([l2_normalize(v) for v in vecs], dtype="float32")
            sim = float((vecs[0] * vecs[1]).sum())
        else: sim = 0.0
        preds.append(pred)
        scores.append(sim)
        print(f"\n[{idx+1}/{limit}]")
        print(f"ì§ˆë¬¸: {q}")
        print(f"ì •ë‹µ: {gt}")
        print(f"ë‹µë³€: {pred}")
        print(f"ìœ ì‚¬ë„: {sim:.4f}")
        print("-"*50)
    eval_df["prediction"] = preds
    eval_df["cosine_similarity"] = scores
    eval_df["passed@0.75"] = eval_df["cosine_similarity"] >= 0.75
    eval_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\nì „ì²´ ê²°ê³¼ ì €ì¥: {out_path}")

# =========[ ì‹¤í–‰ë¶€ ]=========
if __name__ == "__main__":
    from argparse import ArgumentParser
    import sys
    parser = ArgumentParser(description="Live+Persist RAG: ëŒ€í™”í˜• ê¸°ë³¸ / --eval ì‹œ í‰ê°€")
    parser.add_argument("-q", "--question", default=None, help="í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ")
    parser.add_argument("--show-context", action="store_true", help="ì»¨í…ìŠ¤íŠ¸(ê·¼ê±°)ë„ í•¨ê»˜ ì¶œë ¥")
    parser.add_argument("--eval", dest="eval_csv", default=None, help="ê³¨ë“ ì…‹ CSV ê²½ë¡œ(ì§€ì • ì‹œ í‰ê°€ ëª¨ë“œ)")
    parser.add_argument("--n", dest="limit", type=int, default=50, help="í‰ê°€ í‘œë³¸ ê°œìˆ˜(ìµœëŒ€)")
    parser.add_argument("--out", dest="out_path", default="evaluation_results.csv", help="í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()
    print("ğŸ’¬ LangGraph Live+Persist (LC-FAISS only)")
    app = build_graph()
    if args.eval_csv:
        evaluate_goldenset(app, csv_path=args.eval_csv, limit=args.limit, out_path=args.out_path)
        sys.exit(0)
    if args.question:
        q = args.question.strip()
        if not q: raise ValueError("ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        try:
            out = app.invoke({"question": q})
            if args.show_context:
                print("\n=== ì»¨í…ìŠ¤íŠ¸ ===")
                print(out.get("context", ""))
            print("\n=== ë‹µë³€ ===")
            print(out.get("answer", ""))
            print()
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}\n")
    else:
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit/quit)")
        while True:
            q = input("ì§ˆë¬¸> ").strip()
            if q.lower() in ("exit", "quit"): break
            if not q: continue
            try:
                out = app.invoke({"question": q})
                if args.show_context:
                    print("\n=== ì»¨í…ìŠ¤íŠ¸ ===")
                    print(out.get("context", ""))
                print("\n=== ë‹µë³€ ===")
                print(out.get("answer", ""))
                print()
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")