# ===== hybrid_rag_embed.py =====
import os
import io
import re
import pickle
from glob import glob
from PIL import Image
import numpy as np
import torch
import faiss
import fitz  # PyMuPDF
import easyocr
from sentence_transformers import SentenceTransformer

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤:", device)

# âœ… ëª¨ë¸ ë¡œë“œ
text_model = SentenceTransformer("jhgan/ko-sroberta-multitask", device=str(device))
ocr_reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())

# âœ… ë²¡í„° ì •ê·œí™” í•¨ìˆ˜
def l2_normalize(vec):
    norm = np.linalg.norm(vec)
    return vec / norm if norm > 0 else vec

# âœ… í…ìŠ¤íŠ¸ ì •ì œ ë° ë¶„í• 
def clean_text(text):
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9.,()\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def is_valid_chunk(chunk, min_len=30, max_len=1200):
    chunk = chunk.strip()
    if not (min_len <= len(chunk) <= max_len):
        return False
    if re.fullmatch(r"[^\uac00-\ud7a3a-zA-Z]+", chunk):
        return False
    if len(set(chunk)) <= 3:
        return False
    return True

def deduplicate_chunks(chunks):
    seen = set()
    unique_chunks = []
    for chunk in chunks:
        key = chunk.strip()
        if key not in seen:
            seen.add(key)
            unique_chunks.append(chunk)
    return unique_chunks

def chunk_text(text, max_len=1000, overlap=200):
    text = clean_text(text)
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + max_len, len(text))
        chunk = text[start:end]
        if is_valid_chunk(chunk):
            chunks.append(chunk.strip())
        if end == len(text):
            break
        start += max_len - overlap
    return deduplicate_chunks(chunks)

# âœ… ì„ë² ë”© ìƒì„± (OCR í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
def create_text_embeddings(text_chunks):
    try:
        text_embs = text_model.encode(text_chunks, batch_size=8, convert_to_numpy=True)
        return [(chunk, l2_normalize(emb)) for chunk, emb in zip(text_chunks, text_embs)]
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return []

# âœ… PDF ë° ì´ë¯¸ì§€ ì²˜ë¦¬

def process_pdf_page(page, rag_docs, rag_vecs, source_file, page_num):
    try:
        pix = page.get_pixmap(dpi=200)
        img = Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")
        ocr_results = ocr_reader.readtext(np.array(img), detail=1)
        ocr_text = " ".join(text for _, text, conf in ocr_results if conf >= 0.7)
        page_text = page.get_text()
        combined_text = f"{page_text} {ocr_text}"
        text_chunks = chunk_text(combined_text)
        for chunk, emb in create_text_embeddings(text_chunks):
            rag_docs.append({
                "type": "ocr_text",
                "source": source_file,
                "file": f"{os.path.basename(source_file)}_page{page_num + 1}.png",
                "text": chunk,
                "embedding_model": "ko-sroberta-multitask"
            })
            rag_vecs.append(emb)
    except Exception as e:
        print(f"âŒ PDF í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

def process_image_file(path, rag_docs, rag_vecs):
    try:
        img = Image.open(path).convert("RGB")
        ocr_results = ocr_reader.readtext(np.array(img), detail=1)
        ocr_text = " ".join(text for _, text, conf in ocr_results if conf >= 0.7)
        text_chunks = chunk_text(ocr_text)
        for chunk, emb in create_text_embeddings(text_chunks):
            rag_docs.append({
                "type": "ocr_text",
                "source": path,
                "file": os.path.basename(path),
                "text": chunk,
                "embedding_model": "ko-sroberta-multitask"
            })
            rag_vecs.append(emb)
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

def process_file(path, rag_docs, rag_vecs):
    if path.lower().endswith(".pdf"):
        doc = fitz.open(path)
        for i, page in enumerate(doc):
            process_pdf_page(page, rag_docs, rag_vecs, source_file=path, page_num=i)
    else:
        process_image_file(path, rag_docs, rag_vecs)

if __name__ == "__main__":
    rag_docs, rag_vecs = [], []

    # ğŸ” ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    files = glob("pdfs/*.pdf") + glob("images/*.png") + glob("images/*.jpg") + glob("images/*.jpeg")
    print(f"ğŸ“ ì´ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(files)}ê°œ")

    for idx, path in enumerate(files):
        print(f"\nğŸ“„ ({idx+1}/{len(files)}) íŒŒì¼ ì²˜ë¦¬ ì¤‘: {path}")

        before_chunks = len(rag_docs)
        process_file(path, rag_docs, rag_vecs)
        after_chunks = len(rag_docs)

        print(f"   â• ìƒì„±ëœ ì²­í¬ ìˆ˜: {after_chunks - before_chunks}")
        print(f"   âœ… ëˆ„ì  ì²­í¬ ìˆ˜: {after_chunks}")

    # ğŸ§  FAISS ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
    print("\nğŸ“¦ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    matrix = np.array(rag_vecs).astype("float32")
    index = faiss.IndexFlatL2(matrix.shape[1])
    index.add(matrix)
    print(f"âœ… ì¸ë±ìŠ¤ì— ë²¡í„° {len(rag_vecs)}ê°œ ì¶”ê°€ ì™„ë£Œ")

    # ğŸ’¾ ì €ì¥
    faiss.write_index(index, "multimodal_rag.index")
    with open("multimodal_rag.pkl", "wb") as f:
        pickle.dump(rag_docs, f)

    print("\nğŸ‰ ì €ì¥ ì™„ë£Œ:")
    print(f"   - ì¸ë±ìŠ¤ íŒŒì¼: multimodal_rag.index")
    print(f"   - ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: multimodal_rag.pkl")
    print(f"   - ì´ ì €ì¥ ì²­í¬ ìˆ˜: {len(rag_docs)}")
