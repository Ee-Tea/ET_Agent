# -*- coding: utf-8 -*-
"""
PDF í…ìŠ¤íŠ¸ + (í•„ìš” ì‹œ) OCR, ì´ë¯¸ì§€ OCRê¹Œì§€ í¬í•¨í•˜ì—¬
LangChain-FAISS ë””ë ‰í† ë¦¬(index.faiss/index.pkl)ì— ì €ì¥í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì„œ.

- ê¸°ë³¸ íë¦„: ë¡œë“œ â†’ ì •ì œ/ë¶„í•  â†’ ì„ë² ë”© â†’ FAISS ì €ì¥/ê°±ì‹ 
- PDFëŠ” PyPDFLoaderë¡œ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , í˜ì´ì§€ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ PyMuPDF+easyocrë¡œ OCR ë³´ì¶©
- ì´ë¯¸ì§€(jpg/png/jpeg)ëŠ” easyocrë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ í¬í•¨(ì˜µì…˜)

í•„ìš” env (.env):
  PDF_DIR=./cropinfo
  IMAGE_DIR=./images               # ì„ íƒ
  VECTOR_DB_PATH=faiss_pdf_db
  EMBED_MODEL_NAME=jhgan/ko-sroberta-multitask
  CHUNK_SIZE=900
  CHUNK_OVERLAP=150
  USE_OCR=1                        # 1ì´ë©´ OCR ì‚¬ìš©, 0ì´ë©´ ë¯¸ì‚¬ìš©
  OCR_LANGS=ko,en                  # easyocr ì–¸ì–´
  OCR_CONF=0.7                     # easyocr ìµœì†Œ ì‹ ë¢°ë„
  OCR_TRIGGER_LEN=80               # PDF í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì´ ê°’ ë¯¸ë§Œì´ë©´ OCR ë³´ì¶©
  FORCE_OCR=0                      # 1ì´ë©´ PDF ì „ í˜ì´ì§€ OCR ê°•ì œ
"""

import os
import re
from glob import glob
from typing import List, Any

from dotenv import load_dotenv
load_dotenv()

# ===== ì„¤ì • =====
PDF_DIR = os.getenv("PDF_DIR", "./pdfs")
IMAGE_DIR = os.getenv("IMAGE_DIR", "").strip()
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "faiss_kma_db")

EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

USE_OCR = os.getenv("USE_OCR", "1") == "1"
OCR_LANGS = [s.strip() for s in os.getenv("OCR_LANGS", "ko,en").split(",") if s.strip()]
OCR_CONF = float(os.getenv("OCR_CONF", "0.7"))
OCR_TRIGGER_LEN = int(os.getenv("OCR_TRIGGER_LEN", "80"))
FORCE_OCR = os.getenv("FORCE_OCR", "0") == "1"

# ===== LangChain =====
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ===== OCR / PDF ë³´ì¡° =====
import fitz  # PyMuPDF
import easyocr
from PIL import Image
import numpy as np
import io

# ===== í…ìŠ¤íŠ¸ ì •ì œ/ë¶„í•  =====
def clean_text(text: str) -> str:
    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ì¼ë¶€ êµ¬ë‘ì ë§Œ ë‚¨ê¸°ê³  ê³µë°± ì •ë¦¬
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9.,()/%:\-\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def make_splitter() -> RecursiveCharacterTextSplitter:
    return RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
    )

# ===== OCR ìœ í‹¸ =====
_ocr_reader = None
def get_ocr_reader():
    global _ocr_reader
    if _ocr_reader is None:
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìë™ í™œìš©
        import torch
        _ocr_reader = easyocr.Reader(OCR_LANGS, gpu=torch.cuda.is_available())
    return _ocr_reader

def ocr_image_pil(img: Image.Image) -> str:
    reader = get_ocr_reader()
    arr = np.array(img.convert("RGB"))
    results = reader.readtext(arr, detail=1)
    txts = [t for _, t, conf in results if conf >= OCR_CONF]
    return " ".join(txts).strip()

def ocr_pdf_page(page: fitz.Page, dpi: int = 200) -> str:
    pix = page.get_pixmap(dpi=dpi)
    img = Image.open(io.BytesIO(pix.tobytes("png")))
    return ocr_image_pil(img)

# ===== PDF ë¡œë“œ(+OCR ë³´ê°•) =====
def load_pdfs_with_optional_ocr(pdf_dir: str) -> List[Document]:
    paths = sorted(glob(os.path.join(pdf_dir, "*.pdf")))
    if not paths:
        raise FileNotFoundError(f"PDFê°€ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")

    all_docs: List[Document] = []
    splitter = make_splitter()

    for p in paths:
        print(f"ğŸ“„ PDF ë¡œë“œ: {os.path.basename(p)}")
        # 1) ìš°ì„  PyPDFLoaderë¡œ í˜ì´ì§€ ë‹¨ìœ„ ë¡œë“œ
        pages = []
        try:
            pages = PyPDFLoader(p).load()
        except Exception as e:
            print(f"â— PyPDFLoader ì‹¤íŒ¨: {p} -> {e}")

        # 2) í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ í™•ì¸ & OCR ë³´ê°•
        #    - FORCE_OCRì´ë©´ ë¬´ì¡°ê±´ OCR ìˆ˜í–‰í•´ í…ìŠ¤íŠ¸ì— ë§ë¶™ì„
        #    - ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ OCR_TRIGGER_LEN ë¯¸ë§Œì¼ ë•Œë§Œ OCR ìˆ˜í–‰
        try:
            doc_fitz = fitz.open(p)
        except Exception as e:
            print(f"â— PyMuPDF ì—´ê¸° ì‹¤íŒ¨: {p} -> {e}")
            doc_fitz = None

        processed_docs: List[Document] = []

        for i, d in enumerate(pages):
            base_txt = clean_text(d.page_content or "")
            ocr_txt = ""
            need_ocr = FORCE_OCR or (len(base_txt) < OCR_TRIGGER_LEN)
            if USE_OCR and need_ocr and doc_fitz:
                try:
                    ocr_txt = clean_text(ocr_pdf_page(doc_fitz[i]))
                except Exception as e:
                    print(f"â— OCR ì‹¤íŒ¨: {p} page={i+1} -> {e}")

            merged = (base_txt + " " + ocr_txt).strip() if ocr_txt else base_txt
            if not merged:
                continue

            # ë¶„í•  í›„ Documentë¡œ ë³€í™˜(ë©”íƒ€ ìœ ì§€)
            for chunk in splitter.split_text(merged):
                processed_docs.append(
                    Document(
                        page_content=chunk,
                        metadata={
                            **(d.metadata or {}),
                            "source": d.metadata.get("source", p),
                            "page": d.metadata.get("page", i + 1),
                            "ocr_applied": bool(ocr_txt),
                            "file_name": os.path.basename(p),
                            "type": "pdf",
                        },
                    )
                )

        print(f"âœ… ë¶„í•  ì²­í¬ ìˆ˜: {len(processed_docs)} (OCR ì ìš© í˜ì´ì§€ í¬í•¨)")
        all_docs.extend(processed_docs)

    print(f"ğŸ“š PDF ì´ ì²­í¬ ìˆ˜: {len(all_docs)}")
    return all_docs

# ===== ì´ë¯¸ì§€ OCR ë¡œë“œ(ì˜µì…˜) =====
def load_images_ocr(image_dir: str) -> List[Document]:
    if not image_dir:
        return []
    paths = []
    for ext in ("*.png", "*.jpg", "*.jpeg"):
        paths.extend(glob(os.path.join(image_dir, ext)))
    paths = sorted(paths)
    if not paths:
        return []

    splitter = make_splitter()
    docs: List[Document] = []

    print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë“œ: {len(paths)}ê°œ (dir={image_dir})")
    for p in paths:
        try:
            img = Image.open(p).convert("RGB")
            txt = clean_text(ocr_image_pil(img)) if USE_OCR else ""
            if not txt:
                continue
            for chunk in splitter.split_text(txt):
                docs.append(
                    Document(
                        page_content=chunk,
                        metadata={
                            "source": p,
                            "file_name": os.path.basename(p),
                            "type": "image",
                            "ocr_applied": True,
                        },
                    )
                )
        except Exception as e:
            print(f"â— ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {p} -> {e}")

    print(f"âœ… ì´ë¯¸ì§€ ì²­í¬ ìˆ˜: {len(docs)}")
    return docs

# ===== ì¸ë±ìŠ¤ ìƒì„±/ê°±ì‹  =====
def build_or_update_faiss(docs: List[Document], db_path: str) -> None:
    if not docs:
        print("âš ï¸ ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)

    if os.path.exists(db_path):
        print(f"ğŸ“¦ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: {db_path}")
        vs = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        print("â• ìƒˆ ì²­í¬ ì¶”ê°€ í›„ ì¸ë±ìŠ¤ ì €ì¥â€¦")
        vs.add_documents(docs)
        vs.save_local(db_path)
    else:
        print("ğŸ§± ìƒˆ ì¸ë±ìŠ¤ ìƒì„±â€¦")
        vs = FAISS.from_documents(docs, embeddings)
        vs.save_local(db_path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {db_path} (index.faiss / index.pkl)")

# ===== ë©”ì¸ =====
if __name__ == "__main__":
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘")
    all_docs: List[Document] = []

    # 1) PDF(+OCR ë³´ê°•)
    pdf_docs = load_pdfs_with_optional_ocr(PDF_DIR)
    all_docs.extend(pdf_docs)

    # 2) ì´ë¯¸ì§€ OCR (ì˜µì…˜)
    if IMAGE_DIR:
        img_docs = load_images_ocr(IMAGE_DIR)
        all_docs.extend(img_docs)

    # 3) ì¸ë±ìŠ¤ ë¹Œë“œ/ê°±ì‹ 
    build_or_update_faiss(all_docs, VECTOR_DB_PATH)
    print("ğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ")
