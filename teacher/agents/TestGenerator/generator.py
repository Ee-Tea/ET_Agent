import os
import glob
from typing import List, Dict, Any, TypedDict
from abc import ABC, abstractmethod
# from langchain_community.document_loaders import PyPDFLoader   # FAISS ê²½ë¡œ ì œê±°: ë¯¸ì‚¬ìš©
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import FAISS             # FAISS ì œê±°
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langgraph.graph import StateGraph, END
import json
import re
from collections import Counter
import time
from datetime import datetime
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•œ ì„í¬íŠ¸
from dotenv import load_dotenv
import sys
import os

# ìƒëŒ€ ì„í¬íŠ¸ ëŒ€ì‹  ì ˆëŒ€ ê²½ë¡œë¡œ ì„í¬íŠ¸
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from base_agent import BaseAgent

# OpenAI ê´€ë ¨ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings

# ğŸ” Milvus ê´€ë ¨ ì„í¬íŠ¸
from langchain_milvus import Milvus
from pymilvus import connections, utility, Collection, DataType

# milvus_store ìœ í‹¸ ë¶ˆëŸ¬ì˜¤ê¸° (íŒ¨í‚¤ì§€ êµ¬ì¡°ì— ë”°ë¼ ìƒëŒ€/ì ˆëŒ€ ì„í¬íŠ¸ í˜¸í™˜)
try:
    from .milvus_store import load_questions_from_json
except ImportError:
    from milvus_store import load_questions_from_json

# ğŸ” RAGAS ê´€ë ¨ ì„í¬íŠ¸ ì¶”ê°€
try:
    from ragas import evaluate
    from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
    from datasets import Dataset
    from ragas.llms import llm_factory
    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    print("âš ï¸ RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í’ˆì§ˆ ê²€ì¦ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.0"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))
LLM_TIMEOUT = int(os.getenv("LLM_TIMEOUT", "120"))
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "3"))

# ğŸ” RAGAS í’ˆì§ˆ ê²€ì¦ ì„¤ì •
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ:
# RAGAS_ENABLED=true                    # RAGAS í™œì„±í™” (true/false)
# RAGAS_QUALITY_THRESHOLD=0.7          # í’ˆì§ˆ ì„ê³„ê°’ (0.0 ~ 1.0)
# RAGAS_MAX_ATTEMPTS=3                 # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
# OPENAI_API_KEY=your_api_key_here    # OpenAI API í‚¤ (RAGAS ê²€ì¦ìš©)
RAGAS_QUALITY_THRESHOLD = float(os.getenv("RAGAS_QUALITY_THRESHOLD", "0.5"))  # 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶¤
RAGAS_MAX_ATTEMPTS = int(os.getenv("RAGAS_MAX_ATTEMPTS", "3"))
RAGAS_ENABLED = os.getenv("RAGAS_ENABLED", "true").lower() == "true"

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class GraphState(TypedDict):
    """ê·¸ë˜í”„ ìƒíƒœ ì •ì˜"""
    query: str
    documents: List[Document]
    context: str
    quiz_questions: List[Dict[str, Any]]
    difficulty: str
    error: str
    used_sources: List[str]
    generation_attempts: int
    target_quiz_count: int
    subject_area: str
    validated_questions: List[Dict[str, Any]]  # ë¬¸ì œì— ë‹µ í•´ì„¤ê¹Œì§€ í•œ ë²ˆì— ë‚˜ì˜´, ë³´ê¸°ëŠ” 1. 2. 3. 4. ìœ¼ë¡œ ë²ˆí˜¸ê°€ ë¶™ìŒ, ë¬¸ì œì—ëŠ” ë²ˆí˜¸ ì•ˆ ë¶™ìŒ
    ragas_score: float  # ğŸ” RAGAS í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€


class InfoProcessingExamAgent(BaseAgent):
    """
    ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œê¸°ì¤€ì— ë§ëŠ” ìë™ ì¶œì œ ì—ì´ì „íŠ¸
    - full_exam: 5ê³¼ëª© Ã— 20ë¬¸í•­ = ì´ 100ë¬¸í•­
    - subject_quiz: íŠ¹ì • ê³¼ëª© ìµœëŒ€ 40ë¬¸í•­
    - ê³¼ëª©ë³„ ìƒì„±/ê²€ì¦ ë…¸ë“œ 2ê°œ(ì´ 10ê°œ)
    - ì‚¬ìš©ì ì§€ì • ë³‘ë ¬ ì‹¤í–‰
    - ë¨¸ì§€ ìˆœì„œ ê³ ì •
    """

    # 1) ê³¼ëª©/í‚¤ì›Œë“œ + full_exam ê¸°ë³¸ ì¹´ìš´íŠ¸(20)ë¡œ ë³€ê²½
    SUBJECT_AREAS = {
        "ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„": {
            "count": 20,
            "keywords": ["ìš”êµ¬ì‚¬í•­", "UI ì„¤ê³„", "ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„", "ì¸í„°í˜ì´ìŠ¤", "UML", "ê°ì²´ì§€í–¥", "ë””ìì¸íŒ¨í„´", "ëª¨ë“ˆí™”", "ê²°í•©ë„", "ì‘ì§‘ë„"]
        },
        "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ": {
            "count": 20,
            "keywords": ["ìë£Œêµ¬ì¡°", "ìŠ¤íƒ", "í", "ë¦¬ìŠ¤íŠ¸", "í†µí•©êµ¬í˜„", "ëª¨ë“ˆ", "íŒ¨í‚¤ì§•", "í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤", "ì•Œê³ ë¦¬ì¦˜", "ì¸í„°í˜ì´ìŠ¤"]
        },
        "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•": {
            "count": 20,
            "keywords": ["SQL", "íŠ¸ë¦¬ê±°", "DML", "DDL", "DCL", "ì •ê·œí™”", "ê´€ê³„í˜•ëª¨ë¸", "E-Rëª¨ë¸", "ë°ì´í„°ëª¨ë¸ë§", "ë¬´ê²°ì„±"]
        },
        "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©": {
            "count": 20,
            "keywords": ["ê°œë°œí™˜ê²½", "í”„ë¡œê·¸ë˜ë°ì–¸ì–´", "ë¼ì´ë¸ŒëŸ¬ë¦¬", "ìš´ì˜ì²´ì œ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°íƒ€ì…", "ë³€ìˆ˜", "ì—°ì‚°ì"]
        },
        "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬": {
            "count": 20,
            "keywords": ["ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œë°©ë²•ë¡ ", "í”„ë¡œì íŠ¸ê´€ë¦¬", "ë³´ì•ˆ", "ì‹œìŠ¤í…œë³´ì•ˆ", "ë„¤íŠ¸ì›Œí¬ë³´ì•ˆ", "í…Œì¼ëŸ¬ë§", "ìƒëª…ì£¼ê¸°ëª¨ë¸"]
        }
    }

    # 4) ìµœì¢… ë¨¸ì§€ ìˆœì„œ
    MERGE_ORDER = [
        "ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„",
        "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ",
        "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•",
        "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©",
        "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬",
    ]

    def __init__(self, data_folder=None, groq_api_key=None):
        if data_folder is None:
            base_dir = Path(__file__).resolve().parent
            data_folder = base_dir / "data"
        self.data_folder = Path(data_folder)
        os.makedirs(self.data_folder, exist_ok=True)

        if groq_api_key:
            os.environ["OPENAI_API_KEY"] = groq_api_key
        elif not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.embeddings_model = None
        self.llm = None
        self.vectorstore = None
        self.retriever = None
        self.workflow = None

        self.files_in_vectorstore = []

        # ğŸ”§ Milvus ì ‘ì†/ê²€ìƒ‰ ê¸°ë³¸ê°’
        self.milvus_conf = {
            "host": os.getenv("MILVUS_HOST", "localhost"),
            "port": os.getenv("MILVUS_PORT", "19530"),
            "collection": os.getenv("MILVUS_COLLECTION", "info_exam_chunks"),
            "topk": int(os.getenv("MILVUS_TOPK", "15")),
        }

        self._initialize_models()
        self._build_graph()  # 2) ê³¼ëª©ë³„ 2ë…¸ë“œ(ìƒì„±/ê²€ì¦) êµ¬ì¶•

    # --- Subject helpers: spacing-insensitive matching ---
    def _normalize_subject(self, s: str) -> str:
        try:
            return re.sub(r"\s+", "", (s or "")).strip()
        except Exception:
            return (s or "").strip()

    def _subject_aliases(self, subject: str) -> List[str]:
        base = self._normalize_subject(subject)
        alias_map = {
            "ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„": ["ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„", "ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„"],
            "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ": ["ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ", "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ"],
            "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•": ["ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•", "ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"],
            "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©": ["í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©", "í”„ë¡œê·¸ë˜ë°ì–¸ì–´ í™œìš©", "í”„ë¡œê·¸ë˜ë° ì–¸ì–´ í™œìš©"],
            "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬": ["ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬", "ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ê´€ë¦¬", "ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶• ê´€ë¦¬"],
        }
        for key, aliases in alias_map.items():
            if base == self._normalize_subject(key):
                return aliases
        # ê¸°ë³¸: ì…ë ¥ ê·¸ëŒ€ë¡œì™€ ê³µë°± ì œê±°í˜• ë‘˜ ë‹¤ ì‹œë„
        uniq = []
        for cand in [subject, base]:
            if cand and cand not in uniq:
                uniq.append(cand)
        return uniq

    @property
    def name(self) -> str:
        return "InfoProcessingExamAgent"

    @property
    def description(self) -> str:
        return "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ 5ê³¼ëª© ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±/ê²€ì¦í•˜ì—¬ 100ë¬¸ì œ(ë˜ëŠ” ê³¼ëª©ë³„ ì§€ì • ìˆ˜)ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤."

    def invoke(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Args (í™•ì¥):
          - mode: "full_exam" | "subject_quiz" | "partial_exam"
          - difficulty: "ì´ˆê¸‰" | "ì¤‘ê¸‰" | "ê³ ê¸‰" (default: "ì¤‘ê¸‰")
          - subject_area: subject_quiz ëª¨ë“œì—ì„œ í•„ìˆ˜
          - target_count: subject_quiz ëª¨ë“œì—ì„œ ìš”ì²­ ë¬¸í•­ ìˆ˜ (ìµœëŒ€ 40)
          - selected_subjects: partial_exam ëª¨ë“œì—ì„œ ì„ íƒí•  ê³¼ëª© ë¦¬ìŠ¤íŠ¸
          - questions_per_subject: partial_exam ëª¨ë“œì—ì„œ ê³¼ëª©ë‹¹ ë¬¸ì œ ìˆ˜
          - parallel_agents: ë™ì‹œ ë³‘ë ¬ ì‹¤í–‰ ê°œìˆ˜ (default: 2, ê¶Œì¥: 2~5)
          - save_to_file: bool
          - filename: ì €ì¥ íŒŒì¼ëª…
          - milvus_question_path: (ì„ íƒ) JSON ë¬¸ì œì€í–‰ ê²½ë¡œ. ì§€ì • ì‹œ í•´ë‹¹ JSONì„ Milvusì— ì ì¬ í›„ ê²€ìƒ‰ì— ì‚¬ìš©
        """
        try:
            mode = input_data.get("mode", "full_exam")
            difficulty = input_data.get("difficulty", "ì¤‘ê¸‰")
            save_to_file = input_data.get("save_to_file", False)
            filename = input_data.get("filename")
            parallel_agents = max(1, int(input_data.get("parallel_agents", 2)))  # 3) ë³‘ë ¬ ê°œìˆ˜

            # âœ… Milvus ì‚¬ìš© (ê¸°ë³¸)
            if not self._build_retriever_from_milvus(input_data):
                return {
                    "success": False,
                    "error": "Milvus ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì»¬ë ‰ì…˜/ì ‘ì†/ì„ë² ë”© ì„¤ì • í™•ì¸)"
                }

            if mode == "full_exam":
                # 1) 5ê³¼ëª© Ã— 20ë¬¸í•­ = ì´ 100ë¬¸í•­
                result = self._generate_full_exam(difficulty=difficulty,
                                                  parallel_agents=parallel_agents)
            elif mode == "partial_exam":
                # ì„ íƒëœ ê³¼ëª©ë“¤ì— ëŒ€í•´ ì§€ì •ëœ ë¬¸ì œ ìˆ˜ë§Œí¼ ìƒì„±
                selected_subjects = input_data.get("selected_subjects", [])
                questions_per_subject = input_data.get("questions_per_subject", 10)
                
                if not selected_subjects or not isinstance(selected_subjects, list):
                    return {
                        "success": False,
                        "error": "partial_exam ëª¨ë“œì—ì„œëŠ” selected_subjects ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    }
                
                if not all(subj in self.SUBJECT_AREAS for subj in selected_subjects):
                    invalid_subjects = [subj for subj in selected_subjects if subj not in self.SUBJECT_AREAS]
                    return {
                        "success": False,
                        "error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©ëª…ì…ë‹ˆë‹¤: {invalid_subjects}. ê°€ëŠ¥í•œ ê³¼ëª©: {list(self.SUBJECT_AREAS.keys())}"
                    }
                
                result = self._generate_partial_exam(
                    selected_subjects=selected_subjects,
                    questions_per_subject=questions_per_subject,
                    difficulty=difficulty,
                    parallel_agents=parallel_agents
                )
            elif mode == "subject_quiz":
                subject_area = input_data.get("subject_area")
                if not subject_area or subject_area not in self.SUBJECT_AREAS:
                    return {
                        "success": False,
                        "error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©ëª…ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê³¼ëª©: {list(self.SUBJECT_AREAS.keys())}"
                    }
                # ìµœëŒ€ 40ê°œ ì œí•œ
                target_count = min(int(input_data.get("target_count", 20)), 40)
                result = self._generate_subject_quiz(
                    subject_area=subject_area,
                    target_count=target_count,
                    difficulty=difficulty
                )
                # subject_quizëŠ” ë‹¨ì¼ ê³¼ëª© ê²°ê³¼ë§Œ ë¦¬í„´
                if "error" in result:
                    return {"success": False, "error": result["error"]}
                response = {"success": True, "result": result}
                if save_to_file:
                    try:
                        file_path = self._save_to_json(result, filename)
                        response["file_path"] = file_path
                    except Exception as e:
                        response["save_error"] = str(e)
                return response
            else:
                return {"success": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œì…ë‹ˆë‹¤. 'full_exam' ë˜ëŠ” 'subject_quiz'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."}

            if "error" in result:
                return {"success": False, "error": result["error"]}

            response = {"success": True, "result": result}
            if save_to_file:
                try:
                    file_path = self._save_to_json(result, filename)
                    response["file_path"] = file_path
                except Exception as e:
                    response["save_error"] = str(e)
            return response

        except Exception as e:
            return {"success": False, "error": f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def _initialize_models(self):
        try:
            self.embeddings_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            self.llm = ChatOpenAI(
                model=OPENAI_LLM_MODEL,
                temperature=LLM_TEMPERATURE,
                max_tokens=LLM_MAX_TOKENS,
                timeout=LLM_TIMEOUT,
                max_retries=LLM_MAX_RETRIES,
                base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"),
                api_key=os.getenv("OPENAI_API_KEY")
            )
            _ = self.llm.invoke("ping")
        except Exception as e:
            raise ValueError(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # âœ… Milvus ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” (milvus_store ì‚¬ìš© ë˜ëŠ” ê¸°ì¡´ ì»¬ë ‰ì…˜ ì ‘ì†)
    def _build_retriever_from_milvus(self, input_data: Dict[str, Any]) -> bool:
        """
        ìš°ì„ ìˆœìœ„:
        1) input_data['milvus_question_path'] ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ MILVUS_QUESTION_PATH ê°€ ìˆìœ¼ë©´
           milvus_store.load_questions_from_jsonìœ¼ë¡œ ì ì¬+ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        2) ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ì ‘ì†í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        """
        try:
            question_path = input_data.get("milvus_question_path") or os.getenv("MILVUS_QUESTION_PATH")
            host = self.milvus_conf["host"]
            port = self.milvus_conf["port"]
            collection = self.milvus_conf["collection"]
            topk = self.milvus_conf["topk"]

            # ì—°ê²° ì •ë¦¬ í›„ ì¬ì ‘ì†
            if "default" in connections.list_connections():
                connections.disconnect(alias="default")
            connections.connect(alias="default", host=host, port=port)

            if question_path and os.path.exists(question_path):
                print(f"ğŸ†• JSON ë¬¸ì œì€í–‰ ì ì¬ í›„ ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„: {question_path}")
                out = load_questions_from_json({
                    "question_path": question_path,
                    "collection_name": collection,
                    "milvus_host": host,
                    "milvus_port": port,
                    "drop_old": False,     # í•„ìš” ì‹œ Trueë¡œ ì´ˆê¸°í™”
                    "k": topk,
                })
                self.vectorstore = out["vectorstore"]
                self.retriever = out["retriever"]
                self.files_in_vectorstore = []
                return True

            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì ‘ì†
            if not utility.has_collection(collection):
                print(f"[DEBUG] Milvus ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {collection} â†’ ê²€ìƒ‰ ì—†ì´ ì§„í–‰(í´ë°±)")
                self.vectorstore = None
                self.retriever = None
                return True

            print(f"âœ… ê¸°ì¡´ Milvus ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection}")
            # solution_agent ë°©ì‹: í•„ë“œ ìë™ ì¶”ë¡  + L2 ë©”íŠ¸ë¦­
            def infer_fields(coll_name: str):
                c = Collection(coll_name)
                vec_field, text_field, dim = None, None, None
                for f in c.schema.fields:
                    if f.dtype == DataType.FLOAT_VECTOR and vec_field is None:
                        vec_field = f.name
                        try:
                            dim = int(f.params.get("dim") or 0)
                        except Exception:
                            dim = None
                candidates = ("text", "page_content", "content", "question", "item_title", "title")
                varchar_fields = [f.name for f in c.schema.fields if f.dtype == DataType.VARCHAR]
                for name in candidates:
                    if name in varchar_fields:
                        text_field = name
                        break
                if text_field is None and varchar_fields:
                    text_field = varchar_fields[0]
                if vec_field is None:
                    raise RuntimeError(f"[Milvus] '{coll_name}'ì— FLOAT_VECTOR í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                print(f"[Milvus] '{coll_name}' â†’ text_field='{text_field}', vector_field='{vec_field}', dim={dim}")
                return text_field, vec_field, dim

            txt_f, vec_f, _ = infer_fields(collection)
            self.vectorstore = Milvus(
                embedding_function=self.embeddings_model,
                collection_name=collection,
                connection_args={"host": host, "port": port},
                text_field=txt_f,
                vector_field=vec_f,
                index_params={"index_type": "AUTOINDEX", "metric_type": "L2"},
                search_params={"metric_type": "L2"},
            )
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": topk})
            self.files_in_vectorstore = []
            return True

        except Exception as e:
            print(f"[DEBUG] _build_retriever_from_milvus error: {e} â†’ ê²€ìƒ‰ ì—†ì´ ì§„í–‰(í´ë°±)")
            self.vectorstore = None
            self.retriever = None
            return True

    # (ì´ì „ FAISS ê²½ë¡œ ëŒ€ì²´: ì´ë¦„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ë„ ë‚´ë¶€ëŠ” Milvus ì‚¬ìš©)
    def _build_vectorstore_from_all_pdfs(self) -> bool:
        """
        ê³¼ê±° FAISS êµ¬ì¶• í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ë‚¨ê²¨ë‘ì§€ë§Œ,
        í˜„ì¬ëŠ” Milvus ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        return self._build_retriever_from_milvus({})

    def get_pdf_files(self) -> List[str]:
        # Milvus ì‚¬ìš©ìœ¼ë¡œ ì˜ë¯¸ëŠ” ì ì§€ë§Œ, ì™¸ë¶€ í˜¸í™˜ì„ ìœ„í•´ ë‚¨ê¹€
        return glob.glob(os.path.join(self.data_folder, "*.pdf"))

    # ---- ê³µí†µ ë…¸ë“œ êµ¬í˜„(ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©) ----
    def _retrieve_documents(self, state: GraphState) -> GraphState:
        try:
            query = state["query"]
            subject_area = state.get("subject_area", "")
            enhanced_query = f"{subject_area} {query}".strip()
            print(f"[DEBUG] _retrieve_documents: query='{query}', subject_area='{subject_area}', enhanced_query='{enhanced_query}'")
            topk = self.milvus_conf["topk"]
            documents: List[Document] = []
            # ê³¼ëª©ëª…ì´ DBì— ê³µë°± í¬í•¨/ë¯¸í¬í•¨ìœ¼ë¡œ ì„ì—¬ìˆëŠ” ê²½ìš°ë¥¼ ëª¨ë‘ ì»¤ë²„
            subject_aliases = self._subject_aliases(subject_area) if subject_area else []
            expr = None
            if subject_aliases:
                if len(subject_aliases) == 1:
                    expr = f"subject == '{subject_aliases[0]}'"
                else:
                    or_clauses = [f"subject == '{a}'" for a in subject_aliases]
                    expr = " or ".join(or_clauses)
            try:
                # 1) expr í¬í•¨ ê²€ìƒ‰
                if self.vectorstore and hasattr(self.vectorstore, "similarity_search"):
                    if expr:
                        documents = self.vectorstore.similarity_search(enhanced_query, k=topk, expr=expr)
                    else:
                        documents = self.vectorstore.similarity_search(enhanced_query, k=topk)
                elif self.retriever:
                    if expr and hasattr(self.retriever, "search_kwargs"):
                        self.retriever.search_kwargs.update({"expr": expr})
                    documents = self.retriever.invoke(enhanced_query)
            except Exception as e:
                print(f"[DEBUG] _retrieve_documents: primary search failed ({e}), fallback without expr")
                documents = []

            # 2) ê²°ê³¼ê°€ ë¹„ì–´ ìˆìœ¼ë©´ expr ì œê±° í›„ ì¬ê²€ìƒ‰
            if not documents:
                try:
                    if self.vectorstore and hasattr(self.vectorstore, "similarity_search"):
                        documents = self.vectorstore.similarity_search(enhanced_query, k=topk)
                    elif self.retriever:
                        if hasattr(self.retriever, "search_kwargs") and "expr" in self.retriever.search_kwargs:
                            self.retriever.search_kwargs.pop("expr", None)
                        documents = self.retriever.invoke(enhanced_query)
                    print(f"[DEBUG] _retrieve_documents: fallback without expr â†’ {len(documents)} docs")
                except Exception as e2:
                    print(f"[DEBUG] _retrieve_documents: fallback without expr failed ({e2})")
                    documents = []

            # 3) ì—¬ì „íˆ ë¹„ì–´ ìˆìœ¼ë©´ ì£¼ì œ ì ‘ë‘ì‚¬ ì—†ì´ ìˆœìˆ˜ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
            if not documents and query and query != enhanced_query:
                try:
                    if self.vectorstore and hasattr(self.vectorstore, "similarity_search"):
                        documents = self.vectorstore.similarity_search(query, k=topk)
                    elif self.retriever:
                        documents = self.retriever.invoke(query)
                    print(f"[DEBUG] _retrieve_documents: fallback plain query â†’ {len(documents)} docs")
                except Exception as e3:
                    print(f"[DEBUG] _retrieve_documents: plain query search failed ({e3})")
                    documents = []

            if subject_area and documents:
                # ë©”íƒ€ë°ì´í„° subjectë„ ê³µë°± ë¬´ì‹œí•˜ê³  ë³„ì¹­ìœ¼ë¡œ ì •ê·œí™” ë§¤ì¹­
                aliases_norm = {self._normalize_subject(a) for a in subject_aliases}
                filtered = []
                for d in documents:
                    meta_subj_norm = self._normalize_subject(d.metadata.get('subject') or '')
                    if meta_subj_norm in aliases_norm:
                        filtered.append(d)
                if filtered:
                    documents = filtered

            print(f"[DEBUG] _retrieve_documents: found {len(documents)} documents")
            # Milvus ë¬¸ì„œì—ëŠ” source_fileì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì™„
            source_files = []
            for doc in documents:
                src = doc.metadata.get('source_file') or doc.metadata.get('subject') or 'milvus'
                source_files.append(src)
            used_sources = list(Counter(source_files).keys())
            return {**state, "documents": documents, "used_sources": used_sources}
        except Exception as e:
            print(f"[DEBUG] _retrieve_documents: error {e}")
            return {**state, "error": f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}

    def _prepare_context(self, state: GraphState) -> GraphState:
        documents = state.get("documents", [])
        key_sents = []
        for doc in documents:
            for line in doc.page_content.split("\n"):
                line = line.strip()
                if len(line) > 100 or any(k in line for k in ["ì •ì˜", "íŠ¹ì§•", "ì¢…ë¥˜", "ì˜ˆì‹œ", "ì›ë¦¬", "êµ¬ì„±", "ì ˆì°¨", "ì¥ì ", "ë‹¨ì "]):
                    key_sents.append(line)
        context = "\n".join(key_sents)[:2000]
        # subject_areaë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
        subject_area = state.get("subject_area", "")
        print(f"[DEBUG] _prepare_context: subject_area='{subject_area}'")
        return {**state, "context": context, "subject_area": subject_area}

    def _validate_with_ragas(self, questions: List[Dict[str, Any]], context: str, subject_area: str = "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬") -> float:
        """ê°œì„ ëœ RAGASë¥¼ ì‚¬ìš©í•œ ë¬¸ì œ í’ˆì§ˆ ê²€ì¦"""
        if not RAGAS_AVAILABLE or not RAGAS_ENABLED:
            print("[DEBUG] RAGAS ë¹„í™œì„±í™”ë¨ - í’ˆì§ˆ ê²€ì¦ ê±´ë„ˆëœ€")
            return 1.0  # RAGASê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
        
        try:
            print(f"[DEBUG] RAGAS í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(questions)}ê°œ ë¬¸ì œ")
            
            # RAGAS LLM ë° ì„ë² ë”© ì„¤ì •
            try:
                openai_api_key = os.getenv("OPENAI_API_KEY")
                if not openai_api_key:
                    print("[DEBUG] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - RAGAS ê²€ì¦ ê±´ë„ˆëœ€")
                    return 1.0
                
                # RAGASì—ì„œ ì‚¬ìš©í•  LLM ì„¤ì •: í™˜ê²½ì˜ BASE_URLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©(ê°•ì œ ë®ì–´ì“°ê¸° ê¸ˆì§€)
                os.environ["OPENAI_API_KEY"] = openai_api_key
                base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
                
                ragas_model = os.getenv("RAGAS_LLM_MODEL") or os.getenv("OPENAI_LLM_MODEL", "gpt-4o-mini")
                llm = llm_factory(
                    model=ragas_model,
                    base_url=base_url
                )
                
                # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (HuggingFace ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©)
                from langchain_huggingface import HuggingFaceEmbeddings
                embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"normalize_embeddings": True}
                )
                
                print(f"[DEBUG] RAGAS LLM ì„¤ì • ì™„ë£Œ (OpenAI {os.getenv('OPENAI_LLM_MODEL', 'gpt-4o-mini')})")
                print("[DEBUG] RAGAS ì„ë² ë”© ì„¤ì • ì™„ë£Œ (HuggingFace all-MiniLM-L6-v2)")
                    
            except Exception as llm_error:
                print(f"[DEBUG] RAGAS LLM/ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨: {llm_error}")
                return 1.0
            
            # RAGAS í‰ê°€ ë°ì´í„° êµ¬ì„± (ê°œì„ ëœ ë²„ì „)
            eval_data = {
                "question": [],
                "contexts": [],
                "answer": [],
                "ground_truth": []
            }
            
            for q in questions:
                question_text = q.get("question", "")
                
                # ë‹µë³€ êµ¬ì„±: ì„ íƒì§€ + ì •ë‹µ ë²ˆí˜¸ + í•´ì„¤
                options = q.get("options", [])
                answer_num = q.get("answer", "1")
                explanation = q.get("explanation", "")
                
                # ì„ íƒì§€ë¥¼ í¬í•¨í•œ í’ë¶€í•œ ë‹µë³€ êµ¬ì„±
                options_text = "\n".join([f"{i+1}. {opt}" for i, opt in enumerate(options)])
                try:
                    correct_option = options[int(answer_num)-1] if answer_num.isdigit() and int(answer_num) <= len(options) else options[0]
                except (IndexError, ValueError):
                    correct_option = options[0] if options else "ì •ë‹µ ì—†ìŒ"
                
                # Answer Relevancyë¥¼ ìœ„í•œ ê°„ê²°í•œ ë‹µë³€ êµ¬ì„±
                answer_text = f"{correct_option}. {explanation}"
                
                # ë” ë‚˜ì€ ì»¨í…ìŠ¤íŠ¸: ì—¬ëŸ¬ ê´€ë ¨ ë¬¸ì„œ ì¡°ê° ì‚¬ìš©
                contexts_list = [
                    context[:800],  # ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ê¸¸ê²Œ
                    f"ë¬¸ì œ ìœ í˜•: {subject_area} ê³¼ëª©ì˜ ê°ê´€ì‹ ë¬¸ì œ",
                    f"ì„ íƒì§€: {options_text}"
                ]
                
                eval_data["question"].append(question_text)
                eval_data["contexts"].append(contexts_list)
                eval_data["answer"].append(answer_text)
                eval_data["ground_truth"].append(f"{correct_option}. {explanation}")
            
            # RAGAS í‰ê°€ ì‹¤í–‰
            dataset = Dataset.from_dict(eval_data)
            results = evaluate(
                dataset,
                metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
                llm=llm,
                embeddings=embeddings
            )
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)
            scores = []
            metric_scores = {}
            
            # ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì¶”ì¶œ
            for metric_name in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:
                if hasattr(results, metric_name):
                    metric_values = getattr(results, metric_name)
                    if isinstance(metric_values, list):
                        # NaNê³¼ None ê°’ ì œì™¸
                        valid_values = []
                        for val in metric_values:
                            if val is not None and not (hasattr(val, '__iter__') and len(str(val)) == 0):
                                try:
                                    float_val = float(val)
                                    if not (float_val != float_val):  # NaN ì²´í¬
                                        valid_values.append(float_val)
                                except (ValueError, TypeError):
                                    continue
                        
                        if valid_values:
                            avg_metric = sum(valid_values) / len(valid_values)
                            metric_scores[metric_name] = avg_metric
                            scores.append(avg_metric)
                            print(f"[DEBUG] {metric_name}: {avg_metric:.4f}")
            
            if scores:
                avg_score = sum(scores) / len(scores)
                print(f"[DEBUG] RAGAS ì „ì²´ í‰ê·  ì ìˆ˜: {avg_score:.4f}")
                print(f"[DEBUG] ë©”íŠ¸ë¦­ë³„ ì ìˆ˜: {metric_scores}")
                return avg_score
            else:
                print("[DEBUG] RAGAS ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ë°˜í™˜")
                return 1.0
                
        except Exception as e:
            print(f"[DEBUG] RAGAS ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return 1.0  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼

    def _generate_quiz_incremental(self, state: GraphState) -> GraphState:
        try:
            context = state.get("context", "")
            target_quiz_count = state.get("target_quiz_count", 5)
            validated_questions = state.get("validated_questions", [])
            subject_area = state.get("subject_area", "")
            needed_count = target_quiz_count - len(validated_questions)
            print(f"[DEBUG] _generate_quiz_incremental: context_len={len(context)}, target={target_quiz_count}, validated={len(validated_questions)}, needed={needed_count}")

            if needed_count <= 0:
                return {**state, "quiz_questions": validated_questions}
            if not context.strip():
                new_attempts = state.get("generation_attempts", 0) + 1
                print(f"[DEBUG] _generate_quiz_incremental: no context, attempts={new_attempts}")
                # ì»¨í…ìŠ¤íŠ¸ ì—†ì„ ë•Œ ê³¼ëª© ì¼ë°˜ ê°œë… ê¸°ë°˜ ìƒì„± í´ë°±
                fallback_prompt = (
                    f"ë‹¹ì‹ ì€ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {subject_area} ê³¼ëª©ì˜ ê¸°ë³¸ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ "
                    f"ê°ê´€ì‹ ë¬¸ì œ {generate_count}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n\n"
                    "ì¶œì œ ê·œì¹™:\n"
                    "1) ë³´ê¸°ì—ëŠ” ë²ˆí˜¸(1. 2. 3. 4.)ë¥¼ ì ˆëŒ€ ë¶™ì´ì§€ ë§ê³ , ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "2) ì •ë‹µ(answer)ì€ ë³´ê¸°ì˜ 'ë²ˆí˜¸'(ë¬¸ìì—´)ë¡œë§Œ ì ìœ¼ì„¸ìš”. ì˜ˆ: \"2\"\n"
                    "3) ë¬¸ì œëŠ” ì¤‘ë³µ ì—†ì´ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "4) ë³´ê¸°ëŠ” ìƒí˜¸ ë°°íƒ€ì ì´ë©° ê¸¸ì´ë¥¼ ë„ˆë¬´ ê¸¸ê²Œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”(ê° 3~12ë‹¨ì–´ ê¶Œì¥).\n"
                    "5) í•´ì„¤(explanation)ì€ ì •ë‹µ ê·¼ê±°ë¥¼ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.\n"
                    "6) ì•„ë˜ JSON ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
                    "{\n  \"questions\": [\n    {\n      \"question\": \"ë¬¸ì œ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±\",\n      \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", \"ì„ íƒì§€3\", \"ì„ íƒì§€4\"],\n      \"answer\": \"1\",\n      \"explanation\": \"ì •ë‹µì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ì„¤\"\n    }\n  ]\n}"
                )
                try:
                    self.llm.temperature = 0.15
                    self.llm.max_tokens = 900
                    fb_resp = self.llm.invoke(fallback_prompt)
                    fb_content = getattr(fb_resp, "content", str(fb_resp))
                    new_questions = self._parse_quiz_response(fb_content, subject_area)
                    new_questions = self._filter_duplicate_questions(new_questions, validated_questions, subject_area)
                    if new_questions:
                        return {
                            **state,
                            "quiz_questions": new_questions,
                            "validated_questions": validated_questions,
                            "generation_attempts": new_attempts
                        }
                except Exception:
                    pass
                return {
                    **state, 
                    "quiz_questions": [],
                    "validated_questions": validated_questions,
                    "generation_attempts": new_attempts,
                    "error": "ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
                }

            generate_count = max(min(needed_count, 10), 1)

            # ğŸ”§ JSON í˜•ì‹ì— ì£¼ì„ì´ ë“¤ì–´ê°€ë˜ ë¬¸ì œ ìˆ˜ì •
            prompt_template = PromptTemplate(
                input_variables=["context", "subject_area", "needed_count"],
                template=(
                    "ë‹¹ì‹ ì€ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {subject_area} ê³¼ëª©ì˜ ê°ê´€ì‹ ë¬¸ì œ {needed_count}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n\n"
                    "ì¡°ê±´:\n"
                    "â€¢ ë³´ê¸°ì—ëŠ” ë²ˆí˜¸(1. 2. 3. 4.)ë¥¼ ë¶™ì´ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                    "â€¢ answerì—ëŠ” ì •ë‹µì˜ 'ë²ˆí˜¸'ë§Œ ë¬¸ìì—´ë¡œ ì ìœ¼ì‹­ì‹œì˜¤. ì˜ˆ: \"2\"\n"
                    "â€¢ ì¶œë ¥ì€ ì•„ë˜ JSON í˜•ì‹ë§Œ í¬í•¨í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.\n\n"
                    "[ë¬¸ì„œ ë‚´ìš©]\n{context}\n\n"
                    "[ì‘ë‹µ í˜•ì‹]\n"
                    "{{\n"
                    "  \"questions\": [\n"
                    "    {{\n"
                    "      \"question\": \"ë¬¸ì œ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±\",\n"
                    "      \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", \"ì„ íƒì§€3\", \"ì„ íƒì§€4\"],\n"
                    "      \"answer\": \"1\",\n"
                    "      \"explanation\": \"ì •ë‹µì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ì„¤\"\n"
                    "    }}\n"
                    "  ]\n"
                    "}}\n"
                )
            )

            prompt = prompt_template.format(
                context=context, subject_area=subject_area, needed_count=generate_count
            )

            print(f"[DEBUG] _generate_quiz_incremental: calling LLM for {generate_count} questions")
            self.llm.temperature = 0.15
            self.llm.max_tokens = 900
            response = self.llm.invoke(prompt)
            response_content = getattr(response, "content", str(response))
            print(f"[DEBUG] _generate_quiz_incremental: LLM response length={len(response_content)}")
            
            new_questions = self._parse_quiz_response(response_content, subject_area)
            # ì¤‘ë³µ í•„í„°ë§(ë™ì¼ í„´/ì´ì „/ë²¡í„°ìŠ¤í† ì–´ ìœ ì‚¬ ì œê±°)
            new_questions = self._filter_duplicate_questions(new_questions, validated_questions, subject_area)
            print(f"[DEBUG] _generate_quiz_incremental: parsed {len(new_questions)} questions")

            if not new_questions:
                new_attempts = state.get("generation_attempts", 0) + 1
                print(f"[DEBUG] _generate_quiz_incremental: failed to generate questions, attempts={new_attempts}")
                return {
                    **state,
                    "quiz_questions": [],
                    "validated_questions": validated_questions,
                    "generation_attempts": new_attempts,
                    "error": "ìœ íš¨í•œ ë¬¸ì œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                }

            # ğŸ” RAGAS í’ˆì§ˆ ê²€ì¦ ì¶”ê°€
            if RAGAS_ENABLED and new_questions:
                print(f"[DEBUG] _generate_quiz_incremental: RAGAS í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
                subject_area = state.get("subject_area", "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬")
                quality_score = self._validate_with_ragas(new_questions, context, subject_area)
                
                # í’ˆì§ˆ ì„ê³„ê°’ ì²´í¬
                if quality_score >= RAGAS_QUALITY_THRESHOLD:
                    print(f"[DEBUG] _generate_quiz_incremental: RAGAS í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ ({quality_score:.4f} >= {RAGAS_QUALITY_THRESHOLD})")
                    new_attempts = state.get("generation_attempts", 0) + 1
                    return {
                        **state,
                        "quiz_questions": new_questions,
                        "validated_questions": validated_questions,
                        "generation_attempts": new_attempts,
                        "ragas_score": quality_score
                    }
                else:
                    print(f"[DEBUG] _generate_quiz_incremental: RAGAS í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({quality_score:.4f} < {RAGAS_QUALITY_THRESHOLD})")
                    # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì¬ìƒì„± ì‹œë„
                    current_attempts = state.get("generation_attempts", 0) + 1
                    if current_attempts < RAGAS_MAX_ATTEMPTS:
                        print(f"[DEBUG] _generate_quiz_incremental: ì¬ìƒì„± ì‹œë„ ({current_attempts}/{RAGAS_MAX_ATTEMPTS})")
                        return {
                            **state,
                            "generation_attempts": current_attempts,
                            "error": f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({quality_score:.4f}), ì¬ìƒì„± ì‹œë„ ì¤‘..."
                        }
                    else:
                        print(f"[DEBUG] _generate_quiz_incremental: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return {
                            **state,
                            "quiz_questions": new_questions,
                            "validated_questions": validated_questions,
                            "generation_attempts": current_attempts,
                            "error": f"ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ìµœì¢… ì ìˆ˜: {quality_score:.4f})",
                            "ragas_score": quality_score
                        }
            else:
                # RAGAS ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ì¡´ ë¡œì§
                new_attempts = state.get("generation_attempts", 0) + 1
                print(f"[DEBUG] _generate_quiz_incremental: generated {len(new_questions)} questions, attempts={new_attempts}")
                return {
                    **state,
                    "quiz_questions": new_questions,
                    "validated_questions": validated_questions,
                    "generation_attempts": new_attempts
                }
        except Exception as e:
            new_attempts = state.get("generation_attempts", 0) + 1
            print(f"[DEBUG] _generate_quiz_incremental: exception {e}, attempts={new_attempts}")
            return {
                **state, 
                "quiz_questions": [],
                "validated_questions": state.get("validated_questions", []),
                "generation_attempts": new_attempts,
                "error": f"ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            }

    def _validate_quiz_incremental(self, state: GraphState) -> GraphState:
        subject_area = state.get("subject_area", "")
        previously_validated = state.get("validated_questions", [])
        new_questions = state.get("quiz_questions", [])
        context = state.get("context", "")
        target_quiz_count = state.get("target_quiz_count", 5)
        generation_attempts = state.get("generation_attempts", 0)
        error = state.get("error", "")

        print(f"[DEBUG] _validate_quiz_incremental: subject={subject_area}, new_questions={len(new_questions)}, previously_validated={len(previously_validated)}, error={error}")

        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ê²€ì¦í•˜ì§€ ì•Šê³  ì—ëŸ¬ ìƒíƒœ ìœ ì§€
        if error:
            print(f"[DEBUG] _validate_quiz_incremental: skipping validation due to error: {error}")
            return state

        if not new_questions:
            print(f"[DEBUG] _validate_quiz_incremental: no new questions to validate")
            return {**state, "validated_questions": previously_validated}

        newly_validated = []
        print(f"[DEBUG] _validate_quiz_incremental: validating {len(new_questions)} questions")

        # ê°„ë‹¨í•œ ê²€ì¦: ëª¨ë“  ë¬¸ì œë¥¼ ìœ íš¨í•˜ë‹¤ê³  ê°€ì • (LLM í˜¸ì¶œ ì—†ì´)
        # ì‹¤ì œë¡œëŠ” LLM ê²€ì¦ì„ í•  ìˆ˜ ìˆì§€ë§Œ, í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
        for q in new_questions:
            if len(newly_validated) >= target_quiz_count - len(previously_validated):
                break
            # ê¸°ë³¸ ê²€ì¦: í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            if q.get("question") and q.get("options") and q.get("answer") and q.get("explanation"):
                newly_validated.append(q)
                print(f"[DEBUG] _validate_quiz_incremental: validated question: {q.get('question', '')[:50]}...")

        all_validated = previously_validated + newly_validated
        print(f"[DEBUG] _validate_quiz_incremental: total validated: {len(all_validated)}/{target_quiz_count}")

        return {
            **state,
            "validated_questions": all_validated,
            "quiz_questions": all_validated,
            "error": ""  # ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”
        }

    def _check_completion(self, state: GraphState) -> str:
        validated_count = len(state.get("validated_questions", []))
        target_count = state.get("target_quiz_count", 5)
        generation_attempts = state.get("generation_attempts", 0)
        error = state.get("error", "")
        
        print(f"[DEBUG] _check_completion: validated={validated_count}, target={target_count}, attempts={generation_attempts}, error={error}")
        
        # ëª©í‘œ ë‹¬ì„±
        if validated_count >= target_count:
            print(f"[DEBUG] Target reached ({validated_count}/{target_count}), completing")
            return "complete"
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬
        if generation_attempts >= 5:  # 5íšŒë¡œ ì¦ê°€
            print(f"[DEBUG] Max attempts reached ({generation_attempts}), completing")
            return "complete"
        
        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
        if error:
            print(f"[DEBUG] Error detected: {error}, completing")
            return "complete"
        
        # ê³„ì† ìƒì„±
        print(f"[DEBUG] Need more questions ({validated_count}/{target_count}), continuing generation (attempt {generation_attempts})")
        return "generate_more"

    def _parse_quiz_response(self, response: str, subject_area: str = "") -> List[Dict[str, Any]]:
        try:
            print(f"[DEBUG] _parse_quiz_response: raw response length={len(response)}")
            print(f"[DEBUG] _parse_quiz_response: response preview='{response[:200]}...'")
            
            # 1. JSON ë¸”ë¡ ì°¾ê¸° (```json ... ```)
            json_block_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_block_match:
                json_str = json_block_match.group(1)
                print(f"[DEBUG] _parse_quiz_response: found JSON block, length={len(json_str)}")
            else:
                # 2. ì¼ë°˜ JSON ê°ì²´ ì°¾ê¸°
                json_str_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response.strip(), re.DOTALL)
                if not json_str_match:
                    print(f"[DEBUG] _parse_quiz_response: no JSON found in response")
                    return []
                json_str = json_str_match.group(0)
                print(f"[DEBUG] _parse_quiz_response: found JSON object, length={len(json_str)}")

            # 3. JSON ë¬¸ìì—´ ì •ë¦¬ (ê³¼ë„í•œ ë°±ìŠ¬ë˜ì‹œ ì œê±°ëŠ” JSONì„ ê¹¨ëœ¨ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œí™”)
            json_str = json_str.replace('\\u312f', '').replace('\\n', ' ')
            print(f"[DEBUG] _parse_quiz_response: cleaned JSON='{json_str[:200]}...'")
            
            # 4. JSON íŒŒì‹±
            data = json.loads(json_str)
            if "questions" not in data or not isinstance(data["questions"], list):
                print(f"[DEBUG] _parse_quiz_response: invalid data structure, keys={list(data.keys()) if isinstance(data, dict) else 'not dict'}")
                return []

            questions = data.get("questions", [])
            print(f"[DEBUG] _parse_quiz_response: found {len(questions)} questions")
            
            # 5. ê° ë¬¸ì œ ì²˜ë¦¬ ë° ì •ê·œí™”(ë³´ê¸°ì— ë²ˆí˜¸ ì œê±°, ê³¼ëª© ì£¼ì…, ë³´ê¸° ì •ë¦¬, ì •ë‹µ ë³´ì •)
            for i, question in enumerate(questions):
                if "options" in question and isinstance(question["options"], list):
                    numbered_options = []
                    for j, option_text in enumerate(question["options"], 1):
                        cleaned_text = re.sub(r'^\s*\d+\.\s*', '', str(option_text)).strip()
                        numbered_options.append(f"  {j}. {cleaned_text}")
                    question["options"] = numbered_options
                if "subject" not in question:
                    question["subject"] = subject_area
                # ë³´ê¸° ê¸¸ì´/ì¤‘ë³µ í•„í„°ë§ ë° 4ê°œ ì œí•œ
                dedup_opts, seen = [], set()
                for opt in question["options"]:
                    base = re.sub(r'^\s*\d+\.\s*', '', opt).strip()
                    if not base or base.lower() in seen:
                        continue
                    seen.add(base.lower())
                    dedup_opts.append(opt)
                if len(dedup_opts) >= 4:
                    question["options"] = dedup_opts[:4]
                # ì •ë‹µ ì¸ë±ìŠ¤ ìœ íš¨ì„± ë³´ì •
                ans = str(question.get("answer", "")).strip()
                if ans not in {"1","2","3","4"}:
                    question["answer"] = "1"
                print(f"[DEBUG] _parse_quiz_response: processed question {i+1}: {question.get('question', '')[:50]}...")
            
            return questions
        except Exception as e:
            print(f"[DEBUG] _parse_quiz_response: exception during parsing: {e}")
            print(f"[DEBUG] _parse_quiz_response: response that caused error: '{response[:500]}...'")
            return []

    # ---------- ì¤‘ë³µ íƒì§€/ì œê±° ìœ í‹¸ ----------
    def _norm_text(self, text: str) -> str:
        try:
            s = re.sub(r"\s+", " ", str(text or "")).strip().lower()
            s = re.sub(r"^[0-9]+\.\s*", "", s)
            return s
        except Exception:
            return str(text or "").strip().lower()

    def _jaccard_sim(self, a: str, b: str) -> float:
        ta = set(self._norm_text(a).split())
        tb = set(self._norm_text(b).split())
        if not ta or not tb:
            return 0.0
        inter = len(ta & tb)
        union = len(ta | tb)
        return inter / max(union, 1)

    def _filter_duplicate_questions(self, new_qs: List[Dict[str, Any]], prev_validated: List[Dict[str, Any]], subject_area: str) -> List[Dict[str, Any]]:
        if not new_qs:
            return []
        kept: List[Dict[str, Any]] = []
        seen_norm: set = set()
        prev_texts = [q.get("question", "") for q in (prev_validated or [])]
        for q in new_qs:
            qtext = q.get("question", "")
            norm = self._norm_text(qtext)
            if not norm:
                continue
            if norm in seen_norm:
                continue
            dup_local = any(self._jaccard_sim(qtext, p) >= 0.9 for p in prev_texts)
            if dup_local:
                continue
            is_dup_vs = False
            try:
                if self.vectorstore and hasattr(self.vectorstore, "similarity_search"):
                    expr = f"subject == '{subject_area}'" if subject_area else None
                    docs = self.vectorstore.similarity_search(qtext, k=3, expr=expr) if expr else self.vectorstore.similarity_search(qtext, k=3)
                    for d in docs or []:
                        if self._jaccard_sim(qtext, d.page_content) >= 0.9:
                            is_dup_vs = True
                            break
            except Exception:
                try:
                    docs = self.retriever.invoke(qtext) if self.retriever else []
                    for d in docs or []:
                        if self._jaccard_sim(qtext, getattr(d, "page_content", "")) >= 0.9:
                            is_dup_vs = True
                            break
                except Exception:
                    pass
            if is_dup_vs:
                continue
            kept.append(q)
            seen_norm.add(norm)
        return kept

    # ---------- í•µì‹¬: ê·¸ë˜í”„ êµ¬ì„± ë³€ê²½ (ê³¼ëª©ë³„ 2ë…¸ë“œ Ã— 5ê³¼ëª© = 10ë…¸ë“œ) ----------
    def _build_graph(self):
        """
        ê³µí†µ ì‚¬ì „ ë‹¨ê³„: retrieve -> prepare_context
        ì´í›„ ê³¼ëª©ë³„ ë¼ìš°íŒ…: (subject)generate -> (subject)validate -> ì¡°ê±´ë¶€ ë£¨í”„
        """
        workflow = StateGraph(GraphState)

        # ê³µí†µ ì „ì²˜ë¦¬
        workflow.add_node("retrieve", self._retrieve_documents)
        workflow.add_node("prepare_context", self._prepare_context)

        # ê³¼ëª©ë³„ ë…¸ë“œ í•¨ìˆ˜: subjectë¥¼ í´ë¡œì €ë¡œ ë¬¶ì–´ 2ê°œ ë…¸ë“œ ìƒì„±
        def make_generate_node(subject_name):
            def _gen(state: GraphState) -> GraphState:
                # subject_nameì„ stateì— ë³´ì¦
                print(f"[DEBUG] {subject_name}_generate ë…¸ë“œ ì‹¤í–‰")
                state = {**state, "subject_area": subject_name}
                return self._generate_quiz_incremental(state)
            return _gen

        def make_validate_node(subject_name):
            def _val(state: GraphState) -> GraphState:
                state = {**state, "subject_area": subject_name}
                return self._validate_quiz_incremental(state)
            return _val

        # ê³¼ëª©ë³„ ë…¸ë“œ ì¶”ê°€
        subject_to_nodes = {}
        for subj in self.SUBJECT_AREAS.keys():
            gen_name = f"{subj}_generate"
            val_name = f"{subj}_validate"
            workflow.add_node(gen_name, make_generate_node(subj))
            workflow.add_node(val_name, make_validate_node(subj))
            # ê³¼ëª©ë³„ ë‚´ë¶€ ì—£ì§€
            workflow.add_edge(gen_name, val_name)
            workflow.add_conditional_edges(
                val_name,
                self._check_completion,
                {"generate_more": gen_name, "complete": END}
            )
            subject_to_nodes[subj] = (gen_name, val_name)

        # ë¼ìš°í„°: prepare_context ì´í›„ ê³¼ëª©ë³„ generateë¡œ ë¶„ê¸°
        def _route_to_subject(state: GraphState) -> str:
            subj = state.get("subject_area", "")
            print(f"[DEBUG] _route_to_subject: subject_area='{subj}', available_subjects={list(subject_to_nodes.keys())}")
            if subj in subject_to_nodes:
                gen_name, val_name = subject_to_nodes[subj]  # íŠœí”Œ ì–¸íŒ¨í‚¹
                print(f"[DEBUG] Found subject '{subj}', returning generate node: {gen_name}")
                return gen_name  # generate ë…¸ë“œëª…ë§Œ ë°˜í™˜
            # ê¸°ë³¸ê°’(ì•ˆ ë§ìœ¼ë©´ ì„¤ê³„ë¡œ)
            print(f"[DEBUG] Subject '{subj}' not found, using default: ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„")
            gen_name, val_name = subject_to_nodes["ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„"]
            return gen_name

        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "prepare_context")
        
        # ìˆ˜ì •: _route_to_subject í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ê°’ê³¼ ë…¸ë“œëª…ì„ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
        # _route_to_subjectëŠ” ë…¸ë“œëª…ì„ ë°˜í™˜í•˜ë¯€ë¡œ, routing_dictëŠ” {ë…¸ë“œëª…: ë…¸ë“œëª…} í˜•íƒœì—¬ì•¼ í•¨
        routing_dict = {subject_to_nodes[subj][0]: subject_to_nodes[subj][0] for subj in subject_to_nodes.keys()}
        print(f"[DEBUG] routing_dict: {routing_dict}")
        print(f"[DEBUG] Available nodes: {list(workflow.nodes.keys())}")
        print(f"[DEBUG] routing_dict keys: {list(routing_dict.keys())}")
        print(f"[DEBUG] routing_dict keys in nodes: {[k in workflow.nodes for k in routing_dict.keys()]}")
        workflow.add_conditional_edges("prepare_context", _route_to_subject, routing_dict)

        self.workflow = workflow.compile()
    # --------------------------------------------------------------------

    # ë‹¨ì¼ ê³¼ëª© ìƒì„±(ë‚´ë¶€ëŠ” ê·¸ë˜í”„ í•œ ë²ˆ ì‹¤í–‰)
    def _generate_subject_quiz(self, subject_area: str, target_count: int = 5, difficulty: str = "ì¤‘ê¸‰") -> Dict[str, Any]:
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (retrieverê°€ Noneì¸ ê²½ìš°)
        if self.retriever is None:
            print("[DEBUG] Retrieverê°€ Noneì…ë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            try:
                self._build_retriever_from_milvus({})
                print("[DEBUG] ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"[DEBUG] ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë¬¸ì œ ìƒì„±)
        
        if subject_area not in self.SUBJECT_AREAS:
            return {"error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©: {subject_area}"}
        keywords = self.SUBJECT_AREAS[subject_area]["keywords"]

        all_validated_questions = []
        max_rounds = 10
        current_round = 0

        while len(all_validated_questions) < target_count and current_round < max_rounds:
            current_round += 1
            remaining_needed = target_count - len(all_validated_questions)

            # RAGAS ê¸°ë°˜ ê°œì„ : í‚¤ì›Œë“œ ì¡°í•© ëŒ€ì‹  ì˜ë¯¸ìˆëŠ” ì¿¼ë¦¬ ìƒì„±
            for i in range(0, len(keywords), 3):  # 3ê°œì”© ê·¸ë£¹í™”
                if len(all_validated_questions) >= target_count:
                    break
                
                # í‚¤ì›Œë“œ ì¡°í•©ì„ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬ë¡œ ë³€í™˜
                keyword_group = keywords[i:i+3]
                if len(keyword_group) >= 2:
                    # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬ ìƒì„±
                    primary_concept = keyword_group[0]
                    secondary_concept = keyword_group[1] if len(keyword_group) > 1 else ""
                    
                    if secondary_concept:
                        query = f"{primary_concept}ì™€ {secondary_concept}ì˜ ê°œë…ê³¼ í™œìš©"
                    else:
                        query = f"{primary_concept}ì˜ ê°œë…ê³¼ íŠ¹ì§•"
                else:
                    query = " ".join(keyword_group)

                initial_state = {
                    "query": query,
                    "target_quiz_count": min(remaining_needed, 3),  # í•œ ë²ˆì— 3ë¬¸ì œì”© ìƒì„±
                    "difficulty": difficulty,
                    "generation_attempts": 0,
                    "quiz_questions": [],
                    "validated_questions": [],
                    "subject_area": subject_area,
                    "keywords": keyword_group  # RAGAS ê²€ì¦ì—ì„œ ì‚¬ìš©í•  í‚¤ì›Œë“œ ì •ë³´
                }
                # ê³¼ëª©ë³„ ë¼ìš°íŒ… ê·¸ë˜í”„ ë‹¨ë°œ ì‹¤í–‰
                result = self.workflow.invoke(initial_state)

                if result.get("error"):
                    continue

                new_qs = result.get("validated_questions", [])
                if new_qs:
                    # ì¤‘ë³µ ì œê±°
                    exists = {q.get("question", "") for q in all_validated_questions}
                    for q in new_qs:
                        if q.get("question", "") not in exists:
                            all_validated_questions.append(q)
                            exists.add(q.get("question", ""))

                if len(all_validated_questions) >= target_count:
                    break

            if current_round < max_rounds and len(all_validated_questions) < target_count:
                time.sleep(1.5)

        final_questions = all_validated_questions[:target_count]
        return {
            "subject_area": subject_area,
            "difficulty": difficulty,
            "requested_count": target_count,
            "quiz_count": len(final_questions),
            "questions": final_questions,
            "status": "SUCCESS" if len(final_questions) >= target_count else "PARTIAL"
        }

    # 3) ì‚¬ìš©ì ì§€ì • ë³‘ë ¬ ì‹¤í–‰ë¡œ 5ê³¼ëª© ë™ì‹œ ì²˜ë¦¬(ìµœëŒ€ parallel_agents ë™ì‹œ)
    def _generate_full_exam(self, difficulty: str = "ì¤‘ê¸‰", parallel_agents: int = 2) -> Dict[str, Any]:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        start_time = time.time()

        requested_per_subject = {s: info["count"] for s, info in self.SUBJECT_AREAS.items()}

        full_exam_result = {
            "exam_title": "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ëª¨ì˜ê³ ì‚¬",
            "total_questions": 0,
            "difficulty": difficulty,
            "subjects": {},
            "all_questions": [],
            "generation_summary": {},
            "failed_subjects": [],
            "model_info": "Groq llama-4-scout-17b-16e-instruct",
            "parallel_agents": parallel_agents
        }

        # ë³‘ë ¬ë¡œ ê³¼ëª© ìƒì„± ì‹¤í–‰
        futures = {}
        with ThreadPoolExecutor(max_workers=parallel_agents) as ex:
            for subject_area, target in requested_per_subject.items():
                futures[ex.submit(
                    self._generate_subject_quiz,
                    subject_area=subject_area,
                    target_count=target,
                    difficulty=difficulty
                )] = subject_area

            per_subject_results = {}
            for fut in as_completed(futures):
                subject_area = futures[fut]
                try:
                    per_subject_results[subject_area] = fut.result()
                except Exception as e:
                    per_subject_results[subject_area] = {"error": str(e)}

        # 4) ë¨¸ì§€ ìˆœì„œì— ë”°ë¼ ì·¨í•©
        total_generated = 0
        merged_questions = []
        for subject_area in self.MERGE_ORDER:
            res = per_subject_results.get(subject_area, {"error": "ê²°ê³¼ ì—†ìŒ"})
            if "error" in res:
                full_exam_result["failed_subjects"].append({
                    "subject": subject_area,
                    "error": res["error"]
                })
                full_exam_result["subjects"][subject_area] = {
                    "requested_count": requested_per_subject[subject_area],
                    "actual_count": 0,
                    "questions": [],
                    "status": "FAILED"
                }
            else:
                qs = res.get("questions", [])
                total_generated += len(qs)
                merged_questions.extend(qs)
                full_exam_result["subjects"][subject_area] = {
                    "requested_count": requested_per_subject[subject_area],
                    "actual_count": len(qs),
                    "questions": qs,
                    "status": res.get("status", "UNKNOWN")
                }

        elapsed_time = time.time() - start_time
        full_exam_result["total_questions"] = total_generated
        full_exam_result["all_questions"] = merged_questions
        full_exam_result["generation_summary"] = {
            "target_total": sum(requested_per_subject.values()),  # 100
            "actual_total": total_generated,
            "success_rate": f"{(total_generated / max(1, sum(requested_per_subject.values())))*100:.1f}%",
            "successful_subjects": 5 - len(full_exam_result["failed_subjects"]),
            "failed_subjects": len(full_exam_result["failed_subjects"]),
            "completion_status": "COMPLETE" if total_generated >= sum(requested_per_subject.values()) else "PARTIAL",
            "generation_time": f"{elapsed_time:.1f}ì´ˆ"
        }
        return full_exam_result

    def _generate_partial_exam(self, selected_subjects: List[str], questions_per_subject: int = 10, 
                              difficulty: str = "ì¤‘ê¸‰", parallel_agents: int = 2) -> Dict[str, Any]:
        """ì„ íƒëœ ê³¼ëª©ë“¤ì— ëŒ€í•´ ì§€ì •ëœ ë¬¸ì œ ìˆ˜ë§Œí¼ ìƒì„±"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        start_time = time.time()

        # self._generate_workflow_diagram("partial_exam", {
        #     "selected_subjects": selected_subjects,
        #     "questions_per_subject": questions_per_subject,
        #     "difficulty": difficulty,
        #     "parallel_agents": parallel_agents
        # })

        partial_exam_result = {
            "exam_title": f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì„ íƒê³¼ëª© ëª¨ì˜ê³ ì‚¬ ({len(selected_subjects)}ê³¼ëª©)",
            "total_questions": 0,
            "difficulty": difficulty,
            "selected_subjects": selected_subjects,
            "questions_per_subject": questions_per_subject,
            "subjects": {},
            "all_questions": [],
            "generation_summary": {},
            "failed_subjects": [],
            "model_info": "Groq llama-4-scout-17b-16e-instruct",
            "parallel_agents": parallel_agents
        }

        # ë³‘ë ¬ë¡œ ì„ íƒëœ ê³¼ëª© ìƒì„± ì‹¤í–‰
        futures = {}
        with ThreadPoolExecutor(max_workers=parallel_agents) as ex:
            for subject_area in selected_subjects:
                futures[ex.submit(
                    self._generate_subject_quiz,
                    subject_area=subject_area,
                    target_count=questions_per_subject,
                    difficulty=difficulty
                )] = subject_area

            per_subject_results = {}
            for fut in as_completed(futures):
                subject_area = futures[fut]
                try:
                    per_subject_results[subject_area] = fut.result()
                except Exception as e:
                    per_subject_results[subject_area] = {"error": str(e)}

        # ê²°ê³¼ ì·¨í•©
        total_generated = 0
        merged_questions = []
        for subject_area in selected_subjects:
            res = per_subject_results.get(subject_area, {"error": "ê²°ê³¼ ì—†ìŒ"})
            if "error" in res:
                partial_exam_result["failed_subjects"].append({
                    "subject": subject_area,
                    "error": res["error"]
                })
                partial_exam_result["subjects"][subject_area] = {
                    "requested_count": questions_per_subject,
                    "actual_count": 0,
                    "questions": [],
                    "status": "FAILED"
                }
            else:
                qs = res.get("questions", [])
                total_generated += len(qs)
                merged_questions.extend(qs)
                partial_exam_result["subjects"][subject_area] = {
                    "requested_count": questions_per_subject,
                    "actual_count": len(qs),
                    "questions": qs,
                    "status": res.get("status", "UNKNOWN")
                }

        elapsed_time = time.time() - start_time
        partial_exam_result["total_questions"] = total_generated
        partial_exam_result["all_questions"] = merged_questions
        partial_exam_result["generation_summary"] = {
            "target_total": len(selected_subjects) * questions_per_subject,
            "actual_total": total_generated,
            "success_rate": f"{(total_generated / max(1, len(selected_subjects) * questions_per_subject))*100:.1f}%",
            "successful_subjects": len(selected_subjects) - len(partial_exam_result["failed_subjects"]),
            "failed_subjects": len(partial_exam_result["failed_subjects"]),
            "completion_status": "COMPLETE" if total_generated >= len(selected_subjects) * questions_per_subject else "PARTIAL",
            "generation_time": f"{elapsed_time:.1f}ì´ˆ"
        }
        return partial_exam_result

    # íŒŒì¼ ì €ì¥ í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼(ì¤‘ë³µ ì •ì˜ëŠ” ë§ˆì§€ë§‰ ì •ì˜ê°€ ìœ íš¨)
    def _save_to_json(self, exam_result: Dict[str, Any], filename: str = None) -> str:
        save_dir = "C:\\ET_Agent\\teacher\\TestGenerator\\test"
        os.makedirs(save_dir, exist_ok=True)

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if "exam_title" in exam_result:
                filename = f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬_ëª¨ì˜ê³ ì‚¬_100ë¬¸ì œ_{timestamp}.json"
            else:
                subject = exam_result.get("subject_area", "ë¬¸ì œ")
                count = exam_result.get("quiz_count", 0)
                filename = f"{subject}_{count}ë¬¸ì œ_{timestamp}.json"

        if not os.path.isabs(filename):
            filename = os.path.join(save_dir, filename)
        elif not filename.startswith(save_dir):
            filename = os.path.join(save_dir, os.path.basename(filename))

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(exam_result, f, ensure_ascii=False, indent=2)
        return filename

    def _save_to_json(self, exam_result: Dict[str, Any], filename: str = None) -> str:
        """ì‹œí—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬_ë¬¸ì œìƒì„±_{timestamp}.json"
        
        filepath = os.path.join(os.path.dirname(__file__), "test", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(exam_result, f, ensure_ascii=False, indent=2)
        return filename

    def _generate_workflow_diagram(self, mode: str, params: Dict[str, Any]) -> None:
        """Graphvizë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œ ìƒì„± ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            from graphviz import Digraph
            import time
            
            # ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
            dot = Digraph(comment=f'ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ë¬¸ì œ ìƒì„± ì›Œí¬í”Œë¡œìš° - {mode}')
            dot.attr(rankdir='TB', size='12,8')
            
            # ë…¸ë“œ ìŠ¤íƒ€ì¼ ì •ì˜
            dot.attr('node', shape='box', style='rounded,filled', fontname='Arial', fontsize='10')
            
            # ì‹œì‘ ë…¸ë“œ
            dot.node('start', 'ì‚¬ìš©ì ì…ë ¥', fillcolor='lightgreen')
            
            # ì…ë ¥ íŒŒì‹± ë…¸ë“œë“¤
            dot.node('parse', 'LLM ê¸°ë°˜\nì…ë ¥ íŒŒì‹±', fillcolor='lightblue')
            dot.node('validate', 'íŒŒë¼ë¯¸í„° ê²€ì¦\n(ê³¼ëª©/ë¬¸ì œìˆ˜/ë‚œì´ë„)', fillcolor='lightyellow')
            
            # ëª¨ë“œë³„ ë¶„ê¸°
            if mode == "partial_exam":
                dot.node('mode', 'PARTIAL_EXAM\n(ì„ íƒê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('parallel', 'ë³‘ë ¬ ì²˜ë¦¬\n(ThreadPoolExecutor)', fillcolor='lightcoral')
                dot.node('merge', 'ê²°ê³¼ í†µí•©\n(ê³¼ëª©ë³„ ê²°ê³¼ ë³‘í•©)', fillcolor='lightcoral')
            elif mode == "single_subject":
                dot.node('mode', 'SINGLE_SUBJECT\n(ë‹¨ì¼ ê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('single', 'ë‹¨ì¼ ì—ì´ì „íŠ¸\n(ì§ë ¬ ì²˜ë¦¬)', fillcolor='lightcoral')
            else:
                dot.node('mode', 'FULL_EXAM\n(ì „ì²´ ê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('full_parallel', 'ì „ì²´ ë³‘ë ¬\n(5ê³¼ëª© ë™ì‹œ)', fillcolor='lightcoral')
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            dot.node('agent', 'TestGenerator\n.execute()', fillcolor='lightpink')
            dot.node('result', 'ê²°ê³¼ ì²˜ë¦¬\n(ëª¨ë“œë³„ ê²°ê³¼ ì¶”ì¶œ)', fillcolor='lightcyan')
            
            # ë°ì´í„° ë³€í™˜
            dot.node('transform', 'ë°ì´í„° ë³€í™˜\n(QA í˜•ì‹)', fillcolor='lightcyan')
            dot.node('output', 'ì¶œë ¥\n(JSON/PDF)', fillcolor='lightgreen')
            
            # ì—£ì§€ ì—°ê²°
            dot.edge('start', 'parse')
            dot.edge('parse', 'validate')
            dot.edge('validate', 'mode')
            
            if mode == "partial_exam":
                dot.edge('mode', 'parallel')
                dot.edge('parallel', 'agent')
                dot.edge('agent', 'merge')
                dot.edge('merge', 'result')
            elif mode == "single_subject":
                dot.edge('mode', 'single')
                dot.edge('single', 'agent')
                dot.edge('agent', 'result')
            else:
                dot.edge('mode', 'full_parallel')
                dot.edge('full_parallel', 'agent')
                dot.edge('agent', 'result')
            
            dot.edge('result', 'transform')
            dot.edge('transform', 'output')
            
            # ì„œë¸Œê·¸ë˜í”„ë¡œ ê³¼ëª©ë³„ ì²˜ë¦¬ êµ¬ì¡° í‘œì‹œ
            if mode == "partial_exam":
                with dot.subgraph(name='cluster_subjects') as c:
                    c.attr(label='ê³¼ëª©ë³„ ë³‘ë ¬ ì²˜ë¦¬', style='filled', fillcolor='lightgray')
                    subjects = params.get("selected_subjects", [])
                    count_per_subject = params.get("questions_per_subject", 10)
                    for i, subject in enumerate(subjects):
                        c.node(f'subject_{i}', f'{subject}\n({count_per_subject}ë¬¸ì œ)')
                        if i > 0:
                            c.edge(f'subject_{i-1}', f'subject_{i}', style='dashed')
            
            # íŒŒì¼ ì €ì¥
            # output_dir = os.path.join(os.path.dirname(__file__), "workflow_diagrams")
            # os.makedirs(output_dir, exist_ok=True)
            
            # timestamp = time.strftime("%Y%m%d_%H%M%S")
            # filename = f"generation_workflow_{mode}_{timestamp}"
            # filepath = os.path.join(output_dir, filename)
            
            # dot.render(filepath, format='png', cleanup=True)
            # print(f"\n ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì™„ë£Œ: {filepath}.png")
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ë„ ì¶œë ¥
            print(f"\nğŸ“Š ì›Œí¬í”Œë¡œìš° ìš”ì•½:")
            print(f"   â”Œâ”€ ëª¨ë“œ: {mode.upper()}")
            if mode == "partial_exam":
                subjects = params.get("selected_subjects", [])
                count_per_subject = params.get("questions_per_subject", 10)
                print(f"   â”œâ”€ ì„ íƒëœ ê³¼ëª©: {', '.join(subjects)}")
                print(f"   â”œâ”€ ê³¼ëª©ë‹¹ ë¬¸ì œ ìˆ˜: {count_per_subject}ê°œ")
                print(f"   â””â”€ ë³‘ë ¬ ì—ì´ì „íŠ¸: {params.get('parallel_agents', 2)}ê°œ")
            
        except ImportError:
            print("\nâš ï¸  Graphvizê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install graphvizë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"\nâŒ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}")
