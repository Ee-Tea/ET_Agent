import os
import glob
from typing import List, Dict, Any, TypedDict
from abc import ABC, abstractmethod
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langgraph.graph import StateGraph, END
import json
import re
from collections import Counter
import time
from datetime import datetime
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•œ ì„í¬íŠ¸
from dotenv import load_dotenv
import sys
import os

# ìƒëŒ€ ì„í¬íŠ¸ ëŒ€ì‹  ì ˆëŒ€ ê²½ë¡œë¡œ ì„í¬íŠ¸
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from base_agent import BaseAgent

# OpenAI ê´€ë ¨ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.0"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))
LLM_TIMEOUT = int(os.getenv("LLM_TIMEOUT", "120"))
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "3"))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class GraphState(TypedDict):
    """ê·¸ë˜í”„ ìƒíƒœ ì •ì˜"""
    query: str
    documents: List[Document]
    context: str
    quiz_questions: List[Dict[str, Any]]
    difficulty: str
    error: str
    used_sources: List[str]
    generation_attempts: int
    target_quiz_count: int
    subject_area: str
    validated_questions: List[Dict[str, Any]]  # ë¬¸ì œì— ë‹µ í•´ì„¤ê¹Œì§€ í•œ ë²ˆì— ë‚˜ì˜´, ë³´ê¸°ëŠ” 1. 2. 3. 4. ìœ¼ë¡œ ë²ˆí˜¸ê°€ ë¶™ìŒ, ë¬¸ì œì—ëŠ” ë²ˆí˜¸ ì•ˆ ë¶™ìŒ


class InfoProcessingExamAgent(BaseAgent):
    """
    ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œê¸°ì¤€ì— ë§ëŠ” ìë™ ì¶œì œ ì—ì´ì „íŠ¸
    - full_exam: 5ê³¼ëª© Ã— 20ë¬¸í•­ = ì´ 100ë¬¸í•­
    - subject_quiz: íŠ¹ì • ê³¼ëª© ìµœëŒ€ 40ë¬¸í•­
    - ê³¼ëª©ë³„ ìƒì„±/ê²€ì¦ ë…¸ë“œ 2ê°œ(ì´ 10ê°œ)
    - ì‚¬ìš©ì ì§€ì • ë³‘ë ¬ ì‹¤í–‰
    - ë¨¸ì§€ ìˆœì„œ ê³ ì •
    """

    # 1) ê³¼ëª©/í‚¤ì›Œë“œ + full_exam ê¸°ë³¸ ì¹´ìš´íŠ¸(20)ë¡œ ë³€ê²½
    SUBJECT_AREAS = {
        "ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„": {
            "count": 20,
            "keywords": ["ìš”êµ¬ì‚¬í•­", "UI ì„¤ê³„", "ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„", "ì¸í„°í˜ì´ìŠ¤", "UML", "ê°ì²´ì§€í–¥", "ë””ìì¸íŒ¨í„´", "ëª¨ë“ˆí™”", "ê²°í•©ë„", "ì‘ì§‘ë„"]
        },
        "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ": {
            "count": 20,
            "keywords": ["ìë£Œêµ¬ì¡°", "ìŠ¤íƒ", "í", "ë¦¬ìŠ¤íŠ¸", "í†µí•©êµ¬í˜„", "ëª¨ë“ˆ", "íŒ¨í‚¤ì§•", "í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤", "ì•Œê³ ë¦¬ì¦˜", "ì¸í„°í˜ì´ìŠ¤"]
        },
        "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•": {
            "count": 20,
            "keywords": ["SQL", "íŠ¸ë¦¬ê±°", "DML", "DDL", "DCL", "ì •ê·œí™”", "ê´€ê³„í˜•ëª¨ë¸", "E-Rëª¨ë¸", "ë°ì´í„°ëª¨ë¸ë§", "ë¬´ê²°ì„±"]
        },
        "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©": {
            "count": 20,
            "keywords": ["ê°œë°œí™˜ê²½", "í”„ë¡œê·¸ë˜ë°ì–¸ì–´", "ë¼ì´ë¸ŒëŸ¬ë¦¬", "ìš´ì˜ì²´ì œ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°íƒ€ì…", "ë³€ìˆ˜", "ì—°ì‚°ì"]
        },
        "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬": {
            "count": 20,
            "keywords": ["ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œë°©ë²•ë¡ ", "í”„ë¡œì íŠ¸ê´€ë¦¬", "ë³´ì•ˆ", "ì‹œìŠ¤í…œë³´ì•ˆ", "ë„¤íŠ¸ì›Œí¬ë³´ì•ˆ", "í…Œì¼ëŸ¬ë§", "ìƒëª…ì£¼ê¸°ëª¨ë¸"]
        }
    }

    # 4) ìµœì¢… ë¨¸ì§€ ìˆœì„œ
    MERGE_ORDER = [
        "ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„",
        "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ",
        "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•",
        "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©",
        "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬",
    ]

    def __init__(self, data_folder=None, groq_api_key=None):
        if data_folder is None:
            base_dir = Path(__file__).resolve().parent
            data_folder = base_dir / "data"
        self.data_folder = Path(data_folder)
        os.makedirs(self.data_folder, exist_ok=True)

        if groq_api_key:
            os.environ["OPENAI_API_KEY"] = groq_api_key
        elif not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.embeddings_model = None
        self.llm = None
        self.vectorstore = None
        self.retriever = None
        self.workflow = None

        self.files_in_vectorstore = []

        self._initialize_models()
        self._build_graph()  # 2) ê³¼ëª©ë³„ 2ë…¸ë“œ(ìƒì„±/ê²€ì¦) êµ¬ì¶•

    @property
    def name(self) -> str:
        return "InfoProcessingExamAgent"

    @property
    def description(self) -> str:
        return "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ 5ê³¼ëª© ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±/ê²€ì¦í•˜ì—¬ 100ë¬¸ì œ(ë˜ëŠ” ê³¼ëª©ë³„ ì§€ì • ìˆ˜)ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤."

    def invoke(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Args (í™•ì¥):
          - mode: "full_exam" | "subject_quiz" | "partial_exam"
          - difficulty: "ì´ˆê¸‰" | "ì¤‘ê¸‰" | "ê³ ê¸‰" (default: "ì¤‘ê¸‰")
          - subject_area: subject_quiz ëª¨ë“œì—ì„œ í•„ìˆ˜
          - target_count: subject_quiz ëª¨ë“œì—ì„œ ìš”ì²­ ë¬¸í•­ ìˆ˜ (ìµœëŒ€ 40)
          - selected_subjects: partial_exam ëª¨ë“œì—ì„œ ì„ íƒí•  ê³¼ëª© ë¦¬ìŠ¤íŠ¸
          - questions_per_subject: partial_exam ëª¨ë“œì—ì„œ ê³¼ëª©ë‹¹ ë¬¸ì œ ìˆ˜
          - parallel_agents: ë™ì‹œ ë³‘ë ¬ ì‹¤í–‰ ê°œìˆ˜ (default: 2, ê¶Œì¥: 2~5)
          - save_to_file: bool
          - filename: ì €ì¥ íŒŒì¼ëª…
        """
        try:
            mode = input_data.get("mode", "full_exam")
            difficulty = input_data.get("difficulty", "ì¤‘ê¸‰")
            save_to_file = input_data.get("save_to_file", False)
            filename = input_data.get("filename")
            parallel_agents = max(1, int(input_data.get("parallel_agents", 2)))  # 3) ë³‘ë ¬ ê°œìˆ˜

            if not self._build_vectorstore_from_all_pdfs():
                return {
                    "success": False,
                    "error": f"'{self.data_folder}' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
                }

            if mode == "full_exam":
                # 1) 5ê³¼ëª© Ã— 20ë¬¸í•­ = ì´ 100ë¬¸í•­
                result = self._generate_full_exam(difficulty=difficulty,
                                                  parallel_agents=parallel_agents)
            elif mode == "partial_exam":
                # ì„ íƒëœ ê³¼ëª©ë“¤ì— ëŒ€í•´ ì§€ì •ëœ ë¬¸ì œ ìˆ˜ë§Œí¼ ìƒì„±
                selected_subjects = input_data.get("selected_subjects", [])
                questions_per_subject = input_data.get("questions_per_subject", 10)
                
                if not selected_subjects or not isinstance(selected_subjects, list):
                    return {
                        "success": False,
                        "error": "partial_exam ëª¨ë“œì—ì„œëŠ” selected_subjects ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    }
                
                if not all(subj in self.SUBJECT_AREAS for subj in selected_subjects):
                    invalid_subjects = [subj for subj in selected_subjects if subj not in self.SUBJECT_AREAS]
                    return {
                        "success": False,
                        "error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©ëª…ì…ë‹ˆë‹¤: {invalid_subjects}. ê°€ëŠ¥í•œ ê³¼ëª©: {list(self.SUBJECT_AREAS.keys())}"
                    }
                
                result = self._generate_partial_exam(
                    selected_subjects=selected_subjects,
                    questions_per_subject=questions_per_subject,
                    difficulty=difficulty,
                    parallel_agents=parallel_agents
                )
            elif mode == "subject_quiz":
                subject_area = input_data.get("subject_area")
                if not subject_area or subject_area not in self.SUBJECT_AREAS:
                    return {
                        "success": False,
                        "error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©ëª…ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê³¼ëª©: {list(self.SUBJECT_AREAS.keys())}"
                    }
                # ìµœëŒ€ 40ê°œ ì œí•œ
                target_count = min(int(input_data.get("target_count", 20)), 40)
                result = self._generate_subject_quiz(
                    subject_area=subject_area,
                    target_count=target_count,
                    difficulty=difficulty
                )
                # subject_quizëŠ” ë‹¨ì¼ ê³¼ëª© ê²°ê³¼ë§Œ ë¦¬í„´
                if "error" in result:
                    return {"success": False, "error": result["error"]}
                response = {"success": True, "result": result}
                if save_to_file:
                    try:
                        file_path = self._save_to_json(result, filename)
                        response["file_path"] = file_path
                    except Exception as e:
                        response["save_error"] = str(e)
                return response
            else:
                return {"success": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œì…ë‹ˆë‹¤. 'full_exam' ë˜ëŠ” 'subject_quiz'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."}

            if "error" in result:
                return {"success": False, "error": result["error"]}

            response = {"success": True, "result": result}
            if save_to_file:
                try:
                    file_path = self._save_to_json(result, filename)
                    response["file_path"] = file_path
                except Exception as e:
                    response["save_error"] = str(e)
            return response

        except Exception as e:
            return {"success": False, "error": f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def _initialize_models(self):
        try:
            self.embeddings_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            self.llm = ChatOpenAI(
                model=OPENAI_LLM_MODEL,
                temperature=LLM_TEMPERATURE,
                max_tokens=LLM_MAX_TOKENS,
                timeout=LLM_TIMEOUT,
                max_retries=LLM_MAX_RETRIES,
                base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"),
                api_key=os.getenv("OPENAI_API_KEY")
            )
            _ = self.llm.invoke("ping")
        except Exception as e:
            raise ValueError(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _build_vectorstore_from_all_pdfs(self) -> bool:
        pdf_files = self.get_pdf_files()
        if not pdf_files:
            return False
        if self.vectorstore and set(self.files_in_vectorstore) == set(pdf_files):
            return True

        all_documents = []
        for pdf_path in pdf_files:
            try:
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                for doc in documents:
                    doc.metadata['source_file'] = os.path.basename(pdf_path)
                all_documents.extend(documents)
            except Exception:
                continue

        if not all_documents:
            return False

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = splitter.split_documents(all_documents)
        self.vectorstore = FAISS.from_documents(splits, self.embeddings_model)
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 15})
        self.files_in_vectorstore = pdf_files
        return True

    def get_pdf_files(self) -> List[str]:
        return glob.glob(os.path.join(self.data_folder, "*.pdf"))

    # ---- ê³µí†µ ë…¸ë“œ êµ¬í˜„(ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©) ----
    def _retrieve_documents(self, state: GraphState) -> GraphState:
        try:
            query = state["query"]
            subject_area = state.get("subject_area", "")
            enhanced_query = f"{subject_area} {query}".strip()
            print(f"[DEBUG] _retrieve_documents: query='{query}', subject_area='{subject_area}', enhanced_query='{enhanced_query}'")
            documents = self.retriever.invoke(enhanced_query)
            print(f"[DEBUG] _retrieve_documents: found {len(documents)} documents")
            source_files = [doc.metadata.get('source_file', 'Unknown') for doc in documents]
            used_sources = list(Counter(source_files).keys())
            return {**state, "documents": documents, "used_sources": used_sources}
        except Exception as e:
            print(f"[DEBUG] _retrieve_documents: error {e}")
            return {**state, "error": f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}

    def _prepare_context(self, state: GraphState) -> GraphState:
        documents = state.get("documents", [])
        key_sents = []
        for doc in documents:
            for line in doc.page_content.split("\n"):
                line = line.strip()
                if len(line) > 100 or any(k in line for k in ["ì •ì˜", "íŠ¹ì§•", "ì¢…ë¥˜", "ì˜ˆì‹œ", "ì›ë¦¬", "êµ¬ì„±", "ì ˆì°¨", "ì¥ì ", "ë‹¨ì "]):
                    key_sents.append(line)
        context = "\n".join(key_sents)[:2000]
        # subject_areaë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
        subject_area = state.get("subject_area", "")
        print(f"[DEBUG] _prepare_context: subject_area='{subject_area}'")
        return {**state, "context": context, "subject_area": subject_area}

    def _generate_quiz_incremental(self, state: GraphState) -> GraphState:
        try:
            context = state.get("context", "")
            target_quiz_count = state.get("target_quiz_count", 5)
            validated_questions = state.get("validated_questions", [])
            subject_area = state.get("subject_area", "")
            needed_count = target_quiz_count - len(validated_questions)
            print(f"[DEBUG] _generate_quiz_incremental: context_len={len(context)}, target={target_quiz_count}, validated={len(validated_questions)}, needed={needed_count}")

            if needed_count <= 0:
                return {**state, "quiz_questions": validated_questions}
            if not context.strip():
                new_attempts = state.get("generation_attempts", 0) + 1
                print(f"[DEBUG] _generate_quiz_incremental: no context, attempts={new_attempts}")
                return {
                    **state, 
                    "quiz_questions": [],
                    "validated_questions": validated_questions,
                    "generation_attempts": new_attempts,
                    "error": "ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
                }

            generate_count = max(min(needed_count, 10), 1)

            prompt_template = PromptTemplate(
                input_variables=["context", "subject_area", "needed_count"],
                template=(
                    "ë‹¹ì‹ ì€ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {subject_area} ê³¼ëª©ì˜ ê°ê´€ì‹ ë¬¸ì œ {needed_count}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n\n"
                    "**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**\n\n"
                    "[ë¬¸ì„œ ë‚´ìš©]\n{context}\n\n"
                    "[ì‘ë‹µ í˜•ì‹]\n"
                    "{{\n"
                    "  \"questions\": [\n"
                    "    {{\n"
                    "      \"question\": \"ë¬¸ì œ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±\",\n"
                    "      \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", \"ì„ íƒì§€3\", \"ì„ íƒì§€4\"],\n"
                    "      \"answer\": \"1\",ì„ íƒì§€ì˜ ìˆ«ìë§Œ ì¶œë ¥í•´ì•¼ í•¨\n"
                    "      \"explanation\": \"ì •ë‹µì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ì„¤\"\n"
                    "    }}\n"
                    "  ]\n"
                    "}}\n\n"
                    "**ì‘ë‹µì€ ìœ„ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**"
                )
            )

            prompt = prompt_template.format(
                context=context, subject_area=subject_area, needed_count=generate_count
            )

            print(f"[DEBUG] _generate_quiz_incremental: calling LLM for {generate_count} questions")
            self.llm.temperature = 0.2
            self.llm.max_tokens = 1024
            response = self.llm.invoke(prompt)
            response_content = getattr(response, "content", str(response))
            print(f"[DEBUG] _generate_quiz_incremental: LLM response length={len(response_content)}")
            
            new_questions = self._parse_quiz_response(response_content, subject_area)
            print(f"[DEBUG] _generate_quiz_incremental: parsed {len(new_questions)} questions")

            if not new_questions:
                new_attempts = state.get("generation_attempts", 0) + 1
                print(f"[DEBUG] _generate_quiz_incremental: failed to generate questions, attempts={new_attempts}")
                return {
                    **state,
                    "quiz_questions": [],
                    "validated_questions": validated_questions,
                    "generation_attempts": new_attempts,
                    "error": "ìœ íš¨í•œ ë¬¸ì œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                }

            new_attempts = state.get("generation_attempts", 0) + 1
            print(f"[DEBUG] _generate_quiz_incremental: generated {len(new_questions)} questions, attempts={new_attempts}")
            return {
                **state,
                "quiz_questions": new_questions,
                "validated_questions": validated_questions,
                "generation_attempts": new_attempts
            }
        except Exception as e:
            new_attempts = state.get("generation_attempts", 0) + 1
            print(f"[DEBUG] _generate_quiz_incremental: exception {e}, attempts={new_attempts}")
            return {
                **state, 
                "quiz_questions": [],
                "validated_questions": state.get("validated_questions", []),
                "generation_attempts": new_attempts,
                "error": f"ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            }

    def _validate_quiz_incremental(self, state: GraphState) -> GraphState:
        subject_area = state.get("subject_area", "")
        previously_validated = state.get("validated_questions", [])
        new_questions = state.get("quiz_questions", [])
        context = state.get("context", "")
        target_quiz_count = state.get("target_quiz_count", 5)
        generation_attempts = state.get("generation_attempts", 0)
        error = state.get("error", "")

        print(f"[DEBUG] _validate_quiz_incremental: subject={subject_area}, new_questions={len(new_questions)}, previously_validated={len(previously_validated)}, error={error}")

        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ê²€ì¦í•˜ì§€ ì•Šê³  ì—ëŸ¬ ìƒíƒœ ìœ ì§€
        if error:
            print(f"[DEBUG] _validate_quiz_incremental: skipping validation due to error: {error}")
            return state

        if not new_questions:
            print(f"[DEBUG] _validate_quiz_incremental: no new questions to validate")
            return {**state, "validated_questions": previously_validated}

        newly_validated = []
        print(f"[DEBUG] _validate_quiz_incremental: validating {len(new_questions)} questions")

        # ê°„ë‹¨í•œ ê²€ì¦: ëª¨ë“  ë¬¸ì œë¥¼ ìœ íš¨í•˜ë‹¤ê³  ê°€ì • (LLM í˜¸ì¶œ ì—†ì´)
        # ì‹¤ì œë¡œëŠ” LLM ê²€ì¦ì„ í•  ìˆ˜ ìˆì§€ë§Œ, í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
        for q in new_questions:
            if len(newly_validated) >= target_quiz_count - len(previously_validated):
                break
            # ê¸°ë³¸ ê²€ì¦: í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            if q.get("question") and q.get("options") and q.get("answer") and q.get("explanation"):
                newly_validated.append(q)
                print(f"[DEBUG] _validate_quiz_incremental: validated question: {q.get('question', '')[:50]}...")

        all_validated = previously_validated + newly_validated
        print(f"[DEBUG] _validate_quiz_incremental: total validated: {len(all_validated)}/{target_quiz_count}")

        return {
            **state,
            "validated_questions": all_validated,
            "quiz_questions": all_validated,
            "error": ""  # ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”
        }

    def _check_completion(self, state: GraphState) -> str:
        validated_count = len(state.get("validated_questions", []))
        target_count = state.get("target_quiz_count", 5)
        generation_attempts = state.get("generation_attempts", 0)
        error = state.get("error", "")
        
        print(f"[DEBUG] _check_completion: validated={validated_count}, target={target_count}, attempts={generation_attempts}, error={error}")
        
        # ëª©í‘œ ë‹¬ì„±
        if validated_count >= target_count:
            print(f"[DEBUG] Target reached ({validated_count}/{target_count}), completing")
            return "complete"
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬
        if generation_attempts >= 5:  # 5íšŒë¡œ ì¦ê°€
            print(f"[DEBUG] Max attempts reached ({generation_attempts}), completing")
            return "complete"
        
        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
        if error:
            print(f"[DEBUG] Error detected: {error}, completing")
            return "complete"
        
        # ê³„ì† ìƒì„±
        print(f"[DEBUG] Need more questions ({validated_count}/{target_count}), continuing generation (attempt {generation_attempts})")
        return "generate_more"

    def _parse_quiz_response(self, response: str, subject_area: str = "") -> List[Dict[str, Any]]:
        try:
            print(f"[DEBUG] _parse_quiz_response: raw response length={len(response)}")
            print(f"[DEBUG] _parse_quiz_response: response preview='{response[:200]}...'")
            
            # 1. JSON ë¸”ë¡ ì°¾ê¸° (```json ... ```)
            json_block_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_block_match:
                json_str = json_block_match.group(1)
                print(f"[DEBUG] _parse_quiz_response: found JSON block, length={len(json_str)}")
            else:
                # 2. ì¼ë°˜ JSON ê°ì²´ ì°¾ê¸°
                json_str_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response.strip(), re.DOTALL)
                if not json_str_match:
                    print(f"[DEBUG] _parse_quiz_response: no JSON found in response")
                    return []
                json_str = json_str_match.group(0)
                print(f"[DEBUG] _parse_quiz_response: found JSON object, length={len(json_str)}")

            # 3. JSON ë¬¸ìì—´ ì •ë¦¬
            json_str = json_str.replace('\\u312f', '').replace('\\n', ' ').replace('\\', '')
            print(f"[DEBUG] _parse_quiz_response: cleaned JSON='{json_str[:200]}...'")
            
            # 4. JSON íŒŒì‹±
            data = json.loads(json_str)
            if "questions" not in data or not isinstance(data["questions"], list):
                print(f"[DEBUG] _parse_quiz_response: invalid data structure, keys={list(data.keys()) if isinstance(data, dict) else 'not dict'}")
                return []

            questions = data.get("questions", [])
            print(f"[DEBUG] _parse_quiz_response: found {len(questions)} questions")
            
            # 5. ê° ë¬¸ì œ ì²˜ë¦¬
            for i, question in enumerate(questions):
                if "options" in question and isinstance(question["options"], list):
                    numbered_options = []
                    for j, option_text in enumerate(question["options"], 1):
                        cleaned_text = re.sub(r'^\s*\d+\.\s*', '', option_text).strip()
                        numbered_options.append(f"  {j}. {cleaned_text}")
                    question["options"] = numbered_options
                if "subject" not in question:
                    question["subject"] = subject_area
                print(f"[DEBUG] _parse_quiz_response: processed question {i+1}: {question.get('question', '')[:50]}...")
            
            return questions
        except Exception as e:
            print(f"[DEBUG] _parse_quiz_response: exception during parsing: {e}")
            print(f"[DEBUG] _parse_quiz_response: response that caused error: '{response[:500]}...'")
            return []

    # ---------- í•µì‹¬: ê·¸ë˜í”„ êµ¬ì„± ë³€ê²½ (ê³¼ëª©ë³„ 2ë…¸ë“œ Ã— 5ê³¼ëª© = 10ë…¸ë“œ) ----------
    def _build_graph(self):
        """
        ê³µí†µ ì‚¬ì „ ë‹¨ê³„: retrieve -> prepare_context
        ì´í›„ ê³¼ëª©ë³„ ë¼ìš°íŒ…: (subject)generate -> (subject)validate -> ì¡°ê±´ë¶€ ë£¨í”„
        """
        workflow = StateGraph(GraphState)

        # ê³µí†µ ì „ì²˜ë¦¬
        workflow.add_node("retrieve", self._retrieve_documents)
        workflow.add_node("prepare_context", self._prepare_context)

        # ê³¼ëª©ë³„ ë…¸ë“œ í•¨ìˆ˜: subjectë¥¼ í´ë¡œì €ë¡œ ë¬¶ì–´ 2ê°œ ë…¸ë“œ ìƒì„±
        def make_generate_node(subject_name):
            def _gen(state: GraphState) -> GraphState:
                # subject_nameì„ stateì— ë³´ì¦
                print(f"[DEBUG] {subject_name}_generate ë…¸ë“œ ì‹¤í–‰")
                state = {**state, "subject_area": subject_name}
                return self._generate_quiz_incremental(state)
            return _gen

        def make_validate_node(subject_name):
            def _val(state: GraphState) -> GraphState:
                state = {**state, "subject_area": subject_name}
                return self._validate_quiz_incremental(state)
            return _val

        # ê³¼ëª©ë³„ ë…¸ë“œ ì¶”ê°€
        subject_to_nodes = {}
        for subj in self.SUBJECT_AREAS.keys():
            gen_name = f"{subj}_generate"
            val_name = f"{subj}_validate"
            workflow.add_node(gen_name, make_generate_node(subj))
            workflow.add_node(val_name, make_validate_node(subj))
            # ê³¼ëª©ë³„ ë‚´ë¶€ ì—£ì§€
            workflow.add_edge(gen_name, val_name)
            workflow.add_conditional_edges(
                val_name,
                self._check_completion,
                {"generate_more": gen_name, "complete": END}
            )
            subject_to_nodes[subj] = (gen_name, val_name)

        # ë¼ìš°í„°: prepare_context ì´í›„ ê³¼ëª©ë³„ generateë¡œ ë¶„ê¸°
        def _route_to_subject(state: GraphState) -> str:
            subj = state.get("subject_area", "")
            print(f"[DEBUG] _route_to_subject: subject_area='{subj}', available_subjects={list(subject_to_nodes.keys())}")
            if subj in subject_to_nodes:
                gen_name, val_name = subject_to_nodes[subj]  # íŠœí”Œ ì–¸íŒ¨í‚¹
                print(f"[DEBUG] Found subject '{subj}', returning generate node: {gen_name}")
                return gen_name  # generate ë…¸ë“œëª…ë§Œ ë°˜í™˜
            # ê¸°ë³¸ê°’(ì•ˆ ë§ìœ¼ë©´ ì„¤ê³„ë¡œ)
            print(f"[DEBUG] Subject '{subj}' not found, using default: ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„")
            gen_name, val_name = subject_to_nodes["ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„"]
            return gen_name

        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "prepare_context")
        
        # ìˆ˜ì •: _route_to_subject í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ê°’ê³¼ ë…¸ë“œëª…ì„ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
        # _route_to_subjectëŠ” ë…¸ë“œëª…ì„ ë°˜í™˜í•˜ë¯€ë¡œ, routing_dictëŠ” {ë…¸ë“œëª…: ë…¸ë“œëª…} í˜•íƒœì—¬ì•¼ í•¨
        routing_dict = {subject_to_nodes[subj][0]: subject_to_nodes[subj][0] for subj in subject_to_nodes.keys()}
        print(f"[DEBUG] routing_dict: {routing_dict}")
        print(f"[DEBUG] Available nodes: {list(workflow.nodes.keys())}")
        print(f"[DEBUG] routing_dict keys: {list(routing_dict.keys())}")
        print(f"[DEBUG] routing_dict keys in nodes: {[k in workflow.nodes for k in routing_dict.keys()]}")
        workflow.add_conditional_edges("prepare_context", _route_to_subject, routing_dict)

        self.workflow = workflow.compile()
    # --------------------------------------------------------------------

    # ë‹¨ì¼ ê³¼ëª© ìƒì„±(ë‚´ë¶€ëŠ” ê·¸ë˜í”„ í•œ ë²ˆ ì‹¤í–‰)
    def _generate_subject_quiz(self, subject_area: str, target_count: int = 5, difficulty: str = "ì¤‘ê¸‰") -> Dict[str, Any]:
        if subject_area not in self.SUBJECT_AREAS:
            return {"error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ê³¼ëª©: {subject_area}"}
        keywords = self.SUBJECT_AREAS[subject_area]["keywords"]

        all_validated_questions = []
        max_rounds = 10
        current_round = 0

        while len(all_validated_questions) < target_count and current_round < max_rounds:
            current_round += 1
            remaining_needed = target_count - len(all_validated_questions)

            for i in range(0, len(keywords), 2):
                if len(all_validated_questions) >= target_count:
                    break
                combo = " ".join(keywords[i:i+3])

                initial_state = {
                    "query": combo,
                    "target_quiz_count": remaining_needed,
                    "difficulty": difficulty,
                    "generation_attempts": 0,
                    "quiz_questions": [],
                    "validated_questions": [],
                    "subject_area": subject_area
                }
                # ê³¼ëª©ë³„ ë¼ìš°íŒ… ê·¸ë˜í”„ ë‹¨ë°œ ì‹¤í–‰
                result = self.workflow.invoke(initial_state)

                if result.get("error"):
                    continue

                new_qs = result.get("validated_questions", [])
                if new_qs:
                    # ì¤‘ë³µ ì œê±°
                    exists = {q.get("question", "") for q in all_validated_questions}
                    for q in new_qs:
                        if q.get("question", "") not in exists:
                            all_validated_questions.append(q)
                            exists.add(q.get("question", ""))

                if len(all_validated_questions) >= target_count:
                    break

            if current_round < max_rounds and len(all_validated_questions) < target_count:
                time.sleep(1.5)

        final_questions = all_validated_questions[:target_count]
        return {
            "subject_area": subject_area,
            "difficulty": difficulty,
            "requested_count": target_count,
            "quiz_count": len(final_questions),
            "questions": final_questions,
            "status": "SUCCESS" if len(final_questions) >= target_count else "PARTIAL"
        }

    # 3) ì‚¬ìš©ì ì§€ì • ë³‘ë ¬ ì‹¤í–‰ë¡œ 5ê³¼ëª© ë™ì‹œ ì²˜ë¦¬(ìµœëŒ€ parallel_agents ë™ì‹œ)
    def _generate_full_exam(self, difficulty: str = "ì¤‘ê¸‰", parallel_agents: int = 2) -> Dict[str, Any]:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        start_time = time.time()

        requested_per_subject = {s: info["count"] for s, info in self.SUBJECT_AREAS.items()}

        full_exam_result = {
            "exam_title": "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ëª¨ì˜ê³ ì‚¬",
            "total_questions": 0,
            "difficulty": difficulty,
            "subjects": {},
            "all_questions": [],
            "generation_summary": {},
            "failed_subjects": [],
            "model_info": "Groq llama-4-scout-17b-16e-instruct",
            "parallel_agents": parallel_agents
        }

        # ë³‘ë ¬ë¡œ ê³¼ëª© ìƒì„± ì‹¤í–‰
        futures = {}
        with ThreadPoolExecutor(max_workers=parallel_agents) as ex:
            for subject_area, target in requested_per_subject.items():
                futures[ex.submit(
                    self._generate_subject_quiz,
                    subject_area=subject_area,
                    target_count=target,
                    difficulty=difficulty
                )] = subject_area

            per_subject_results = {}
            for fut in as_completed(futures):
                subject_area = futures[fut]
                try:
                    per_subject_results[subject_area] = fut.result()
                except Exception as e:
                    per_subject_results[subject_area] = {"error": str(e)}

        # 4) ë¨¸ì§€ ìˆœì„œì— ë”°ë¼ ì·¨í•©
        total_generated = 0
        merged_questions = []
        for subject_area in self.MERGE_ORDER:
            res = per_subject_results.get(subject_area, {"error": "ê²°ê³¼ ì—†ìŒ"})
            if "error" in res:
                full_exam_result["failed_subjects"].append({
                    "subject": subject_area,
                    "error": res["error"]
                })
                full_exam_result["subjects"][subject_area] = {
                    "requested_count": requested_per_subject[subject_area],
                    "actual_count": 0,
                    "questions": [],
                    "status": "FAILED"
                }
            else:
                qs = res.get("questions", [])
                total_generated += len(qs)
                merged_questions.extend(qs)
                full_exam_result["subjects"][subject_area] = {
                    "requested_count": requested_per_subject[subject_area],
                    "actual_count": len(qs),
                    "questions": qs,
                    "status": res.get("status", "UNKNOWN")
                }

        elapsed_time = time.time() - start_time
        full_exam_result["total_questions"] = total_generated
        full_exam_result["all_questions"] = merged_questions
        full_exam_result["generation_summary"] = {
            "target_total": sum(requested_per_subject.values()),  # 100
            "actual_total": total_generated,
            "success_rate": f"{(total_generated / max(1, sum(requested_per_subject.values())))*100:.1f}%",
            "successful_subjects": 5 - len(full_exam_result["failed_subjects"]),
            "failed_subjects": len(full_exam_result["failed_subjects"]),
            "completion_status": "COMPLETE" if total_generated >= sum(requested_per_subject.values()) else "PARTIAL",
            "generation_time": f"{elapsed_time:.1f}ì´ˆ"
        }
        return full_exam_result

    def _generate_partial_exam(self, selected_subjects: List[str], questions_per_subject: int = 10, 
                              difficulty: str = "ì¤‘ê¸‰", parallel_agents: int = 2) -> Dict[str, Any]:
        """ì„ íƒëœ ê³¼ëª©ë“¤ì— ëŒ€í•´ ì§€ì •ëœ ë¬¸ì œ ìˆ˜ë§Œí¼ ìƒì„±"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        start_time = time.time()

        self._generate_workflow_diagram("partial_exam", {
            "selected_subjects": selected_subjects,
            "questions_per_subject": questions_per_subject,
            "difficulty": difficulty,
            "parallel_agents": parallel_agents
        })

        partial_exam_result = {
            "exam_title": f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì„ íƒê³¼ëª© ëª¨ì˜ê³ ì‚¬ ({len(selected_subjects)}ê³¼ëª©)",
            "total_questions": 0,
            "difficulty": difficulty,
            "selected_subjects": selected_subjects,
            "questions_per_subject": questions_per_subject,
            "subjects": {},
            "all_questions": [],
            "generation_summary": {},
            "failed_subjects": [],
            "model_info": "Groq llama-4-scout-17b-16e-instruct",
            "parallel_agents": parallel_agents
        }

        # ë³‘ë ¬ë¡œ ì„ íƒëœ ê³¼ëª© ìƒì„± ì‹¤í–‰
        futures = {}
        with ThreadPoolExecutor(max_workers=parallel_agents) as ex:
            for subject_area in selected_subjects:
                futures[ex.submit(
                    self._generate_subject_quiz,
                    subject_area=subject_area,
                    target_count=questions_per_subject,
                    difficulty=difficulty
                )] = subject_area

            per_subject_results = {}
            for fut in as_completed(futures):
                subject_area = futures[fut]
                try:
                    per_subject_results[subject_area] = fut.result()
                except Exception as e:
                    per_subject_results[subject_area] = {"error": str(e)}

        # ê²°ê³¼ ì·¨í•©
        total_generated = 0
        merged_questions = []
        for subject_area in selected_subjects:
            res = per_subject_results.get(subject_area, {"error": "ê²°ê³¼ ì—†ìŒ"})
            if "error" in res:
                partial_exam_result["failed_subjects"].append({
                    "subject": subject_area,
                    "error": res["error"]
                })
                partial_exam_result["subjects"][subject_area] = {
                    "requested_count": questions_per_subject,
                    "actual_count": 0,
                    "questions": [],
                    "status": "FAILED"
                }
            else:
                qs = res.get("questions", [])
                total_generated += len(qs)
                merged_questions.extend(qs)
                partial_exam_result["subjects"][subject_area] = {
                    "requested_count": questions_per_subject,
                    "actual_count": len(qs),
                    "questions": qs,
                    "status": res.get("status", "UNKNOWN")
                }

        elapsed_time = time.time() - start_time
        partial_exam_result["total_questions"] = total_generated
        partial_exam_result["all_questions"] = merged_questions
        partial_exam_result["generation_summary"] = {
            "target_total": len(selected_subjects) * questions_per_subject,
            "actual_total": total_generated,
            "success_rate": f"{(total_generated / max(1, len(selected_subjects) * questions_per_subject))*100:.1f}%",
            "successful_subjects": len(selected_subjects) - len(partial_exam_result["failed_subjects"]),
            "failed_subjects": len(partial_exam_result["failed_subjects"]),
            "completion_status": "COMPLETE" if total_generated >= len(selected_subjects) * questions_per_subject else "PARTIAL",
            "generation_time": f"{elapsed_time:.1f}ì´ˆ"
        }
        return partial_exam_result

    def _save_to_json(self, exam_result: Dict[str, Any], filename: str = None) -> str:
        save_dir = "C:\\ET_Agent\\teacher\\TestGenerator\\test"
        os.makedirs(save_dir, exist_ok=True)

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if "exam_title" in exam_result:
                filename = f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬_ëª¨ì˜ê³ ì‚¬_100ë¬¸ì œ_{timestamp}.json"
            else:
                subject = exam_result.get("subject_area", "ë¬¸ì œ")
                count = exam_result.get("quiz_count", 0)
                filename = f"{subject}_{count}ë¬¸ì œ_{timestamp}.json"

        if not os.path.isabs(filename):
            filename = os.path.join(save_dir, filename)
        elif not filename.startswith(save_dir):
            filename = os.path.join(save_dir, os.path.basename(filename))

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(exam_result, f, ensure_ascii=False, indent=2)
        return filename

    def _save_to_json(self, exam_result: Dict[str, Any], filename: str = None) -> str:
        """ì‹œí—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬_ë¬¸ì œìƒì„±_{timestamp}.json"
        
        filepath = os.path.join(os.path.dirname(__file__), "test", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(exam_result, f, ensure_ascii=False, indent=2)
        return filename

    def _generate_workflow_diagram(self, mode: str, params: Dict[str, Any]) -> None:
        """Graphvizë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œ ìƒì„± ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            from graphviz import Digraph
            import time
            
            # ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
            dot = Digraph(comment=f'ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ë¬¸ì œ ìƒì„± ì›Œí¬í”Œë¡œìš° - {mode}')
            dot.attr(rankdir='TB', size='12,8')
            
            # ë…¸ë“œ ìŠ¤íƒ€ì¼ ì •ì˜
            dot.attr('node', shape='box', style='rounded,filled', fontname='Arial', fontsize='10')
            
            # ì‹œì‘ ë…¸ë“œ
            dot.node('start', 'ì‚¬ìš©ì ì…ë ¥', fillcolor='lightgreen')
            
            # ì…ë ¥ íŒŒì‹± ë…¸ë“œë“¤
            dot.node('parse', 'LLM ê¸°ë°˜\nì…ë ¥ íŒŒì‹±', fillcolor='lightblue')
            dot.node('validate', 'íŒŒë¼ë¯¸í„° ê²€ì¦\n(ê³¼ëª©/ë¬¸ì œìˆ˜/ë‚œì´ë„)', fillcolor='lightyellow')
            
            # ëª¨ë“œë³„ ë¶„ê¸°
            if mode == "partial_exam":
                dot.node('mode', 'PARTIAL_EXAM\n(ì„ íƒê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('parallel', 'ë³‘ë ¬ ì²˜ë¦¬\n(ThreadPoolExecutor)', fillcolor='lightcoral')
                dot.node('merge', 'ê²°ê³¼ í†µí•©\n(ê³¼ëª©ë³„ ê²°ê³¼ ë³‘í•©)', fillcolor='lightcoral')
            elif mode == "single_subject":
                dot.node('mode', 'SINGLE_SUBJECT\n(ë‹¨ì¼ ê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('single', 'ë‹¨ì¼ ì—ì´ì „íŠ¸\n(ì§ë ¬ ì²˜ë¦¬)', fillcolor='lightcoral')
            else:
                dot.node('mode', 'FULL_EXAM\n(ì „ì²´ ê³¼ëª© ëª¨ë“œ)', fillcolor='orange')
                dot.node('full_parallel', 'ì „ì²´ ë³‘ë ¬\n(5ê³¼ëª© ë™ì‹œ)', fillcolor='lightcoral')
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            dot.node('agent', 'TestGenerator\n.execute()', fillcolor='lightpink')
            dot.node('result', 'ê²°ê³¼ ì²˜ë¦¬\n(ëª¨ë“œë³„ ê²°ê³¼ ì¶”ì¶œ)', fillcolor='lightcyan')
            
            # ë°ì´í„° ë³€í™˜
            dot.node('transform', 'ë°ì´í„° ë³€í™˜\n(QA í˜•ì‹)', fillcolor='lightcyan')
            dot.node('output', 'ì¶œë ¥\n(JSON/PDF)', fillcolor='lightgreen')
            
            # ì—£ì§€ ì—°ê²°
            dot.edge('start', 'parse')
            dot.edge('parse', 'validate')
            dot.edge('validate', 'mode')
            
            if mode == "partial_exam":
                dot.edge('mode', 'parallel')
                dot.edge('parallel', 'agent')
                dot.edge('agent', 'merge')
                dot.edge('merge', 'result')
            elif mode == "single_subject":
                dot.edge('mode', 'single')
                dot.edge('single', 'agent')
                dot.edge('agent', 'result')
            else:
                dot.edge('mode', 'full_parallel')
                dot.edge('full_parallel', 'agent')
                dot.edge('agent', 'result')
            
            dot.edge('result', 'transform')
            dot.edge('transform', 'output')
            
            # ì„œë¸Œê·¸ë˜í”„ë¡œ ê³¼ëª©ë³„ ì²˜ë¦¬ êµ¬ì¡° í‘œì‹œ
            if mode == "partial_exam":
                with dot.subgraph(name='cluster_subjects') as c:
                    c.attr(label='ê³¼ëª©ë³„ ë³‘ë ¬ ì²˜ë¦¬', style='filled', fillcolor='lightgray')
                    subjects = params.get("selected_subjects", [])
                    count_per_subject = params.get("questions_per_subject", 10)
                    for i, subject in enumerate(subjects):
                        c.node(f'subject_{i}', f'{subject}\n({count_per_subject}ë¬¸ì œ)')
                        if i > 0:
                            c.edge(f'subject_{i-1}', f'subject_{i}', style='dashed')
            
            # íŒŒì¼ ì €ì¥
            output_dir = os.path.join(os.path.dirname(__file__), "workflow_diagrams")
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"generation_workflow_{mode}_{timestamp}"
            filepath = os.path.join(output_dir, filename)
            
            dot.render(filepath, format='png', cleanup=True)
            print(f"\n ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì™„ë£Œ: {filepath}.png")
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ë„ ì¶œë ¥
            print(f"\nğŸ“Š ì›Œí¬í”Œë¡œìš° ìš”ì•½:")
            print(f"   â”Œâ”€ ëª¨ë“œ: {mode.upper()}")
            if mode == "partial_exam":
                subjects = params.get("selected_subjects", [])
                count_per_subject = params.get("questions_per_subject", 10)
                print(f"   â”œâ”€ ì„ íƒëœ ê³¼ëª©: {', '.join(subjects)}")
                print(f"   â”œâ”€ ê³¼ëª©ë‹¹ ë¬¸ì œ ìˆ˜: {count_per_subject}ê°œ")
                print(f"   â””â”€ ë³‘ë ¬ ì—ì´ì „íŠ¸: {params.get('parallel_agents', 2)}ê°œ")
            
        except ImportError:
            print("\nâš ï¸  Graphvizê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install graphvizë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"\nâŒ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}")
