import os
import glob
from typing import List, Dict, Any
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import FAISS  # FAISS ì œê±°
from langchain.schema import Document
from langchain_huggingface import HuggingFaceEmbeddings
# ğŸ” Milvus ê´€ë ¨ ì„í¬íŠ¸ ì¶”ê°€
from langchain_milvus import Milvus
from pymilvus import connections, utility
from collections import Counter


class RAGEngine:
    """
    RAG(Retrieval-Augmented Generation) ì—”ì§„
    PDF íŒŒì¼ ë¡œë”©, ì„ë² ë”©, ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬, ë¬¸ì„œ ê²€ìƒ‰ì„ ë‹´ë‹¹
    """
    
    def __init__(self, data_folder: str = os.path.join(os.path.dirname(__file__),"data")):
        """
        RAG ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            data_folder: PDF íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
        """
        self.data_folder = data_folder
        os.makedirs(self.data_folder, exist_ok=True)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings_model = None
        self.vectorstore = None
        self.retriever = None
        
        # ë²¡í„° ìŠ¤í† ì–´ì— í¬í•¨ëœ íŒŒì¼ ëª©ë¡ ì¶”ì 
        self.files_in_vectorstore = []
        
        # ğŸ”§ Milvus ì„¤ì • ì¶”ê°€
        self.milvus_conf = {
            "host": os.getenv("MILVUS_HOST", "localhost"),
            "port": os.getenv("MILVUS_PORT", "19530"),
            "collection": os.getenv("MILVUS_COLLECTION", "rag_documents"),
            "topk": int(os.getenv("MILVUS_TOPK", "15")),
            "drop_existing": os.getenv("MILVUS_DROP_EXISTING", "false").lower() == "true",
        }
        
        self._initialize_embeddings()
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.embeddings_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            raise ValueError(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def get_pdf_files(self) -> List[str]:
        """
        data í´ë”ì—ì„œ PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Returns:
            PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        return glob.glob(os.path.join(self.data_folder, "*.pdf"))
    
    def build_vectorstore_from_all_pdfs(self) -> bool:
        """
        ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±/ì—…ë°ì´íŠ¸ (ì¦ë¶„ ë°©ì‹)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        pdf_files = self.get_pdf_files()
        if not pdf_files:
            return False

        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆê³  íŒŒì¼ ëª©ë¡ì´ ê°™ìœ¼ë©´ ì¬ì‚¬ìš©
        if self.vectorstore and set(self.files_in_vectorstore) == set(pdf_files):
            return True

        # ìƒˆë¡œìš´ íŒŒì¼ë§Œ ì°¾ê¸° (ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œì—ëŠ” ëª¨ë“  íŒŒì¼)
        new_files = []
        if self.vectorstore:
            new_files = [f for f in pdf_files if f not in self.files_in_vectorstore]
            print(f"ğŸ“ ìƒˆë¡œìš´ íŒŒì¼ {len(new_files)}ê°œ ë°œê²¬")
        else:
            # ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œì—ëŠ” ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬
            new_files = pdf_files
            print(f"ğŸ“ ì²« ë²ˆì§¸ ì‹¤í–‰: {len(new_files)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬")
        
        if not new_files and self.vectorstore:
            return True  # ë³€ê²½ì‚¬í•­ ì—†ìŒ

        # ìƒˆë¡œìš´ íŒŒì¼ë“¤ë§Œ ì²˜ë¦¬
        new_documents = []
        for pdf_path in new_files:
            try:
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                for doc in documents:
                    # ğŸ”§ PDF ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë° Milvus ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì¡°ì •
                    metadata = doc.metadata.copy()
                    
                    # í•„ìˆ˜ í•„ë“œ ë³´ì¥
                    metadata['source_file'] = os.path.basename(pdf_path)
                    
                    # title í•„ë“œê°€ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
                    if not metadata.get('title') or metadata['title'] is None:
                        metadata['title'] = os.path.basename(pdf_path)
                    
                    # author í•„ë“œê°€ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
                    if not metadata.get('author') or metadata['author'] is None:
                        metadata['author'] = 'Unknown'
                    
                    # ê¸°íƒ€ í•„ë“œë“¤ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    for key in ['producer', 'creator', 'creationdate', 'moddate', 'source']:
                        if key not in metadata or metadata[key] is None:
                            metadata[key] = ''
                    
                    # page ê´€ë ¨ í•„ë“œ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
                    if 'page' in metadata and metadata['page'] is not None:
                        try:
                            metadata['page'] = int(metadata['page'])
                        except (ValueError, TypeError):
                            metadata['page'] = 0
                    
                    if 'total_pages' in metadata and metadata['total_pages'] is not None:
                        try:
                            metadata['total_pages'] = int(metadata['total_pages'])
                        except (ValueError, TypeError):
                            metadata['total_pages'] = 1
                    
                    # ìˆ˜ì •ëœ ë©”íƒ€ë°ì´í„°ë¡œ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                    doc.metadata = metadata
                    new_documents.append(doc)
                    
            except Exception as e:
                print(f"PDF ë¡œë“œ ì‹¤íŒ¨: {pdf_path}, ì˜¤ë¥˜: {e}")
                continue

        if new_documents:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            new_splits = text_splitter.split_documents(new_documents)

            # ğŸ”§ Milvus ì—°ê²° ë° ì»¬ë ‰ì…˜ ê´€ë¦¬
            host = self.milvus_conf["host"]
            port = self.milvus_conf["port"]
            collection = self.milvus_conf["collection"]
            
            # ì—°ê²° ì •ë¦¬ í›„ ì¬ì ‘ì†
            if "default" in connections.list_connections():
                connections.disconnect(alias="default")
            connections.connect(alias="default", host=host, port=port)

            if self.vectorstore:
                # ê¸°ì¡´ Milvus ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
                self.vectorstore.add_documents(new_splits)
                print(f"âœ… ê¸°ì¡´ Milvus ë²¡í„°ìŠ¤í† ì–´ì— {len(new_splits)}ê°œ ì²­í¬ ì¶”ê°€")
            else:
                # ğŸ”§ ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆê³  drop_existingì´ Trueì¸ ê²½ìš° ì‚­ì œ
                if self.milvus_conf["drop_existing"] and utility.has_collection(collection):
                    print(f"âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘: {collection}")
                    utility.drop_collection(collection)
                
                # ìƒˆë¡œ ìƒì„±
                index_params = {"index_type": "AUTOINDEX", "metric_type": "IP"}
                search_params = {"metric_type": "IP"}
                
                # ğŸ”§ Milvus ìŠ¤í‚¤ë§ˆ ì„¤ì • ì¶”ê°€
                schema_config = {
                    "auto_id": True,
                    "enable_dynamic_field": True,  # ë™ì  í•„ë“œ í—ˆìš©
                }
                
                self.vectorstore = Milvus.from_documents(
                    documents=new_splits,
                    embedding=self.embeddings_model,
                    collection_name=collection,
                    connection_args={"host": host, "port": port},
                    index_params=index_params,
                    search_params=search_params,
                    **schema_config
                )
                print(f"âœ… ìƒˆ Milvus ë²¡í„°ìŠ¤í† ì–´ ìƒì„±: {len(new_splits)}ê°œ ì²­í¬")

            # retriever ì—…ë°ì´íŠ¸
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.milvus_conf["topk"]})
            
            # íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸
            self.files_in_vectorstore = pdf_files
            
            return True
        
        return False
    
    def retrieve_documents(self, query: str, subject_area: str = "", weakness_concepts: List[str] = None) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            subject_area: ê³¼ëª© ì˜ì—­
            weakness_concepts: ì·¨ì•½ì  ê°œë… ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ì™€ ì‚¬ìš©ëœ ì†ŒìŠ¤ ì •ë³´
        """
        try:
            if not self.retriever:
                return {"error": "ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            # ì·¨ì•½ì  ê°œë…ì´ ìˆëŠ” ê²½ìš° ì¿¼ë¦¬ ê°•í™”
            if weakness_concepts:
                enhanced_query = f"{subject_area} {' '.join(weakness_concepts)} {query}"
            else:
                enhanced_query = f"{subject_area} {query}"
            
            # ë¬¸ì„œ ê²€ìƒ‰
            documents = self.retriever.invoke(enhanced_query)
            
            # ì‚¬ìš©ëœ ì†ŒìŠ¤ íŒŒì¼ ë¶„ì„ (Milvus ë¬¸ì„œì—ëŠ” source_fileì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì™„)
            source_files = []
            for doc in documents:
                src = doc.metadata.get('source_file') or doc.metadata.get('subject') or 'milvus'
                source_files.append(src)
            used_sources = list(Counter(source_files).keys())
            
            return {
                "documents": documents,
                "used_sources": used_sources,
                "query": enhanced_query
            }
            
        except Exception as e:
            return {"error": f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}
    
    def prepare_context(self, documents: List[Document], weakness_concepts: List[str] = None) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        
        Args:
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            weakness_concepts: ì·¨ì•½ì  ê°œë… ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¤€ë¹„ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not documents:
            print("[DEBUG] prepare_context: no documents provided")
            return ""
        
        print(f"[DEBUG] prepare_context: processing {len(documents)} documents")
        
        # ì·¨ì•½ì  ê°œë…ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ ë³„
        key_sents = []
        weakness_related_sents = []
        
        for i, doc in enumerate(documents):
            print(f"[DEBUG] Document {i+1}: content length={len(doc.page_content)}")
            lines = doc.page_content.split("\n")
            print(f"[DEBUG] Document {i+1}: {len(lines)} lines")
            
            for line_num, line in enumerate(lines):
                line = line.strip()
                if len(line) > 30:  # ìµœì†Œ ê¸¸ì´ë¥¼ 30ìœ¼ë¡œ ì¤„ì„
                    # ì·¨ì•½ì  ê°œë…ê³¼ ê´€ë ¨ëœ ë‚´ìš© ìš°ì„  ì„ ë³„
                    is_weakness_related = False
                    if weakness_concepts:
                        is_weakness_related = any(
                            concept.lower() in line.lower() 
                            for concept in weakness_concepts
                        )
                    
                    # ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥
                    is_important = any(k in line for k in [
                        "ì •ì˜", "íŠ¹ì§•", "ì¢…ë¥˜", "ì˜ˆì‹œ", "ì›ë¦¬", 
                        "êµ¬ì„±", "ì ˆì°¨", "ì¥ì ", "ë‹¨ì ", "ë°©ë²•", "ê¸°ëŠ¥",
                        "ìë£Œêµ¬ì¡°", "ìŠ¤íƒ", "í", "ë¦¬ìŠ¤íŠ¸", "íŠ¸ë¦¬", "ê·¸ë˜í”„"
                    ])
                    
                    if is_weakness_related:
                        weakness_related_sents.append(line)
                        print(f"[DEBUG] Weakness related line: {line[:100]}...")
                    elif is_important or len(line) > 80:  # ê¸¸ì´ ê¸°ì¤€ì„ 80ìœ¼ë¡œ ì¤„ì„
                        key_sents.append(line)
                        print(f"[DEBUG] Important line: {line[:100]}...")
        
        print(f"[DEBUG] Found {len(weakness_related_sents)} weakness-related sentences")
        print(f"[DEBUG] Found {len(key_sents)} important sentences")
        
        # ì·¨ì•½ì  ê´€ë ¨ ë‚´ìš©ì„ ì•ìª½ì—, ì¼ë°˜ ë‚´ìš©ì„ ë’¤ìª½ì— ë°°ì¹˜
        all_sents = weakness_related_sents + key_sents
        print(f"[DEBUG] Total sentences: {len(all_sents)}")
        
        # ìµœëŒ€ 30ê°œ ë¬¸ì¥ìœ¼ë¡œ ì œí•œí•˜ê³  ìµœì†Œ 1ê°œëŠ” ë³´ì¥
        context_sents = all_sents[:30] if all_sents else []
        if not context_sents and documents:
            # ì•„ë¬´ê²ƒë„ ì„ íƒë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì²« ëª‡ ì¤„ì„ ì‚¬ìš©
            first_doc = documents[0]
            lines = first_doc.page_content.split("\n")
            context_sents = [line.strip() for line in lines[:5] if len(line.strip()) > 20]
            print(f"[DEBUG] Using fallback context: {len(context_sents)} lines from first document")
        
        context = "\n".join(context_sents)
        print(f"[DEBUG] Final context length: {len(context)} characters")
        
        return context
    
    def get_vectorstore_info(self) -> Dict[str, Any]:
        """
        ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë°˜í™˜
        
        Returns:
            ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ ì •ë³´
        """
        return {
            "is_initialized": self.vectorstore is not None,
            "total_files": len(self.files_in_vectorstore),
            "files": self.files_in_vectorstore.copy(),
            "embeddings_model": "jhgan/ko-sroberta-multitask" if self.embeddings_model else None,
            "vectorstore_type": "Milvus",
            "milvus_config": self.milvus_conf
        }
    
    def clear_vectorstore(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        self.vectorstore = None
        self.retriever = None
        self.files_in_vectorstore = []
    
    def update_data_folder(self, new_data_folder: str):
        """
        ë°ì´í„° í´ë” ê²½ë¡œ ë³€ê²½
        
        Args:
            new_data_folder: ìƒˆë¡œìš´ ë°ì´í„° í´ë” ê²½ë¡œ
        """
        self.data_folder = new_data_folder
        os.makedirs(self.data_folder, exist_ok=True)
        self.clear_vectorstore()  # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
