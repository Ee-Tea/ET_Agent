#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGAS + Milvus í†µí•© í…ŒìŠ¤íŠ¸ íŒŒì¼
ì‹¤ì œ Milvus ë°ì´í„°ë¡œ ë¬¸ì œ ìƒì„± í›„ RAGASë¡œ í’ˆì§ˆ ê²€ì¦
"""

import os
import sys
import json
from typing import Dict, List, Any
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# RAGAS í™˜ê²½ ë³€ìˆ˜ ëª…ì‹œì  ì„¤ì • (OpenAI APIìš©)
os.environ.setdefault("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
os.environ.setdefault("OPENAI_BASE_URL", "https://api.openai.com/v1")
os.environ.setdefault("OPENAI_LLM_MODEL", "gpt-3.5-turbo")

def test_ragas_installation():
    """RAGAS ì„¤ì¹˜ ìƒíƒœ í™•ì¸"""
    try:
        import ragas
        print(f"âœ… RAGAS ì„¤ì¹˜ë¨: {ragas.__version__}")
        return True
    except ImportError:
        print("âŒ RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: uv pip install ragas")
        return False

def test_milvus_connection():
    """Milvus ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        from pymilvus import connections, utility
        from langchain_milvus import Milvus
        from langchain_huggingface import HuggingFaceEmbeddings
        
        # Milvus ì—°ê²° ì„¤ì •
        host = os.getenv("MILVUS_HOST", "localhost")
        port = os.getenv("MILVUS_PORT", "19530")
        collection_name = "info_exam_chunks"  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ë ‰ì…˜ëª…ìœ¼ë¡œ ìˆ˜ì •
        
        print(f"ğŸ”— Milvus ì—°ê²° ì‹œë„: {host}:{port}")
        
        # ì—°ê²°
        if "default" in connections.list_connections():
            connections.disconnect(alias="default")
        connections.connect(alias="default", host=host, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        if not utility.has_collection(collection_name):
            print(f"âŒ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ëª©ë¡ ì¶œë ¥
            available_collections = utility.list_collections()
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {available_collections}")
            return None, None
        
        print(f"âœ… Milvus ì—°ê²° ì„±ê³µ: {collection_name}")
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        from pymilvus import Collection
        collection = Collection(collection_name)
        print(f"ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´:")
        print(f"  - ì—”í‹°í‹° ìˆ˜: {collection.num_entities}")
        print(f"  - ìŠ¤í‚¤ë§ˆ: {collection.schema}")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        # Milvus ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Milvus(
            embedding_function=embeddings_model,
            collection_name=collection_name,
            connection_args={"host": host, "port": port},
            index_params={"index_type": "AUTOINDEX", "metric_type": "IP"},
            search_params={"metric_type": "IP"},
        )
        
        return vectorstore, embeddings_model
        
    except Exception as e:
        print(f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_simple_search(vectorstore):
    """ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        print("\nğŸ” ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        
        # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ"
        documents = vectorstore.similarity_search(query, k=3)
        
        if documents:
            print(f"âœ… ê²€ìƒ‰ ì„±ê³µ! {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
            for i, doc in enumerate(documents[:2]):  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                print(f"  ë¬¸ì„œ {i+1}: {doc.page_content[:100]}...")
                print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")
                print()
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def generate_questions_from_milvus(vectorstore, subject_area: str, num_questions: int = 3):
    """Milvusì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë¬¸ì œ ìƒì„±"""
    try:
        from langchain_openai import ChatOpenAI
        from langchain.prompts import PromptTemplate
        
        # LLM ì´ˆê¸°í™” (OpenAI API ì‚¬ìš©)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return [], ""
        
        try:
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.2,
                max_tokens=1024,
                base_url="https://api.openai.com/v1",
                api_key=openai_api_key
            )
            print("âœ… OpenAI API LLM ì´ˆê¸°í™” ì™„ë£Œ (GPT-3.5-turbo)")
        except Exception as e:
            print(f"âŒ OpenAI API LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return [], ""
        
        print(f"ğŸš€ {subject_area} ê³¼ëª© ë¬¸ì œ ìƒì„± ì‹œì‘...")
        
        # Milvusì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        query = f"{subject_area} ê´€ë ¨ ë¬¸ì œ"
        documents = vectorstore.similarity_search(query, k=5)
        
        if not documents:
            print(f"âŒ {subject_area} ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], ""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(documents):
            context_parts.append(f"ë¬¸ì„œ {i+1}: {doc.page_content}")
            if doc.metadata.get('options'):
                context_parts.append(f"ë³´ê¸°: {doc.metadata['options']}")
            if doc.metadata.get('answer'):
                context_parts.append(f"ì •ë‹µ: {doc.metadata['answer']}")
            if doc.metadata.get('explanation'):
                context_parts.append(f"í•´ì„¤: {doc.metadata['explanation']}")
        
        context = "\n\n".join(context_parts)
        print(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(context)}ì")
        
        # ë¬¸ì œ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt_template = PromptTemplate(
            input_variables=["context", "subject_area", "num_questions"],
            template=(
                "ë‹¹ì‹ ì€ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {subject_area} ê³¼ëª©ì˜ ê°ê´€ì‹ ë¬¸ì œ {num_questions}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n\n"
                "ì¡°ê±´:\n"
                "â€¢ ë³´ê¸°ì—ëŠ” ë²ˆí˜¸(1. 2. 3. 4.)ë¥¼ ë¶™ì´ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "â€¢ answerì—ëŠ” ì •ë‹µì˜ 'ë²ˆí˜¸'ë§Œ ë¬¸ìì—´ë¡œ ì ìœ¼ì‹­ì‹œì˜¤. ì˜ˆ: \"2\"\n"
                "â€¢ ì¶œë ¥ì€ ì•„ë˜ JSON í˜•ì‹ë§Œ í¬í•¨í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.\n\n"
                "[ë¬¸ì„œ ë‚´ìš©]\n{context}\n\n"
                "[ì‘ë‹µ í˜•ì‹]\n"
                "{{\n"
                "  \"questions\": [\n"
                "    {{\n"
                "      \"question\": \"ë¬¸ì œ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±\",\n"
                "      \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", \"ì„ íƒì§€3\", \"ì„ íƒì§€4\"],\n"
                "      \"answer\": \"1\",\n"
                "      \"explanation\": \"ì •ë‹µì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ì„¤\"\n"
                "    }}\n"
                "  ]\n"
                "}}\n"
            )
        )
        
        prompt = prompt_template.format(
            context=context[:3000],  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            subject_area=subject_area,
            num_questions=num_questions
        )
        
        # LLM í˜¸ì¶œ
        print("ğŸ¤– LLMìœ¼ë¡œ ë¬¸ì œ ìƒì„± ì¤‘...")
        response = llm.invoke(prompt)
        response_content = getattr(response, "content", str(response))
        
        # JSON íŒŒì‹±
        questions = parse_quiz_response(response_content, subject_area)
        print(f"âœ… {len(questions)}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ")
        
        return questions, context
        
    except Exception as e:
        print(f"âŒ ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return [], ""

def parse_quiz_response(response: str, subject_area: str = "") -> List[Dict[str, Any]]:
    """LLM ì‘ë‹µì—ì„œ ë¬¸ì œ íŒŒì‹±"""
    import re
    
    try:
        # JSON ë¸”ë¡ ì°¾ê¸°
        json_block_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_block_match:
            json_str = json_block_match.group(1)
        else:
            # ì¼ë°˜ JSON ê°ì²´ ì°¾ê¸°
            json_str_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response.strip(), re.DOTALL)
            if not json_str_match:
                print("âŒ JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
                return []
            json_str = json_str_match.group(0)
        
        # JSON íŒŒì‹±
        data = json.loads(json_str)
        if "questions" not in data or not isinstance(data["questions"], list):
            print("âŒ questions í•„ë“œê°€ ì—†ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return []
        
        questions = data.get("questions", [])
        
        # ê° ë¬¸ì œì— ê³¼ëª© ì •ë³´ ì¶”ê°€
        for question in questions:
            if "subject" not in question:
                question["subject"] = subject_area
        
        return questions
        
    except Exception as e:
        print(f"âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
        return []

def validate_questions_with_ragas(questions: List[Dict[str, Any]], context: str):
    """RAGASë¥¼ ì‚¬ìš©í•˜ì—¬ RAG í’ˆì§ˆ ê²€ì¦"""
    try:
        from ragas import evaluate
        from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
        from datasets import Dataset
        from ragas.llms import llm_factory
        
        if not questions:
            print("âŒ ê²€ì¦í•  ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nğŸ” RAGASë¡œ {len(questions)}ê°œ ë¬¸ì œ RAG í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        # RAGAS LLM ì„¤ì • (OpenAI API ì‚¬ìš©)
        try:
            # RAGAS LLM ì„¤ì • (OpenAI API ì‚¬ìš©)
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            # RAGASì—ì„œ OpenAI API ì‚¬ìš©ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ["OPENAI_API_KEY"] = openai_api_key
            os.environ["OPENAI_BASE_URL"] = "https://api.openai.com/v1"
            
            llm = llm_factory(
                model="gpt-3.5-turbo",
                base_url="https://api.openai.com/v1"
            )
            print("âœ… RAGAS LLM ì„¤ì • ì™„ë£Œ (OpenAI GPT-3.5-turbo)")
                
        except Exception as llm_error:
            print(f"âš ï¸ RAGAS LLM ì„¤ì • ì‹¤íŒ¨: {llm_error}")
            return None
        
        # RAGAS í‰ê°€ ë°ì´í„° êµ¬ì„±
        eval_data = {
            "question": [],
            "contexts": [],
            "answer": [],
            "ground_truth": []
        }
        
        for q in questions:
            question_text = q.get("question", "")[:200]
            answer_text = q.get("answer", "") + ": " + q.get("explanation", "")[:100]
            
            eval_data["question"].append(question_text)
            eval_data["contexts"].append([context[:500]])
            eval_data["answer"].append(answer_text)
            eval_data["ground_truth"].append(answer_text)
        
        dataset = Dataset.from_dict(eval_data)
        
        # RAGAS í‰ê°€ (RAG í’ˆì§ˆ ë©”íŠ¸ë¦­)
        print("ğŸ“Š RAGAS RAG í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        results = evaluate(
            dataset,
            metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
            llm=llm
        )
        
        return results
        
    except Exception as e:
        print(f"âŒ RAGAS ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def display_ragas_results(results, questions: List[Dict[str, Any]]):
    """RAGAS ê²°ê³¼ í‘œì‹œ (êµ¬ì¡° ì ìˆ˜ ì œê±°)"""
    if not results:
        return
    
    print("\nğŸ“Š RAGAS RAG í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    
    try:
        # RAGAS ê²°ê³¼ í‘œì‹œ
        if hasattr(results, '_scores_dict'):
            scores_dict = results._scores_dict
            
            faithfulness_scores = scores_dict.get('faithfulness', [])
            answer_relevancy_scores = scores_dict.get('answer_relevancy', [])
            context_precision_scores = scores_dict.get('context_precision', [])
            context_recall_scores = scores_dict.get('context_recall', [])
            
            print(f"Faithfulness: {faithfulness_scores}")
            print(f"Answer Relevancy: {answer_relevancy_scores}")
            print(f"Context Precision: {context_precision_scores}")
            print(f"Context Recall: {context_recall_scores}")
            
            # RAGAS í‰ê·  ì ìˆ˜
            all_ragas_scores = []
            for metric_scores in scores_dict.values():
                if isinstance(metric_scores, list):
                    valid_scores = [s for s in metric_scores if s is not None and not (isinstance(s, float) and str(s) == 'nan')]
                    all_ragas_scores.extend(valid_scores)
            
            if all_ragas_scores:
                avg_ragas_score = sum(all_ragas_scores) / len(all_ragas_scores)
                print(f"RAGAS í‰ê·  ì ìˆ˜: {avg_ragas_score:.4f}")
        
    except Exception as e:
        print(f"âŒ RAGAS ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ì›ë³¸ ê²°ê³¼: {results}")
    
    # ê°œë³„ ë¬¸ì œ í‘œì‹œ (êµ¬ì¡° ì ìˆ˜ ì—†ì´)
    print(f"\nğŸ“‹ ìƒì„±ëœ ë¬¸ì œ ({len(questions)}ê°œ)")
    print("-" * 60)
    
    for i, question in enumerate(questions, 1):
        question_text = question.get("question", "")
        options = question.get("options", [])
        answer = question.get("answer", "")
        explanation = question.get("explanation", "")
        
        print(f"ë¬¸ì œ {i}: {question_text}")
        
        # ë³´ê¸° í‘œì‹œ
        if options:
            for j, option in enumerate(options, 1):
                print(f"        {j}. {option}")
        print(f"ì •ë‹µ: {answer}")
        print(f"í•´ì„¤: {explanation}")
        print()

def visualize_ragas_results(all_results: Dict[str, Any]):
    """RAGAS ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np
        import pandas as pd
        
        # ì˜ì–´ í°íŠ¸ ì„¤ì • (í•œê¸€ í°íŠ¸ ë¬¸ì œ í•´ê²°)
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        print("\nğŸ“Š RAGAS Results Visualization Started...")
        
        # ë°ì´í„° ì¤€ë¹„
        subjects = []
        metrics_data = {
            'Faithfulness': [],
            'Answer Relevancy': [],
            'Context Precision': [],
            'Context Recall': []
        }
        
        for subject, result in all_results.items():
            if result["ragas_results"] and hasattr(result["ragas_results"], '_scores_dict'):
                subjects.append(subject)
                scores_dict = result["ragas_results"]._scores_dict
                
                # ê° ë©”íŠ¸ë¦­ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
                for metric_name in metrics_data.keys():
                    metric_key = metric_name.lower().replace(' ', '_')
                    scores = scores_dict.get(metric_key, [])
                    if scores:
                        # nan ê°’ ì œê±°í•˜ê³  í‰ê·  ê³„ì‚°
                        valid_scores = [s for s in scores if s is not None and not (isinstance(s, float) and str(s) == 'nan')]
                        if valid_scores:
                            metrics_data[metric_name].append(np.mean(valid_scores))
                        else:
                            metrics_data[metric_name].append(0.0)
                    else:
                        metrics_data[metric_name].append(0.0)
        
        if not subjects:
            print("âŒ No data to visualize.")
            return
        
        # 1. ë©”íŠ¸ë¦­ë³„ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('RAGAS Quality Assessment Results Visualization', fontsize=16, fontweight='bold')
        
        for i, (metric_name, scores) in enumerate(metrics_data.items()):
            row, col = i // 2, i % 2
            ax = axes[row, col]
            
            bars = ax.bar(subjects, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
            ax.set_title(f'{metric_name} Score', fontweight='bold')
            ax.set_ylabel('Score')
            ax.set_ylim(0, 1.0)
            
            # ë§‰ëŒ€ ìœ„ì— ì ìˆ˜ í‘œì‹œ
            for bar, score in zip(bars, scores):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # xì¶• ë ˆì´ë¸” íšŒì „
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('ragas_metrics_comparison.png', dpi=300, bbox_inches='tight')
        print("ğŸ’¾ Metrics comparison graph saved: ragas_metrics_comparison.png")
        
        # 2. ê³¼ëª©ë³„ ì¢…í•© ì ìˆ˜ ë ˆì´ë” ì°¨íŠ¸
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        # ê° ê³¼ëª©ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
        subject_scores = []
        for i, subject in enumerate(subjects):
            subject_avg = np.mean([metrics_data[metric][i] for metric in metrics_data.keys()])
            subject_scores.append(subject_avg)
        
        # ë ˆì´ë” ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        angles = np.linspace(0, 2 * np.pi, len(subjects), endpoint=False).tolist()
        angles += angles[:1]  # ì²« ë²ˆì§¸ ì ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€í•˜ì—¬ ë‹«íŒ ë„í˜• ë§Œë“¤ê¸°
        subject_scores += subject_scores[:1]
        
        ax.plot(angles, subject_scores, 'o-', linewidth=2, label='Subject Average Score')
        ax.fill(angles, subject_scores, alpha=0.25)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(subjects)
        ax.set_ylim(0, 1.0)
        ax.set_title('Subject-wise RAGAS Overall Score (Radar Chart)', fontweight='bold', pad=20)
        
        # ê·¸ë¦¬ë“œ ì¶”ê°€
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig('ragas_subject_radar.png', dpi=300, bbox_inches='tight')
        print("ğŸ’¾ Subject radar chart saved: ragas_subject_radar.png")
        
        # 3. íˆíŠ¸ë§µ
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(metrics_data, index=subjects)
        
        plt.figure(figsize=(10, 6))
        sns.heatmap(df, annot=True, cmap='RdYlGn', vmin=0, vmax=1, 
                   fmt='.3f', cbar_kws={'label': 'Score'})
        plt.title('RAGAS Metrics Heatmap', fontweight='bold', pad=20)
        plt.xlabel('Metrics')
        plt.ylabel('Subjects')
        plt.tight_layout()
        plt.savefig('ragas_heatmap.png', dpi=300, bbox_inches='tight')
        print("ğŸ’¾ Heatmap saved: ragas_heatmap.png")
        
        # 4. ìš”ì•½ í†µê³„ í…Œì´ë¸”
        print("\nğŸ“ˆ RAGAS Results Summary Statistics:")
        print("=" * 80)
        
        summary_df = df.copy()
        summary_df['Average'] = summary_df.mean(axis=1)
        summary_df['Std Dev'] = summary_df.std(axis=1)
        
        print(summary_df.round(4))
        
        # ì „ì²´ í‰ê·  ì ìˆ˜
        overall_avg = summary_df['Average'].mean()
        print(f"\nğŸ¯ Overall Average Score: {overall_avg:.4f}")
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê· 
        print("\nğŸ“Š Metrics Average Score:")
        for metric in metrics_data.keys():
            metric_avg = df[metric].mean()
            print(f"  {metric}: {metric_avg:.4f}")
        
        plt.show()
        print("\nâœ… Visualization completed!")
        
    except ImportError as e:
        print(f"âŒ Visualization library installation required: {e}")
        print("Install command: uv pip install matplotlib seaborn pandas")
    except Exception as e:
        print(f"âŒ Visualization failed: {e}")
        import traceback
        traceback.print_exc()

def save_results_to_file(questions: List[Dict[str, Any]], ragas_results, context: str, subject_area: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        import datetime
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ragas_test_{subject_area}_{timestamp}.json"
        
        # RAGAS ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        ragas_data = {}
        if ragas_results and hasattr(ragas_results, '_scores_dict'):
            scores_dict = ragas_results._scores_dict
            ragas_data = {
                "faithfulness": scores_dict.get('faithfulness', []),
                "answer_relevancy": scores_dict.get('answer_relevancy', []),
                "context_precision": scores_dict.get('context_precision', []),
                "context_recall": scores_dict.get('context_recall', [])
            }
        
        result_data = {
            "test_info": {
                "subject_area": subject_area,
                "timestamp": timestamp,
                "total_questions": len(questions)
            },
            "context": context,
            "questions": questions,
            "ragas_results": ragas_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§ª RAGAS + Milvus í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. RAGAS ì„¤ì¹˜ í™•ì¸
    if not test_ragas_installation():
        return
    
    # 2. Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
    vectorstore, embeddings_model = test_milvus_connection()
    if not vectorstore:
        print("âŒ Milvus ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 3. ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    if not test_simple_search(vectorstore):
        print("âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 4. í…ŒìŠ¤íŠ¸í•  ê³¼ëª©ë“¤
    test_subjects = ["Software Design", "Database Construction", "Programming Language Utilization"]
    
    all_results = {}
    
    for subject in test_subjects:
        print(f"\n{'='*20} {subject} ê³¼ëª© í…ŒìŠ¤íŠ¸ {'='*20}")
        
        # ë¬¸ì œ ìƒì„±
        questions, context = generate_questions_from_milvus(vectorstore, subject, num_questions=3)
        
        if not questions:
            print(f"âŒ {subject} ê³¼ëª© ë¬¸ì œ ìƒì„± ì‹¤íŒ¨")
            continue
        
        # RAGAS í’ˆì§ˆ ê²€ì¦
        ragas_results = validate_questions_with_ragas(questions, context)
        
        # ê²°ê³¼ í‘œì‹œ
        display_ragas_results(ragas_results, questions)
        
        # ê²°ê³¼ ì €ì¥
        filename = save_results_to_file(questions, ragas_results, context, subject)
        
        all_results[subject] = {
            "questions": questions,
            "ragas_results": ragas_results,
            "filename": filename
        }
    
    # 5. ì „ì²´ ìš”ì•½
    print(f"\n{'='*20} ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½ {'='*20}")
    for subject, result in all_results.items():
        if result["ragas_results"]:
            # RAGAS ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ
            ragas_result = result["ragas_results"]
            
            # _scores_dictì—ì„œ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            if hasattr(ragas_result, '_scores_dict'):
                scores_dict = ragas_result._scores_dict
                print(f"\nğŸ“Š {subject} RAGAS í‰ê°€ ê²°ê³¼:")
                
                # ê° ë©”íŠ¸ë¦­ì˜ ì ìˆ˜ í™•ì¸
                faithfulness_scores = scores_dict.get('faithfulness', [])
                answer_relevancy_scores = scores_dict.get('answer_relevancy', [])
                context_precision_scores = scores_dict.get('context_precision', [])
                context_recall_scores = scores_dict.get('context_recall', [])
                
                print(f"Faithfulness: {faithfulness_scores}")
                print(f"Answer Relevancy: {answer_relevancy_scores}")
                print(f"Context Precision: {context_precision_scores}")
                print(f"Context Recall: {context_recall_scores}")
                
                # í‰ê·  ì ìˆ˜ ê³„ì‚° (nan ì œì™¸)
                all_scores = []
                for metric_scores in scores_dict.values():
                    if isinstance(metric_scores, list):
                        valid_scores = [s for s in metric_scores if s is not None and not (isinstance(s, float) and str(s) == 'nan')]
                        all_scores.extend(valid_scores)
                
                if all_scores:
                    avg_score = sum(all_scores) / len(all_scores)
                    print(f"í‰ê·  ì ìˆ˜: {avg_score:.4f}")
                    print(f"ë¬¸ì œ ìˆ˜: {len(result['questions'])}ê°œ")
        else:
            print(f"{subject}: ê²€ì¦ ì‹¤íŒ¨")
    
    # 6. ì‹œê°í™”
    if all_results:
        visualize_ragas_results(all_results)
    
    print("\nâœ… RAGAS + Milvus í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
