import os, re, json, glob
from typing import List, Dict, Any
from dataclasses import dataclass
import pandas as pd

from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall

from teacher.agents.solution.solution_agent import SolutionAgent

from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings

# ğŸ”§ Milvus ì—°ê²°/ìŠ¤í‚¤ë§ˆ í™•ì¸ìš©
from pymilvus import connections, Collection, DataType
from langchain_milvus import Milvus

# --- run_eval() ë§ˆì§€ë§‰ì— ì´ì–´ ë¶™ì´ì„¸ìš” ---
import numpy as np
import matplotlib.pyplot as plt
import os

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GROQAI_API_KEY = os.getenv("GROQAI_API_KEY", "")
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))


# === RAGAS í‰ê°€ì— ì‚¬ìš©í•  LLM/ì„ë² ë”© ëª…ì‹œ (í•„ìˆ˜) ===
# llm = ChatOpenAI(
#     model="gpt-4o-mini",
#     temperature=0,
#     api_key=os.environ["OPENAI_API_KEY"],
# )
llm = ChatOpenAI(
    api_key=GROQAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    model=OPENAI_LLM_MODEL,
    temperature=LLM_TEMPERATURE,
    max_tokens=min(LLM_MAX_TOKENS, 2048),
)

emb = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={"device": "cpu"}
)

# --------- Milvus ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ìœ í‹¸ ---------
def connect_vectorstore(
    collection_name: str,
    host: str = None,
    port: str = None,
    model_name: str = "jhgan/ko-sroberta-multitask",
) -> Milvus:
    """
    ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆë¥¼ ì½ì–´ text/vector í•„ë“œëª…ì„ ìë™ ì¶”ë¡ í•œ ë’¤ LangChain Milvus ë˜í¼ë¡œ ì—°ê²°.
    (problems=vector/text, concept_summary=embedding/content ê°™ì€ ì´ì§ˆ ìŠ¤í‚¤ë§ˆ ëŒ€ì‘)
    """
    host = host or os.getenv("MILVUS_HOST", "localhost")
    port = port or os.getenv("MILVUS_PORT", "19530")

    # ë™ê¸° HTTP ìŠ¤í‚´ìœ¼ë¡œ ì—°ê²° (Streamlit ì•„ë‹˜ â†’ async ë£¨í”„ ì´ìŠˆ íšŒí”¼)
    if "default" not in connections.list_connections():
        connections.connect(alias="default", uri=f"http://{host}:{port}")

    # ìŠ¤í‚¤ë§ˆ íƒìƒ‰
    c = Collection(collection_name)
    vec_field = None
    text_field = None

    for f in c.schema.fields:
        if f.dtype == DataType.FLOAT_VECTOR and vec_field is None:
            vec_field = f.name

    varchar_fields = [f.name for f in c.schema.fields if f.dtype == DataType.VARCHAR]
    for cand in ("text", "page_content", "content", "question", "item_title", "title"):
        if cand in varchar_fields:
            text_field = cand
            break
    if text_field is None and varchar_fields:
        text_field = varchar_fields[0]

    if vec_field is None:
        raise RuntimeError(f"[Milvus] '{collection_name}'ì— FLOAT_VECTOR í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # Embeddings
    _emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={"device": "cpu"})

    # LangChain Milvus VectorStore (ê²€ìƒ‰ë§Œ í•  ê²ƒì´ë¯€ë¡œ index/search_paramsëŠ” ìƒëµ)
    vs = Milvus(
        embedding_function=_emb,
        collection_name=collection_name,
        connection_args={"uri": f"http://{host}:{port}"},
        text_field=text_field,
        vector_field=vec_field,
    )
    print(f"âœ… Milvus connected: {collection_name} (text_field={text_field}, vector_field={vec_field})")
    return vs

# ---------- ìœ í‹¸ ----------
def _ctx_text(d) -> str:
    md = getattr(d, "metadata", {}) or {}
    # concept_summaryëŠ” contentì—, problemsëŠ” page_contentì— ìˆì„ ìˆ˜ ìˆìŒ
    txt = (md.get("content") or getattr(d, "page_content", "") or "")
    return str(txt).strip()

def build_contexts(retrieved_problems, retrieved_concepts, *, min_len: int = 20, max_k: int = 10) -> List[str]:
    raw = [ _ctx_text(d) for d in (retrieved_problems or []) + (retrieved_concepts or []) ]
    # ê³µë°± ì œê±° â†’ ì¤‘ë³µ ì œê±° â†’ ë„ˆë¬´ ì§§ì€ ì¡°ê° ì œê±° â†’ ìƒí•œ clip
    cleaned = [s for s in (t.strip() for t in raw) if s]
    deduped = list(dict.fromkeys(cleaned))
    filtered = [s for s in deduped if len(s) >= min_len]
    return filtered[:max_k]

def build_question_with_options(question_text: str, options: List[str]) -> str:
    lines = ["[ë¬¸ì œ]", question_text, "", "[ë³´ê¸°]"]
    for i, opt in enumerate(options, start=1):
        lines.append(f"{i}) {opt}")
    return "\n".join(lines)

def doc_to_ctx_text(d) -> str:
    meta = getattr(d, "metadata", {}) or {}
    opts = meta.get("options", [])
    if isinstance(opts, str):
        try:
            opts = json.loads(opts)
        except Exception:
            opts = [opts]
    opts_blob = "\n".join(f"{i+1}) {o}" for i, o in enumerate(opts)) if opts else ""
    base = str(getattr(d, "page_content", "")) or ""
    if opts_blob:
        return (base + "\n\n[ë³´ê¸°]\n" + opts_blob)[:2000]
    return base[:2000]

def parse_idx_from_text(s: str) -> int | None:
    """
    ëª¨ë¸ ì¶œë ¥ì—ì„œ 'ì •ë‹µ: 1' ë˜ëŠ” '(1)' ë˜ëŠ” '1ë²ˆ' ë“± ìˆ«ì í›„ë³´ë¥¼ robustí•˜ê²Œ ì¶”ì¶œ
    """
    if not s:
        return None
    m = re.search(r"(?:ì •ë‹µ[:\s]*|^|\s|\[|\()(\d{1,2})(?:\)|\]|\s|ë²ˆ|\.|,|$)", s)
    try:
        if m:
            val = int(m.group(1))
            return val
    except Exception:
        pass
    return None

from dataclasses import dataclass
@dataclass
class EvalRow:
    question: str
    contexts: List[str]
    answer: str
    ground_truth: str
    metadata: Dict[str, Any]

# ---------- í•µì‹¬: í…ŒìŠ¤íŠ¸ JSON -> ì‹¤í–‰ -> RAGAS ì…ë ¥ ----------
def run_eval(
    test_json_path: str,
    out_dir: str = "./eval_out",
    vectorstore_p: Milvus = None,
    vectorstore_c: Milvus = None,
):
    os.makedirs(out_dir, exist_ok=True)
    print(f"[ë¡œë“œ] JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°: {test_json_path}")

    # âœ… ìµœìƒìœ„ì— questions í‚¤ê°€ ìˆëŠ” í˜•ì‹ ëŒ€ì‘
    with open(test_json_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    if isinstance(raw, dict) and "questions" in raw and isinstance(raw["questions"], list):
        items = raw["questions"]
        print(f"[ì •ê·œí™”] 'questions' í‚¤ ë°œê²¬ â†’ í•­ëª© {len(items)}ê°œ")
    elif isinstance(raw, list):
        items = raw
        print(f"[ì •ê·œí™”] ìµœìƒìœ„ê°€ list â†’ í•­ëª© {len(items)}ê°œ")
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON êµ¬ì¡°ì…ë‹ˆë‹¤: top-level={type(raw).__name__}")

    print(f"[ë¡œë“œ ì™„ë£Œ] ë¬¸ì œ ìˆ˜: {len(items)}")

    agent = SolutionAgent()
    print("[ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ]")

    rows: List[EvalRow] = []
    golden_rows: List[Dict[str, Any]] = []

    for i, it in enumerate(items, start=1):
        if not isinstance(it, dict):
            print(f"[ê²½ê³ ] {i}ë²ˆì§¸ í•­ëª©ì´ dictê°€ ì•„ë‹˜ â†’ ê±´ë„ˆëœ€ (type={type(it).__name__})")
            continue

        print(f"\n[{i}] ë¬¸ì œ ì²˜ë¦¬ ì‹œì‘")
        q_text     = (it.get("question") or "").strip()
        options    = it.get("options") or []
        gt_idx_str = (it.get("answer") or "").strip()
        gt_idx     = int(gt_idx_str) if gt_idx_str.isdigit() else None
        gt_text    = options[gt_idx - 1] if (gt_idx and 1 <= gt_idx <= len(options)) else ""
        gt_exp     = (it.get("explanation") or "").strip()
        gt_sub     = (it.get("subject") or "").strip()

        user_input_txt = "ì´ ë¬¸ì œì˜ ì •ë‹µì´ ë˜ëŠ” ë³´ê¸° ë²ˆí™”ì™€ ë¬¸ì œ í’€ì´, ê·¸ë¦¬ê³  ì´ ë¬¸ì œì˜ ê³¼ëª© ì´ë¦„ì„ ì•Œë ¤ì£¼ì„¸ìš”."
        question_only = q_text
        options_list  = options

        print(f" - ë¬¸ì œ: {q_text[:50]}...")
        print(f" - ë³´ê¸° ê°œìˆ˜: {len(options_list)}")

        # âœ… ë³€ê²½: ë‘ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
        final_state = agent.invoke(
            user_input_txt=user_input_txt,
            user_problem=question_only,
            user_problem_options=options_list,
            vectorstore_p=vectorstore_p,
            vectorstore_c=vectorstore_c,
            recursion_limit=1000,
        )

        gen_ans = final_state.get("generated_answer", "") or ""
        pred_idx = parse_idx_from_text(gen_ans)
        gen_text = options[pred_idx - 1] if (pred_idx and 1 <= pred_idx <= len(options)) else ""
        gen_exp = final_state.get("generated_explanation", "") or ""
        gen_sub = final_state.get("generated_subject", "") or ""
        print(f" - ì˜ˆì¸¡ ì •ë‹µ: {gen_ans}")
        print(f" - ì˜ˆì¸¡ í’€ì´: {gen_exp[:50]}...")

        # âœ… ë³€ê²½: ìœ ì‚¬ë¬¸ì œ + ê°œë…ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ëª¨ë‘ ì‚¬ìš©
        retrieved_problems = final_state.get("retrieved_docs", []) or []
        retrieved_concepts = final_state.get("concept_contexts", []) or []
        ctx_texts = build_contexts(retrieved_problems, retrieved_concepts)
        print(f" - Retrieval Contexts: {len(ctx_texts)}ê°œ (ìœ ì‚¬ë¬¸ì œ+ê°œë…ìš”ì•½)")

        q_full = f"{user_input_txt}\n\n" + build_question_with_options(q_text, options)

        gt_blob = f"ì •ë‹µ: {gt_idx}) {gt_text}".strip()
        if gt_exp:
            gt_blob += f"\ní’€ì´: {gt_exp}"
        if gt_sub:
            gt_blob += f"\nê³¼ëª©: {gt_sub}"

        rows.append(EvalRow(
            question=q_full,
            contexts=ctx_texts,
            answer=f"ì •ë‹µ: {gen_ans}) {gen_text}\ní’€ì´: {gen_exp}\nê³¼ëª©: {gen_sub}".strip(),
            ground_truth=gt_blob,
            metadata={
                "options": options,
                "gt_answer_idx": gt_idx,
                "gt_sub": gt_sub,
                "pred_idx": pred_idx,
                "gen_sub": gen_sub,
                "validated": final_state.get("validated", False),
            }
        ))

        # âœ… ê³¨ë“ ì…‹ í–‰ ëˆ„ì  (ë¬¸ì œ/ë³´ê¸°/ì •ë‹µ/í’€ì´/ê³¼ëª©: GT ê¸°ì¤€)
        golden_rows.append({
            "id": i,
            "question": q_text,
            "options": json.dumps(options, ensure_ascii=False),
            "gt_answer_idx": gt_idx,
            "gen_answer_idx": pred_idx,
            "gt_sub": gt_sub,
            "gen_subject": gen_sub,
            "gen_explanation": gen_exp,
            "retrieved_problems": [doc.page_content[:500] for doc in retrieved_problems],
            "retrieved_concepts": [doc.page_content[:500] for doc in retrieved_concepts],
            "contexts": ctx_texts
        })

        print(f"[{i}/{len(items)}] ì™„ë£Œ â€” GT:{gt_idx} / Pred:{pred_idx}")

    # ë¹ˆ rows ë°©ì§€
    if not rows:
        print("[ê²½ê³ ] ìˆ˜ì§‘ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ì €ì¥ ë° RAGAS í‰ê°€ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
        return

    df = pd.DataFrame([{
        "question": r.question,
        "contexts": r.contexts,
        "answer": r.answer,
        "ground_truth": r.ground_truth,
        "metadata": r.metadata,
    } for r in rows])

    os.makedirs(out_dir, exist_ok=True)
    df_path = os.path.join(out_dir, "qa_eval_rows.parquet")
    df.to_parquet(df_path, index=False)
    print(f"[ì €ì¥] QA í–‰ ì €ì¥ ì™„ë£Œ â†’ {df_path}")

    # âœ… ê³¨ë“ ì…‹ CSV ì €ì¥ (ì—‘ì…€ í˜¸í™˜ì„ ìœ„í•´ utf-8-sig)
    golden_df = pd.DataFrame(golden_rows)
    golden_csv = os.path.join(out_dir, "golden_set.csv")
    golden_df.to_csv(golden_csv, index=False, encoding="utf-8-sig")
    print(f"[ì €ì¥] Golden set ì €ì¥ ì™„ë£Œ â†’ {golden_csv}")

    # ---------- RAGAS í‰ê°€ ----------
    try:
        print(f"[RAGAS] using llm={type(llm).__name__}, embeddings={type(emb).__name__}")
        df_eval = df[
            df["contexts"].apply(lambda x: isinstance(x, list) and len(x) > 0) &
            df["ground_truths"].apply(lambda x: isinstance(x, list) and any(bool(t.strip()) for t in x))
        ].copy()

        if df_eval.empty:
            print("[ê²½ê³ ] í‰ê°€ ê°€ëŠ¥í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. (contexts í˜¹ì€ ground_truths ë¹„ì–´ìˆìŒ)")
            # ì €ì¥ë§Œ í•˜ê³  ì¢…ë£Œ
            df.to_parquet(df_path, index=False)
            golden_df.to_csv(golden_csv, index=False, encoding="utf-8-sig")
            return
        ds = Dataset.from_pandas(df_eval)
        ragas_result = evaluate(
            ds,
            metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
            llm=llm,
            embeddings=emb,
        )
        ragas_scores = pd.DataFrame(ragas_result.scores)
        print("[RAGAS í‰ê°€ ì™„ë£Œ]")
        print("RAGAS mean:", ragas_result)

    except Exception as e:
        print("[ì˜¤ë¥˜] RAGAS í‰ê°€ ì‹¤íŒ¨:", repr(e))
        # ë¹ˆ DataFrameìœ¼ë¡œë¼ë„ íŒŒì¼ ìƒì„±
        ragas_scores = pd.DataFrame(columns=["faithfulness", "answer_relevancy", "context_precision", "context_recall"])

    ragas_csv = os.path.join(out_dir, "ragas_scores.csv")
    ragas_scores.to_csv(ragas_csv, index=False)
    print(f"[ì €ì¥] RAGAS ì ìˆ˜ ì €ì¥ ì™„ë£Œ â†’ {ragas_csv}")

    # ---------- ê°ê´€ì‹ ì •í™•ë„(ì •ë‹µ ì¸ë±ìŠ¤ ì¼ì¹˜ìœ¨) ----------
    def multiple_choice_accuracy(md_series: pd.Series) -> float | None:
        ok, tot = 0, 0
        for m in md_series:
            if not isinstance(m, dict):
                continue
            gt = m.get("gt_answer_idx")
            pr = m.get("pred_idx")   # â† ì—¬ê¸°!
            if gt is None or pr is None:
                continue
            tot += 1
            ok += int(gt == pr)
        return ok / tot if tot else None

    mc_acc = multiple_choice_accuracy(df["metadata"])
    print(f"[ì •í™•ë„] ê°ê´€ì‹ ì •ë‹µë¥ : {mc_acc}")

    leaderboard = pd.concat([df.drop(columns=["contexts", "question", "answer", "ground_truth"], errors="ignore"),
                             ragas_scores], axis=1)
    leaderboard["mc_accuracy"] = mc_acc
    lb_csv = os.path.join(out_dir, "agent_eval_leaderboard.csv")
    leaderboard.to_csv(lb_csv, index=False)
    print(f"[ì €ì¥] Leaderboard ì €ì¥ ì™„ë£Œ â†’ {lb_csv}")

    def make_plots_for_run(out_dir: str):
        ragas_csv = os.path.join(out_dir, "ragas_scores.csv")
        if not os.path.exists(ragas_csv):
            print(f"[ì‹œê°í™”] {ragas_csv} ì—†ìŒ â†’ ìŠ¤í‚µ")
            return

        import pandas as pd
        df = pd.read_csv(ragas_csv)
        metrics = ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
        for m in metrics:
            if m not in df.columns:
                df[m] = np.nan

        # (A) í‰ê·  ë§‰ëŒ€ ê·¸ë˜í”„
        means = df[metrics].mean().fillna(0.0)
        plt.figure()
        plt.bar(np.arange(len(metrics)), means.values, width=0.6)
        plt.xticks(np.arange(len(metrics)), metrics, rotation=20, ha="right")
        plt.ylim(0, 1.0)
        plt.ylabel("Mean score")
        plt.title("RAGAS mean scores")
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, "ragas_means_bar.png"), dpi=150)
        plt.close()

        # (B) ë¬¸í•­ë³„ ë¼ì¸ ê·¸ë˜í”„
        plt.figure()
        x = np.arange(1, len(df) + 1)
        for m in metrics:
            plt.plot(x, df[m].values, marker="o", linestyle="-", label=m)
        plt.ylim(0, 1.0)
        plt.xlabel("Question index")
        plt.ylabel("Score")
        plt.title("Per-question RAGAS scores")
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, "per_question_scores.png"), dpi=150)
        plt.close()

        # (C) íˆíŠ¸ë§µ (ë¬¸í•­ Ã— ì§€í‘œ)
        heat = df[metrics].to_numpy(dtype=float)
        plt.figure()
        im = plt.imshow(heat, aspect="auto", vmin=0.0, vmax=1.0)
        plt.colorbar(im, fraction=0.046, pad=0.04)
        plt.yticks(np.arange(len(df)), [str(i) for i in x])
        plt.xticks(np.arange(len(metrics)), metrics, rotation=20, ha="right")
        plt.title("RAGAS scores heatmap")
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, "ragas_heatmap.png"), dpi=150)
        plt.close()

        print(f"[ì‹œê°í™”] ì €ì¥ ì™„ë£Œ â†’ {out_dir}")

    # run_eval()ì˜ ë§ˆì§€ë§‰ ì¤„ ê·¼ì²˜ì— ì¶”ê°€ í˜¸ì¶œ
    make_plots_for_run(out_dir)


# ---------- ì´ë¯¸ ìƒì„±ëœ ë¦¬ë”ë³´ë“œê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° ----------
def already_done(out_dir: str) -> bool:
    lb_csv = os.path.join(out_dir, "agent_eval_leaderboard.csv")
    return os.path.exists(lb_csv)

if __name__ == "__main__":
    # ğŸ”Œ í‰ê°€ ì‹œì‘ ì „ í•œ ë²ˆë§Œ ë‘ ì»¬ë ‰ì…˜ ì—°ê²°
    MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
    MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")

    vectorstore_p = connect_vectorstore("problems", MILVUS_HOST, MILVUS_PORT)
    vectorstore_c = connect_vectorstore("concept_summary", MILVUS_HOST, MILVUS_PORT)

    test_json_dir = "./teacher/exam/test_parsed_exam_json"
    json_files = glob.glob(os.path.join(test_json_dir, "*.json"))
    print(f"'{test_json_dir}' í´ë” ë‚´ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")
    out_dir_base = "./teacher/agents/solution/eval_results"

    skipped, done = 0, 0
    for json_file in json_files:
        print(f"\n=== '{json_file}' ì²˜ë¦¬ ì¤€ë¹„ ===")
        file_name = os.path.basename(json_file).replace(".json", "")
        out_dir = os.path.join(out_dir_base, file_name)

        if already_done(out_dir):
            print(f" - ì´ë¯¸ Leaderboard ì¡´ì¬ â†’ ê±´ë„ˆëœ€ (out_dir: {out_dir})")
            skipped += 1
            continue

        print(f" - ì‹¤í–‰ ì‹œì‘ (out_dir: {out_dir})")
        run_eval(
            test_json_path=json_file,
            out_dir=out_dir,
            vectorstore_p=vectorstore_p,
            vectorstore_c=vectorstore_c,
        )
        print(f"=== '{json_file}' ì²˜ë¦¬ ì™„ë£Œ ===")
        done += 1

    print(f"\nìš”ì•½: ì‹¤í–‰ {done}ê°œ / ê±´ë„ˆëœ€ {skipped}ê°œ")
