import os
from typing import TypedDict, List, Dict, Literal, Optional, Tuple, Any
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re
from langchain_openai import ChatOpenAI
from ..base_agent import BaseAgent
from docling.document_converter import DocumentConverter
from datetime import datetime

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# âœ… ìƒíƒœ ì •ì˜
class SolutionState(TypedDict):
    user_question: str
    user_problems: List[Dict]
    user_problem: str
    user_problem_options: List[str]

    source_type: Literal["internal", "external"]
    # ë‚´ë¶€/ì™¸ë¶€ ì›ì²œ
    short_term_memory: List[Dict]
    external_file_paths: List[str] 

    vectorstore: Milvus
    retrieved_docs: List[Document]
    similar_questions_text : str

    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    results: List[Dict]
    validated: bool
    retry_count: int             # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    exam_title: str
    difficulty: str
    subject: str

    chat_history: List[str]
    
class SolutionAgent(BaseAgent):
    """ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        self.graph = self._create_graph()
        
    @property
    def name(self) -> str:
        return "SolutionAgent"

    @property
    def description(self) -> str:
        return "ì‹œí—˜ë¬¸ì œë¥¼ ì¸ì‹í•˜ì—¬ ë‹µê³¼ í’€ì´, í•´ì„¤ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."

    def _create_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        # âœ… LangGraph êµ¬ì„±
        print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ë¶„ê¸° & ë¡œë”©
        graph.add_node("route", self._route)
        graph.add_node("load_from_short_term_memory", self._load_from_stm)
        graph.add_node("load_from_external_docs", self._load_from_external)

        # ê³µí†µ ì²˜ë¦¬
        graph.add_node("search_similarity", self._search_similar_questions)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        graph.add_node("store", self._store_to_vector_db)
        graph.add_node("next_problem", self._next_problem)

        graph.set_entry_point("route")
        graph.add_conditional_edges(
            "route", 
            lambda s: s["source_type"],
            {"internal": "load_from_short_term_memory", "external": "load_from_external_docs"})
        graph.add_edge("load_from_short_term_memory", "next_problem")
        graph.add_edge("load_from_external_docs", "next_problem")

        graph.add_edge("next_problem", "search_similarity")
        graph.add_edge("search_similarity", "generate_solution")
        graph.add_edge("generate_solution", "validate")

        graph.add_conditional_edges(
            "validate", 
            lambda s: "ok" if s["validated"] else ("back" if s.get("retry_count", 0) < 5 else "fail"),
            {"ok": "store", "back": "generate_solution", "fail": "next_problem"}
        )

        # ì €ì¥ í›„ ë‚¨ì€ ë¬¸ì œê°€ ìˆìœ¼ë©´ next_problemë¡œ ë£¨í”„
        graph.add_conditional_edges(
            "store",
            lambda s: "more" if len(s.get("user_problems", [])) > 0 else "done",
            {"more": "next_problem", "done": END}
        )

        return graph.compile()
    
    def _llm_extract_qas(self, text: str, llm) -> List[tuple]:
        """
        LLMì—ê²Œ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì£¼ê³ 
        [{"question":"...","options":["...","..."]}, ...] ë§Œ ë°›ëŠ”ë‹¤.
        ì‹¤íŒ¨ ì‹œ [] ë°˜í™˜.
        """
        sys_prompt = (
            "ë„ˆëŠ” ì‹œí—˜ ë¬¸ì œ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
            "ë¬¸ì œ ì§ˆë¬¸ê³¼ ë³´ê¸°ë¥¼ êµ¬ë¶„í•´ì„œ questionê³¼ options ë°°ì—´ë¡œ ì¶œë ¥í•œë‹¤."
            "optionsëŠ” ë³´ê¸° í•­ëª©ë§Œ í¬í•¨í•˜ê³ , ì„¤ëª…/í•´ì„¤/ì •ë‹µ ë“±ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. "
            "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ ë¬¸ì¥ì´ë‚˜ ì½”ë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ."
        )
        user_prompt = (
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸í•­ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ, ì •í™•íˆ ì¶”ì¶œí•´ JSON ë°°ì—´ë¡œ ë§Œë“¤ì–´ì¤˜.\n"
            "ìš”êµ¬ ìŠ¤í‚¤ë§ˆ: [{\"question\":\"...\",\"options\":[\"...\",\"...\"]}]\n"
            "ê·œì¹™:\n"
            "- ë¬¸ì œ ì§ˆë¬¸ì—ì„œ ë²ˆí˜¸(ì˜ˆ: 'ë¬¸ì œ 1.' ë“±)ì™€ ë¶ˆí•„ìš”í•œ ë¨¸ë¦¬ê¸€ì€ ì œê±°.\n"
            "- ì˜µì…˜ì€ 4ê°œê°€ ì¼ë°˜ì ì„.\n"
            f"í…ìŠ¤íŠ¸:\n{text}"
        )

        try:
            resp = llm.invoke([{"role":"system","content":sys_prompt},
                            {"role":"user","content":user_prompt}])
            content = (resp.content or "").strip()

            # JSONë§Œ ë‚¨ê¸°ê¸° (í˜¹ì‹œ ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì˜€ì„ ë•Œ ëŒ€ë¹„)
            m = re.search(r"\[.*\]", content, re.S)
            if not m:
                return []
            arr = json.loads(m.group(0))

            results = []
            for item in arr:
                q = (item.get("question") or "").strip()
                opts = [str(o).strip() for o in (item.get("options") or [])]
                if q:
                    results.append((q, opts))
            return results
        except Exception:
            return []

    @staticmethod
    def _split_problem_blocks(raw: str) -> List[str]:
        """
        ê°œì„ ëœ ë¬¸ì œ ë¸”ë¡ ë¶„í•  ì•Œê³ ë¦¬ì¦˜ (ì •ì  ë©”ì„œë“œ ë²„ì „)
        """
        return SolutionAgent.split_problem_blocks_without_keyword_static(raw)
    
    @staticmethod
    def normalize_docling_markdown_static(md: str) -> str:
        """Docling ë§ˆí¬ë‹¤ìš´ ì •ê·œí™” (ì •ì  ë©”ì„œë“œ)"""
        import re
        s = md
        s = re.sub(r'(?m)^\s*(\d+)\.\s*\1\.\s*', r'\1. ', s)  # '1. 1.' -> '1.'
        s = re.sub(r'(?m)^\s*(\d+)\s*\.\s*', r'\1. ', s)      # '1 . ' -> '1. '
        s = re.sub(r'[ \t]+', ' ', s).replace('\r', '')
        return s.strip()

    @staticmethod
    def _find_option_clusters_static(lines: List[str], start: int, end: int) -> List[Tuple[int, int]]:
        """ì˜µì…˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ì •ì  ë©”ì„œë“œ)"""
        import re
        _OPT_LINE = re.compile(
            r'(?m)^\s*(?:\(?([1-5])\)?\.?|[â‘ -â‘¤]|[ê°€-í•˜]\)|[A-Z]\))\s+\S'
        )
        
        clusters = []
        i = start
        while i < end:
            if _OPT_LINE.match(lines[i] or ''):
                j = i
                cnt = 0
                while j < end and _OPT_LINE.match(lines[j] or ''):
                    cnt += 1
                    j += 1
                if cnt >= 3:
                    clusters.append((i, j))  # [i, j) ì˜µì…˜ ë¸”ë¡
                i = j
            else:
                i += 1
        return clusters

    @staticmethod
    def split_problem_blocks_without_keyword_static(text: str) -> List[str]:
        """ê°œì„ ëœ ë¬¸ì œ ë¸”ë¡ ë¶„í•  (ì •ì  ë©”ì„œë“œ ë²„ì „)"""
        import re
        from typing import List, Tuple
        
        if not text:
            return []
            
        text = SolutionAgent.normalize_docling_markdown_static(text)
        lines = text.split('\n')
        n = len(lines)

        # ë¯¸ë¦¬ ì˜µì…˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ê³„ì‚°í•´ë†“ê³ , ê·¸ ë‚´ë¶€ ë²ˆí˜¸ëŠ” ë¬¸í•­ í—¤ë”ë¡œ ì•ˆ ë´„
        clusters = SolutionAgent._find_option_clusters_static(lines, 0, n)

        def in_option_cluster(idx: int) -> bool:
            for a, b in clusters:
                if a <= idx < b:
                    return True
            return False

        # ë¬¸í•­ í—¤ë” í›„ë³´ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        _QHEAD_CAND = re.compile(r'(?m)^\s*(\d{1,3})[.)]\s+\S')
        candidates = []
        for i, ln in enumerate(lines):
            m = _QHEAD_CAND.match(ln or '')
            if not m:
                continue
            if in_option_cluster(i):
                # ë³´ê¸° ë¸”ë¡ ì•ˆì˜ ë²ˆí˜¸ëŠ” ë¬¸í•­ í—¤ë”ê°€ ì•„ë‹˜
                continue
            num = int(m.group(1))
            candidates.append((i, num))

        # ì „ì—­ ì¦ê°€ ì‹œí€€ìŠ¤ + ì„¹ì…˜ ë¦¬ì…‹ í—ˆìš©ìœ¼ë¡œ ì‹¤ì œ í—¤ë” ì„ ë³„
        headers = []
        prev_num = 0
        last_header_idx = -9999
        for i, num in candidates:
            if num == prev_num + 1:
                headers.append(i)
                prev_num = num
                last_header_idx = i
                continue
            # ì„¹ì…˜ ë¦¬ì…‹: num==1ì´ê³ , ìµœê·¼ í—¤ë”ì—ì„œ ì¶©ë¶„íˆ ë–¨ì–´ì ¸ ìˆê±°ë‚˜ ì„¹ì…˜ ëŠë‚Œì˜ ë¼ì¸ ì¡´ì¬ ì‹œ í—ˆìš©
            if num == 1:
                window = '\n'.join(lines[max(0, i-3): i+1])
                if (i - last_header_idx) >= 8 or re.search(r'(â… |â…¡|III|ê³¼ëª©|íŒŒíŠ¸|SECTION)', window):
                    headers.append(i)
                    prev_num = 1
                    last_header_idx = i
                    continue
            # ê·¸ ì™¸ëŠ” ì˜µì…˜/ë…¸ì´ì¦ˆë¡œ ë¬´ì‹œ

        # í—¤ë”ê°€ í•˜ë‚˜ë„ ì•ˆ ì¡íˆë©´ í´ë°± ì „ëµ ì‚¬ìš©
        if not headers:
            print(f"âŒ [ë””ë²„ê·¸] í—¤ë”ê°€ í•˜ë‚˜ë„ ì„ íƒë˜ì§€ ì•ŠìŒ - í´ë°± ì „ëµ ì‚¬ìš©")
            # í´ë°± 1: ë” ëŠìŠ¨í•œ ì¡°ê±´ìœ¼ë¡œ ì¬ì‹œë„
            if candidates:
                print(f"ğŸ”„ [í´ë°±] ìˆœì°¨ ì¡°ê±´ ì—†ì´ ëª¨ë“  í›„ë³´ë¥¼ í—¤ë”ë¡œ ì‚¬ìš©")
                headers = [i for i, num in candidates]
            else:
                # í´ë°± 2: ê¸°ë³¸ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
                print(f"ğŸ”„ [í´ë°±] ê¸°ë³¸ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• ")
                simple_pattern = re.compile(r'(?m)^\s*(\d{1,2})\.\s+')
                for i, ln in enumerate(lines):
                    if simple_pattern.match(ln or ''):
                        headers.append(i)
                        print(f"ğŸ“Œ [í´ë°±] ë¼ì¸ {i}: '{ln[:30]}...' â†’ í—¤ë” ì¶”ê°€")
            
            if not headers:
                print(f"âŒ [í´ë°± ì‹¤íŒ¨] ì „ì²´ë¥¼ 1ê°œ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬")
                return [text] if text.strip() else []

        # í—¤ë” ë²”ìœ„ë¡œ ë¸”ë¡ ë§Œë“¤ê¸°
        headers.append(n)  # sentinel
        blocks = []
        for a, b in zip(headers[:-1], headers[1:]):
            blk = '\n'.join(lines[a:b]).strip()
            if blk:
                blocks.append(blk)
        return blocks

    
    # --------- ë¶„ê¸° ----------
    def _route(self, state: SolutionState) -> SolutionState:
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ì±„ì›Œì¤€ source_typeì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        st = state["source_type"]
        print(f"ğŸ§­ ë¶„ê¸°: {st}")
        return state

    # --------- ë‚´ë¶€: STMì—ì„œ ë¬¸ì œ 1ê°œ êº¼ë‚´ì™€ stateì— ì„¸íŒ… ----------
    def _load_from_stm(self, state: SolutionState) -> SolutionState:
        """
        ë‚´ë¶€ ëª¨ë“œì—ì„œ ë¬¸ì œ ë¡œë“œ (pdf_extracted ìš°ì„ , short_term_memory ì°¨ì„ )
        """
        print("ğŸ“Š [ë‚´ë¶€] ë¬¸ì œ ë¡œë“œ ì‹œì‘")
        
        # 1. pdf_extracted ìš°ì„  í™•ì¸ (PDF ì „ì²˜ë¦¬ ë°ì´í„°)
        pdf_data = state.get("pdf_extracted", {})
        pdf_questions = pdf_data.get("question", []) or []
        
        if pdf_questions:
            print("ğŸ“„ PDF ì „ì²˜ë¦¬ ë°ì´í„°ì—ì„œ ë¬¸ì œ ë¡œë“œ")
            questions = pdf_questions
            options_list = pdf_data.get("options", []) or []
        else:
            # 2. short_term_memoryì—ì„œ ë¡œë“œ
            print("ğŸ“Š short_term_memoryì—ì„œ ë¬¸ì œ ë¡œë“œ")
            stm = state.get("short_term_memory", [])
            questions = [x.get("question", "") for x in stm]
            options_list = [x.get("options", []) for x in stm]
        
        # user_problems ì„¤ì •
        user_problems = []
        for i, question in enumerate(questions):
            options = options_list[i] if i < len(options_list) else []
            if question and options:
                user_problems.append({
                    "question": question,
                    "options": options
                })
        
        state["user_problems"] = user_problems
        state["short_term_memory"] = []  # íë¡œ ì´ê´€
        
        print(f"âœ… [ë‚´ë¶€] ìµœì¢… ë¡œë“œëœ ë¬¸ì œ: {len(user_problems)}ê°œ")
        return state
    
    def _filter_problems_by_range(self, problems: List[Dict], problem_range: Dict) -> List[Dict]:
        """ë¬¸ì œ ë²”ìœ„ì— ë”°ë¼ ë¬¸ì œë“¤ì„ í•„í„°ë§"""
        if not problem_range:
            return problems
        
        range_type = problem_range.get("type")
        
        if range_type == "single":
            # ë‹¨ì¼ ë²ˆí˜¸
            target_num = problem_range.get("number")
            return [p for p in problems if p.get("index") == target_num]
        
        elif range_type == "range":
            # ë²”ìœ„
            start = problem_range.get("start")
            end = problem_range.get("end")
            return [p for p in problems if start <= p.get("index", 0) <= end]
        
        elif range_type == "specific":
            # íŠ¹ì • ë²ˆí˜¸ë“¤
            target_numbers = problem_range.get("numbers", [])
            return [p for p in problems if p.get("index") in target_numbers]
        
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë²”ìœ„ íƒ€ì…: {range_type}")
            return problems
    
    # --------- ì™¸ë¶€: shared stateì—ì„œ ì „ì²˜ë¦¬ëœ ë¬¸ì œ ë¡œë“œ ----------
    def _load_from_external(self, state: SolutionState) -> SolutionState:
        """
        ì „ì²˜ë¦¬ ë…¸ë“œì—ì„œ ì¶”ì¶œëœ ë¬¸ì œë“¤ì„ ì ì ˆí•œ ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì™€ì„œ user_problemsì— ì„¤ì •
        """
        print("ğŸ“„ [ì™¸ë¶€] ë¬¸ì œ ë¡œë“œ ì‹œì‘")
        
        # artifactsì—ì„œ ë¬¸ì œ ì†ŒìŠ¤ì™€ ë²”ìœ„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        artifacts = state.get("artifacts", {})
        problem_source = artifacts.get("problem_source")
        problem_range = artifacts.get("problem_range")
        
        print(f"ğŸ“š ë¬¸ì œ ì†ŒìŠ¤: {problem_source}")
        print(f"ğŸ”¢ ë¬¸ì œ ë²”ìœ„: {problem_range}")
        
        # ë¬¸ì œ ì†ŒìŠ¤ ê²°ì • (ìš°ì„ ìˆœìœ„: PDF ì¡´ì¬ ì—¬ë¶€ > ëª…ì‹œì  ì§€ì • > shared)
        pdf_data = state.get("pdf_extracted", {})
        pdf_questions = pdf_data.get("question", []) or []
        
        if pdf_questions:
            # PDF ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ PDF ìš°ì„ 
            questions = pdf_questions
            options_list = pdf_data.get("options", []) or []
            print("ğŸ“„ PDF ì „ì²˜ë¦¬ stateì—ì„œ ë¬¸ì œ ë¡œë“œ (PDF ë°ì´í„° ì¡´ì¬)")
        elif problem_source == "shared":
            questions = state.get("question", []) or []
            options_list = state.get("options", []) or []
            print("ğŸ“Š shared stateì—ì„œ ë¬¸ì œ ë¡œë“œ")
        elif problem_source == "pdf_extracted" or artifacts.get("pdf_ids"):
            questions = pdf_questions  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
            options_list = pdf_data.get("options", []) or []
            print("ğŸ“„ PDF ì „ì²˜ë¦¬ stateì—ì„œ ë¬¸ì œ ë¡œë“œ (ëª…ì‹œì  ì§€ì •)")
        else:
            # ê¸°ë³¸: shared state ì‚¬ìš©
            questions = state.get("question", []) or []
            options_list = state.get("options", []) or []
            print("ğŸ“Š shared stateì—ì„œ ë¬¸ì œ ë¡œë“œ (ê¸°ë³¸ê°’)")
            
        # ë””ë²„ê·¸ ì •ë³´
        print(f"ğŸ” [ë””ë²„ê·¸] ìµœì¢… ì„ íƒëœ ì†ŒìŠ¤ì˜ ë¬¸ì œ ìˆ˜: {len(questions)}")
        print(f"ğŸ” [ë””ë²„ê·¸] ì „ì²´ state í‚¤ë“¤: {list(state.keys())}")
        print(f"ğŸ” [ë””ë²„ê·¸] pdf_extracted ì¡´ì¬: {'pdf_extracted' in state}")
        if 'pdf_extracted' in state:
            pdf_debug = state['pdf_extracted']
            print(f"ğŸ” [ë””ë²„ê·¸] pdf_extracted ë¬¸ì œ ìˆ˜: {len(pdf_debug.get('question', []))}")
        
        if not questions:
            print("âš ï¸ ì„ íƒëœ ì†ŒìŠ¤ì— ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
            state["user_problems"] = []
            return state
        
        # user_problems í˜•íƒœë¡œ ë³€í™˜
        all_problems = []
        for i, question in enumerate(questions):
            options = options_list[i] if i < len(options_list) else []
            if question and options:
                all_problems.append({
                    "question": question,
                    "options": options,
                    "index": i + 1  # 1-based ë²ˆí˜¸
                })
        
        # ë¬¸ì œ ë²”ìœ„ í•„í„°ë§
        if problem_range:
            filtered_problems = self._filter_problems_by_range(all_problems, problem_range)
            print(f"ğŸ¯ ë²”ìœ„ í•„í„°ë§: {len(all_problems)}ê°œ â†’ {len(filtered_problems)}ê°œ")
        else:
            filtered_problems = all_problems
            print(f"ğŸ“ ì „ì²´ ë¬¸ì œ ë¡œë“œ: {len(filtered_problems)}ê°œ")
        
        # index ì œê±° (solution_agent ë‚´ë¶€ì—ì„œëŠ” í•„ìš” ì—†ìŒ)
        user_problems = []
        for problem in filtered_problems:
            user_problems.append({
                "question": problem["question"],
                "options": problem["options"]
            })
        
        state["user_problems"] = user_problems
        print(f"âœ… ìµœì¢… ë¡œë“œëœ ë¬¸ì œ: {len(user_problems)}ê°œ")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…ìš©)
        saved_file = self.save_user_problems_to_json(user_problems, "user_problems_json.json")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {saved_file}")
        
        return state
    
    # --------- ê¸°ì¡´ PDF ì¶”ì¶œ ë¡œì§ (ë°±ì—…ìš©) ----------
    def _load_from_external_OLD_BACKUP(self, state: SolutionState) -> SolutionState:
        """
        PDF/ë¬¸ì„œ â†’ í…ìŠ¤íŠ¸ â†’ [ë¬¸ì œ ë¸”ë¡ ë¶„í• ] â†’ (ë¸”ë¡ ë‹¨ìœ„) LLM íŒŒì‹± â†’ 'ë¬¸í•­+ë³´ê¸°4'ë§Œ ì €ì¥
        """
        print("ğŸ“„ [ì™¸ë¶€] ì²¨ë¶€ ë¬¸ì„œ ë¡œë“œ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ â†’ ë¸”ë¡ ë‹¨ìœ„ LLM íŒŒì‹± ì‹œì‘")
        paths = state.get("external_file_paths", [])
        if not paths:
            raise ValueError("external_file_paths ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ë¶„ê¸°ì—ì„œëŠ” íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ê¶Œí•œ ë¬¸ì œ í•´ê²°
        import os
        os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
        os.environ['HF_HOME'] = 'C:\\temp\\huggingface_cache'
        
        # cv2 setNumThreads ë¬¸ì œ í•´ê²°
        try:
            import cv2
            if not hasattr(cv2, 'setNumThreads'):
                # setNumThreadsê°€ ì—†ìœ¼ë©´ ë”ë¯¸ í•¨ìˆ˜ ì¶”ê°€
                cv2.setNumThreads = lambda x: None
        except ImportError:
            pass
        
        converter = DocumentConverter()

        # LLM (ì—„ê²©í•œ êµ¬ì¡°í™” ì „ìš©)
        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="moonshotai/kimi-k2-instruct",
            temperature=0
        )

        # ----- ë¸”ë¡ 1ê°œë¥¼ LLMìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ -----
        def parse_block_with_llm(block_text: str) -> Optional[Dict[str, object]]:
            # ë…¸ì´ì¦ˆ ì œê±° (ì •ë‹µ/í•´ì„¤ ë¼ì¸)
            cleaned = []
            for ln in block_text.splitlines():
                if re.search(r"(ì •ë‹µ|í•´ì„¤|ë‹µì•ˆ|í’€ì´|answer|solution)\s*[:ï¼š]", ln, re.I):
                    continue
                cleaned.append(ln)
            cleaned_text = "\n".join(cleaned).strip()

            if len(cleaned_text) < 5:
                return None

            sys_prompt = (
                "ë„ˆëŠ” ì‹œí—˜ ë¸”ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ êµ¬ì¡°í™”í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
                "ì…ë ¥ ë¸”ë¡ì—ëŠ” 'í•œ ë¬¸ì œ'ê°€ ë“¤ì–´ìˆë‹¤. "
                "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•˜ë‚˜ì˜ ê°ì²´ë¡œë§Œ í•˜ë©°, ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ì§€ì¼œë¼:\n"
                '{"question": "<ì§ˆë¬¸ ë³¸ë¬¸(ë²ˆí˜¸/ë¨¸ë¦¬ê¸€ ì œê±°)>", "options": ["<ë³´ê¸°1>","<ë³´ê¸°2>","<ë³´ê¸°3>","<ë³´ê¸°4>"]}\n'
                "ì£¼ì˜ì‚¬í•­:\n"
                "- ë°˜ë“œì‹œ optionsëŠ” ì •í™•íˆ 4ê°œì—¬ì•¼ í•œë‹¤.\n"
                "- ì…ë ¥ ë¸”ë¡ì— ìˆëŠ” ë³´ê¸° í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³  ìƒˆë¡œ ë§Œë“¤ì§€ ë§ˆë¼.\n"
                "- ë¶ˆí•„ìš”í•œ ì„¤ëª…/ì •ë‹µ/í•´ì„¤/ì½”ë“œë¸”ë¡/ë¬¸ìì—´ì€ ì¶œë ¥í•˜ì§€ ë§ˆë¼. JSONë§Œ ì¶œë ¥í•˜ë¼."
            )
            user_prompt = f"ë‹¤ìŒ ë¸”ë¡ì„ êµ¬ì¡°í™”í•˜ë¼:\n```\n{cleaned_text}\n```"

            try:
                resp = llm.invoke([
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ])
                content = (resp.content or "").strip()
                m = re.search(r"\{.*\}", content, re.S)  # JSON ê°ì²´ë§Œ ì¶”ì¶œ
                if not m:
                    return None
                obj = json.loads(m.group(0))

                q = (obj.get("question") or "").strip()
                opts = [str(o).strip() for o in (obj.get("options") or []) if str(o).strip()]
                if not q or len(opts) != 4:
                    return None

                # ë²ˆí˜¸/ë¨¸ë¦¬ê¸€ ì •ë¦¬
                q = re.sub(r"^\s*(?:ë¬¸ì œ\s*)?\d{1,3}\s*[\).:]\s*", "", q).strip()
                norm_opts = []
                for o in opts:
                    o = re.sub(r"^\s*(?:\(?[â‘ -â‘£1-4A-Da-dê°€-ë¼]\)?[\).ï¼\.]?)\s*", "", o).strip()
                    norm_opts.append(o)

                return {"question": q, "options": norm_opts}

            except Exception as e:
                print(f"âš ï¸ LLM íŒŒì‹± ì‹¤íŒ¨: {e}")
                return None

        extracted: List[Dict[str, object]] = []

        for p in paths:
            try:
                result = converter.convert(p)
                doc = result.document
            except Exception as e:
                print(f"âš ï¸ ë³€í™˜ ì‹¤íŒ¨: {p} - {e}")
                continue

            # ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw = ""
            if hasattr(doc, "export_to_markdown"):
                raw = doc.export_to_markdown()
            elif hasattr(doc, "export_to_text"):
                raw = doc.export_to_text()
            raw = (raw or "").replace("\r\n", "\n")

            # âœ… ë¬¸ì œ ë¸”ë¡ ë¶„í•  (ë¹ˆ ì¤„ 2ê°œ ì´ìƒ ê¸°ì¤€ + ì¼ë¶€ í—¤ë” ì œê±°)
            blocks = self._split_problem_blocks(raw)
            print(f"ğŸ“¦ {p} | ì¶”ì • ë¬¸ì œ ë¸”ë¡ ìˆ˜: {len(blocks)}")

            for idx, block in enumerate(blocks, 1):
                item = parse_block_with_llm(block)
                if item:
                    extracted.append({
                        "question": item["question"],
                        "options": item["options"],
                        "source": p,
                        "block_index": idx
                    })

        if not extracted:
            raise ValueError("ë¬¸ì„œì—ì„œ 'ë¬¸í•­ + ë³´ê¸°4'ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLM íŒŒì‹± ê·œì¹™ ë˜ëŠ” ë¸”ë¡ ë¶„í•  ê¸°ì¤€ì„ ì¡°ì •í•˜ì„¸ìš”.")

        # âœ… ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        seen, deduped = set(), []
        for it in extracted:
            key = re.sub(r"\s+", " ", it["question"]).strip()
            if key in seen:
                continue
            seen.add(key)
            deduped.append(it)

        state["user_problems"] = [{"question": it["question"], "options": it["options"]} for it in deduped]
        print(f"âœ… ìµœì¢… ì¶”ì¶œ ë¬¸í•­ ìˆ˜(ë³´ê¸° 4ê°œ): {len(state['user_problems'])}")

        saved_file = self.save_user_problems_to_json(state["user_problems"], "user_problems_json.json")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {saved_file}")
        return state


    # ê°„ë‹¨í•œ ë¬¸ì œ/ë³´ê¸° íŒŒì„œ (ë¬¸ì„œ í¬ë§·ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
    def _split_by_questions(self, text: str) -> List[tuple]:
        blocks = re.split(r"\n\s*\n", text)  # ë¹ˆ ì¤„ ê¸°ì¤€ ê±°ì¹ ê²Œ ë¶„í• 
        results = []
        for b in blocks:
            lines = [ln.strip() for ln in b.splitlines() if ln.strip()]
            if not lines:
                continue
            # ì˜µì…˜ ë¼ì¸ ê°ì§€ (ìˆ«ì. ë˜ëŠ” ìˆ«ì) íŒ¨í„´)
            opts = [ln for ln in lines if re.match(r"^\(?\d+\)?[).]\s*", ln)]
            if opts:
                # ë¬¸ì œë¬¸ì€ ì˜µì…˜ ë¼ì¸ ì œì™¸ ì²« ì¤„ ìœ„ì£¼ë¡œ ì‚¬ìš©
                question_lines = [ln for ln in lines if ln not in opts]
                qtext = " ".join(question_lines) if question_lines else lines[0]
                # ì˜µì…˜ í…ìŠ¤íŠ¸ ì •ì œ: "1) ..." â†’ "..." ë¡œ
                clean_opts = [re.sub(r"^\(?\d+\)?[).]\s*", "", o) for o in opts]
                results.append((qtext, clean_opts))
        return results


    def _search_similar_questions(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        
        vectorstore = state.get("vectorstore")
        if vectorstore is None:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì–´ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            state["retrieved_docs"] = []
            state["similar_questions_text"] = ""
            print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ (ê±´ë„ˆëœ€)")
            return state
        
        try:
            results = vectorstore.similarity_search(state["user_problem"], k=3)
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results = []
        
        similar_questions = []
        for i, doc in enumerate(results):
            metadata = doc.metadata
            options = json.loads(metadata.get("options", "[]"))
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i+1}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer}
                í’€ì´: {explanation}
                """
            similar_questions.append(formatted)
        
        state["retrieved_docs"] = results
        state["similar_questions_text"] = "\n\n".join(similar_questions) 

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ.")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state

    def _generate_solution(self, state: SolutionState) -> SolutionState:

        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="moonshotai/kimi-k2-instruct",
            temperature=0.5
        )

        similar_problems = state.get("similar_questions_text", "")
        print("ìœ ì‚¬ ë¬¸ì œë“¤:\n", similar_problems[:100])

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_question']}
            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {similar_problems}

            1. ì´ ë¬¸ì œì˜ **ì •ë‹µ**ë§Œ ê°„ê²°í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¨¼ì € ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
        """

        response = llm.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["chat_history"].append(f"Q: {state['user_question']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state

    # âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

    def _validate_solution(self, state: SolutionState) -> SolutionState:
        print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
        
        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="moonshotai/kimi-k2-instruct",
            temperature=0
        )

        validation_prompt = f"""
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_question']}

        ë¬¸ì œ ì§ˆë¬¸: {state['user_problem']}
        ë¬¸ì œ ë³´ê¸°: {state['user_problem_options']}

        ìƒì„±ëœ ì •ë‹µ: {state['generated_answer']}
        ìƒì„±ëœ í’€ì´: {state['generated_explanation']}

        ìƒì„±ëœ í•´ë‹µê³¼ í’€ì´ê°€ ë¬¸ì œì™€ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
        ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        validation_response = llm.invoke(validation_prompt)
        result_text = validation_response.content.strip().lower()

        # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
        print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
        state["validated"] = "ë„¤" in result_text
        
        if not state["validated"]:
            state["retry_count"] = state.get("retry_count", 0) + 1
            print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì¬ì‹œë„ {state['retry_count']}/5)")
        else:
            print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
            
        return state


    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:  
        print("\nğŸ§© [4ë‹¨ê³„] ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ì‹œì‘")

        # ë²¡í„° DB ì €ì¥ (ì™¸ë¶€ì¸ ê²½ìš°)
        if state["source_type"] == "external":
            vectorstore = state["vectorstore"] 

            # ì¤‘ë³µ ë¬¸ì œ í™•ì¸
            similar = vectorstore.similarity_search(state["user_problem"], k=1)
            if similar and state["user_problem"].strip() in similar[0].page_content:
                print("âš ï¸ ë™ì¼í•œ ë¬¸ì œê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥ ìƒëµ")
            else:
                # ë¬¸ì œ, í•´ë‹µ, í’€ì´ë¥¼ ê°ê° metadataë¡œ ì €ì¥
                doc = Document(
                    page_content=state["user_problem"],
                    metadata={
                        "options": json.dumps(state.get("user_problem_options", [])), 
                        "answer": state["generated_answer"],
                        "explanation": state["generated_explanation"]
                    }
                )
                vectorstore.add_documents([doc])
                print("âœ… ë¬¸ì œ+í•´ë‹µ+í’€ì´ ì €ì¥ ì™„ë£Œ")
        else:
            print("âš ï¸ ë‚´ë¶€ ì €ì¥ì†ŒëŠ” ë²¡í„° DB ì €ì¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‚´ë¶€ ë¬¸ì œë¡œë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            # ë‚´ë¶€: ìš”êµ¬ ìŠ¤í‚¤ë§ˆ(JSON)ë¡œ íŒŒì¼ ëˆ„ì  ì €ì¥
            store_path = "./internal_store.json"
            data = {
                "exam_title": state.get("exam_title", "ë‚´ë¶€ ë¬¸ì œ ëª¨ìŒ"),
                "total_questions": 0,
                "difficulty": state.get("difficulty", "ì¤‘ê¸‰"),
                "subjects": {},  # subjectëª…: {"requested_count":0,"actual_count":n,"questions":[...]}
            }

            if os.path.exists(store_path):
                try:
                    with open(store_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                except Exception:
                    pass

            subj = state.get("subject", "ê¸°íƒ€")
            subjects = data.setdefault("subjects", {})
            bucket = subjects.setdefault(subj, {"requested_count": 0, "actual_count": 0, "questions": []})

            bucket["questions"].append({
                "question": state["user_problem"],
                "options": [f"  {i+1}. {opt}" for i, opt in enumerate(state.get("user_problem_options", []))],
                "answer": state["generated_answer"],
                "explanation": state["generated_explanation"],
                "subject": subj,
            })
            bucket["actual_count"] = len(bucket["questions"])

            # ì´ ë¬¸í•­ ìˆ˜ ì¬ê³„ì‚°
            total = 0
            for v in subjects.values():
                total += len(v.get("questions", []))
            data["total_questions"] = total

            with open(store_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ë‚´ë¶€ ë¬¸ì œ ì €ì¥(JSON ìŠ¤í‚¤ë§ˆ) ì™„ë£Œ â†’ {store_path}")

        # ê²°ê³¼ë¥¼ stateì— ì €ì¥ (í•­ìƒ ì‹¤í–‰)
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì‹œì‘:")
        print(f"   - í˜„ì¬ ë¬¸ì œ: {state['user_problem'][:50]}...")
        print(f"   - ìƒì„±ëœ ì •ë‹µ: {state['generated_answer'][:30]}...")
        print(f"   - ê²€ì¦ ìƒíƒœ: {state['validated']}")
        
        item = {
            "question": state["user_problem"],
            "options": state["user_problem_options"],
            "generated_answer": state["generated_answer"],
            "generated_explanation": state["generated_explanation"],
            "validated": state["validated"],
            "chat_history": state.get("chat_history", [])
        }
        
        
        state["results"].append(item)
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(state['results'])}ê°œ")
        
        return state
    
    def _next_problem(self, state: SolutionState) -> SolutionState:
        queue = state.get("user_problems", [])
        if not queue:
            raise ValueError("ì²˜ë¦¬í•  ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. user_problemsê°€ ë¹„ì–´ìˆì–´ìš”.")
        
        current = queue.pop(0)
        state["user_problem"] = current.get("question", "")
        state["user_problem_options"] = current.get("options", [])
        state["user_problems"] = queue
        
        print(f"ğŸ“ ë‹¤ìŒ ë¬¸ì œ ì²˜ë¦¬: {state['user_problem'][:50]}...")
        print(f"   - ë‚¨ì€ ë¬¸ì œ ìˆ˜: {len(queue)}")
        
        return state

    def execute(
            self, 
            user_question: str, 
            source_type: Literal["internal", "external"],
            vectorstore: Optional[Milvus] = None,
            short_term_memory: Optional[List[Dict]] = None,
            external_file_paths: Optional[List[str]] = None,
            exam_title: str = "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ëª¨ì˜ê³ ì‚¬ (Groq ìˆœì°¨ ë²„ì „)",
            difficulty: str = "ì¤‘ê¸‰",
            subject: str = "ê¸°íƒ€",
            recursion_limit: int = 1000,
        ) -> Dict:

        # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if vectorstore is None:
            try:
                embedding_model = HuggingFaceEmbeddings(
                    model_name="jhgan/ko-sroberta-multitask",
                    model_kwargs={"device": "cpu"}
                )

                if "default" in connections.list_connections():
                    connections.disconnect("default")
                connections.connect(alias="default", host="localhost", port="19530")

                vectorstore = Milvus(
                    embedding_function=embedding_model,
                    collection_name="problems",
                    connection_args={"host": "localhost", "port": "19530"}
                )
                print("âœ… Milvus ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
                print("   - ë²¡í„°ìŠ¤í† ì–´ ì—†ì´ ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤.")
                vectorstore = None
        
        initial_state: SolutionState = {
            "user_question": user_question,
            "user_problems": [], 
            "user_problem": "",
            "user_problem_options": [],

            "source_type": source_type,
            "short_term_memory": short_term_memory or [],
            "external_file_paths": external_file_paths or [],

            "vectorstore": vectorstore,
            "retrieved_docs": [],
            "similar_questions_text": "",

            "generated_answer": "",
            "generated_explanation": "",
            "validated": False,
            "retry_count": 0,
            "results": [],
            
            "exam_title": exam_title,
            "difficulty": difficulty,
            "subject": subject,

            "chat_history": []
        }
        
        final_state = self.graph.invoke(initial_state, config={"recursion_limit": recursion_limit})
        
        # ê²°ê³¼ í™•ì¸ ë° ë””ë²„ê¹…
        results = final_state.get("results", [])
        print(f"\nğŸ¯ ìµœì¢… ì‹¤í–‰ ê²°ê³¼:")
        print(f"   - ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        print(f"   - ê²°ê³¼ í‚¤ ì¡´ì¬: {'results' in final_state}")
        print(f"   - ìƒíƒœ í‚¤ë“¤: {list(final_state.keys())}")
        
        if results:
            for i, result in enumerate(results):
                print(f"   - ê²°ê³¼ {i+1}: {result.get('question', '')[:30]}...")
        else:
            print("   âš ï¸ resultsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            print(f"   - final_state ë‚´ìš©: {final_state}")
        
        return results

    def save_user_problems_to_json(self, user_problems: List[Dict], filename: str = None) -> str:
        """
        user_problemsë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            user_problems: ì €ì¥í•  ë¬¸ì œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            filename: ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"user_problems_{timestamp}.json"
        
        # íŒŒì¼ ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
        if not os.path.isabs(filename):
            filename = os.path.join(os.getcwd(), filename)
        
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # JSON ë°ì´í„° ì¤€ë¹„
        data = {
            "timestamp": datetime.now().isoformat(),
            "total_problems": len(user_problems),
            "problems": user_problems
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… user_problemsê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
        return filename

    def invoke(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        LangGraph ê·¸ë˜í”„ë¥¼ subgraphë¡œ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            input_data (Dict[str, Any]): ì—ì´ì „íŠ¸ ì‹¤í–‰ì— í•„ìš”í•œ ì…ë ¥ ë°ì´í„°ì…ë‹ˆë‹¤.
            
        Returns:
            Dict[str, Any]: ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼ ë°ì´í„°ì…ë‹ˆë‹¤.
        """
        try:
            # LangGraph ê·¸ë˜í”„ ì‹¤í–‰
            final_state = self.graph.invoke(input_data)
            
            # ê²°ê³¼ ì¶”ì¶œ ë° ë°˜í™˜
            results = final_state.get("results", [])
            generated_answer = final_state.get("generated_answer", "")
            generated_explanation = final_state.get("generated_explanation", "")
            
            return {
                "results": results,
                "generated_answer": generated_answer,
                "generated_explanation": generated_explanation,
                "final_state": final_state
            }
            
        except Exception as e:
            print(f"âŒ SolutionAgent invoke ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "results": [],
                "generated_answer": "",
                "generated_explanation": "",
                "error": str(e)
            }

if __name__ == "__main__":
    # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )

    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port":"19530"}
    )

    agent = SolutionAgent()

    # ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(agent.graph.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì›Œí¬í”Œë¡œìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

    # âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    user_question = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” : ").strip()
    
    results = agent.execute(user_question, "external", vectorstore, external_file_paths=["./user_problems.pdf"])

    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    results_data = {
        "timestamp": datetime.now().isoformat(),
        "user_question": user_question,
        "total_results": len(results),
        "results": results
    }
    
    results_filename = f"solution_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_filename, "w", encoding="utf-8") as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… í•´ë‹µ ê²°ê³¼ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_filename}")
