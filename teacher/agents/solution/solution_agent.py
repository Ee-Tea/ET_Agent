import os
from typing import TypedDict, List, Dict, Literal, Optional, Tuple, Any
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re, hashlib
from langchain_openai import ChatOpenAI
from ..base_agent import BaseAgent


load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))

# âœ… ìƒíƒœ ì •ì˜
class SolutionState(TypedDict):
    # ì‚¬ìš©ì ì…ë ¥
    user_input_txt: str

    # ë¬¸ì œë¦¬ìŠ¤íŠ¸, ë¬¸ì œ, ë³´ê¸°
    user_problem: str
    user_problem_options: List[str]
    
    vectorstore: Milvus

    retrieved_docs: List[Document]
    similar_questions_text : str

    # ë¬¸ì œ í•´ë‹µ/í’€ì´/ê³¼ëª© ìƒì„±
    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    generated_subject: str

    results: List[Dict]
    validated: bool
    retry_count: int             # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    chat_history: List[str]
    
class SolutionAgent(BaseAgent):
    """ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        self.graph = self._create_graph()
        
    @property
    def name(self) -> str:
        return "SolutionAgent"

    @property
    def description(self) -> str:
        return "ì‹œí—˜ë¬¸ì œë¥¼ ì¸ì‹í•˜ì—¬ ë‹µê³¼ í’€ì´, í•´ì„¤ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."

    def _llm(self, temperature: float = 0):
        return ChatOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"),
            model=OPENAI_LLM_MODEL,  # âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸
            temperature=temperature,
        )

    def _create_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        # âœ… LangGraph êµ¬ì„±
        print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ê³µí†µ ì²˜ë¦¬
        graph.add_node("search_similarity", self._search_similar_questions)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        graph.add_node("store", self._store_to_vector_db)

        graph.set_entry_point("search_similarity")
        graph.add_edge("search_similarity", "generate_solution")
        graph.add_edge("generate_solution", "validate")
        graph.add_edge("store", END)

        graph.add_conditional_edges(
            "validate", 
            lambda s: "ok" if s["validated"] else ("back" if s.get("retry_count", 0) < 5 else END),
            {"ok": "store", "back": "generate_solution"}
        )

        return graph.compile()
    
    #----------------------------------------nodes------------------------------------------------------

    def _search_similar_questions(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        print(state["user_problem"], state["user_problem_options"])
        
        vectorstore = state.get("vectorstore")
        if vectorstore is None:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì–´ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            state["retrieved_docs"] = []
            state["similar_questions_text"] = ""
            print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ (ê±´ë„ˆëœ€)")
            return state
        
        try:
            results = vectorstore.similarity_search(state["user_problem"], k=3)
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results = []
        
        similar_questions = []
        for i, doc in enumerate(results):
            metadata = doc.metadata
            options = json.loads(metadata.get("options", "[]"))
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")
            subject = metadata.get("subject", "ê¸°íƒ€")

            # ì •ë‹µì´ ë³´ê¸° ë²ˆí˜¸(ë¬¸ìì—´)ë¼ë©´ í•´ë‹¹ ë²ˆí˜¸ì˜ ë³´ê¸° í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ì„œ í‘œì‹œ
            answer_text = ""
            try:
                answer_idx = int(answer) - 1
                if 0 <= answer_idx < len(options):
                    answer_text = options[answer_idx]
            except Exception:
                answer_text = ""

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i+1}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer} ({answer_text})
                í’€ì´: {explanation}
                ê³¼ëª©: {subject}
                """
            similar_questions.append(formatted)
        
        state["retrieved_docs"] = results
        state["similar_questions_text"] = "\n\n".join(similar_questions) 

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ.")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state

    def _generate_solution(self, state: SolutionState) -> SolutionState:

        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm_gen = self._llm(0.5)  

        similar_problems = state.get("similar_questions_text", "")
        print("ìœ ì‚¬ ë¬¸ì œë“¤:\n", similar_problems[:100])

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_input_txt']}

            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {similar_problems}

            1. ì‚¬ìš©ê°€ì ì…ë ¥í•œ ë¬¸ì œì˜ **ì •ë‹µ**ì˜ ë³´ê¸° ë²ˆí˜¸ë¥¼ ì •ë‹µìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
            3. ì´ ë¬¸ì œì˜ ê³¼ëª©ì„ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ê³¼ëª© 5ê°œ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒìœ¼ë¡œ ì§€ì •í•´ ì£¼ì„¸ìš”. ìœ ì‚¬ ë¬¸ì œë“¤ì˜ ê³¼ëª©ì„ ì°¸ê³ í•´ë„ ì¢‹ìŠµë‹ˆë‹¤. [ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„, ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•, í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©, ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬]

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
            ê³¼ëª©: ...
        """

        response = llm_gen.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        subject_match = re.search(r"ê³¼ëª©:\s*(.+)", result)
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["generated_subject"] = subject_match.group(1).strip() if subject_match else "ê¸°íƒ€"

        state["chat_history"].append(f"Q: {state['user_input_txt']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state

    # âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

    def _validate_solution(self, state: SolutionState) -> SolutionState:
        print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
        
        llm = self._llm(0)

        validation_prompt = f"""
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_input_txt']}

        ë¬¸ì œ ì§ˆë¬¸: {state['user_problem']}
        ë¬¸ì œ ë³´ê¸°: {state['user_problem_options']}

        ìƒì„±ëœ ì •ë‹µ: {state['generated_answer']}
        ìƒì„±ëœ í’€ì´: {state['generated_explanation']}
        ìƒì„±ëœ ê³¼ëª©: {state['generated_subject']}

        ìƒì„±ëœ í•´ë‹µê³¼ í’€ì´, ê³¼ëª©ì´ ë¬¸ì œì™€ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
        ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        validation_response = llm.invoke(validation_prompt)
        result_text = validation_response.content.strip().lower()

        # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
        print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
        state["validated"] = "ë„¤" in result_text
        
        if not state["validated"]:
            state["retry_count"] = state.get("retry_count", 0) + 1
            print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì¬ì‹œë„ {state['retry_count']}/5)")
        else:
            print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
            
        return state


    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:
        vectorstore = state.get("vectorstore")
        q    = state.get("user_problem", "") or ""
        opts = state.get("user_problem_options", []) or []

        from langchain_core.documents import Document
        import json, hashlib

        # ---------- helpers ----------
        norm = lambda s: " ".join((s or "").split()).strip()

        def parse_opts(v):
            if isinstance(v, str):
                try: v = json.loads(v)
                except: v = [v]
            return [norm(str(x)) for x in (v or [])]

        def doc_id_of(q, opts):
            base = norm(q) + "||" + "||".join(parse_opts(opts))
            return hashlib.sha1(base.encode()).hexdigest()

        def _clean_str(v):
            if isinstance(v, (bytes, bytearray)):
                try: v = v.decode("utf-8", "ignore")
                except Exception: v = str(v)
            if isinstance(v, str):
                s = v.strip()
                if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
                    try:
                        u = json.loads(s)
                        if isinstance(u, str): s = u.strip()
                    except Exception:
                        pass
                if s.lower() in ("", "null", "none"):
                    return ""
                return s
            return v

        def _is_blank(v):
            v = _clean_str(v)
            if v is None: return True
            if isinstance(v, str): return v == ""
            try: return len(v) == 0
            except Exception: return False

        def _escape(s: str) -> str:
            # Milvus exprìš© ì´ìŠ¤ì¼€ì´í”„
            return s.replace("\\", "\\\\").replace('"', r"\"")

        did = doc_id_of(q, opts)

        # ---------- ì™„ì „ ì¼ì¹˜ 1ê°œë§Œ: retrieved_docs[0] ----------
        docs = state.get("retrieved_docs", []) or []
        exact = None
        if docs:
            d = docs[0]
            same_q = norm(d.page_content) == norm(q)
            same_o = parse_opts(d.metadata.get("options", "[]")) == parse_opts(opts)
            print("[DEBUG] exact-match?", same_q and same_o,
                "| Q:", repr(norm(d.page_content)), "==", repr(norm(q)),
                "| OPTS:", parse_opts(d.metadata.get("options", "[]")), "==", parse_opts(opts))
            if same_q and same_o:
                exact = d

        # ---------- ì‚­ì œâ†’ì¶”ê°€ (upsert) ----------
        def upsert(meta, pk_to_delete=None, text_to_delete=None):
            if not vectorstore:
                print("âš ï¸ vectorstore ì—†ìŒ â†’ ì €ì¥ ìŠ¤í‚µ(ê²°ê³¼ë§Œ ê¸°ë¡)")
                return
            # 1) PKë¡œ ì‚­ì œ (ê°€ì¥ ì•ˆì „)
            if pk_to_delete is not None:
                try:
                    vectorstore.delete([pk_to_delete])
                    print(f"[DEBUG] delete by PK ok: {pk_to_delete}")
                except Exception as e:
                    print(f"[DEBUG] delete(ids=[{pk_to_delete}]) ì‹¤íŒ¨: {e}")

            # 2) expr ì‚­ì œ: í•„ë“œëª… í›„ë³´ ìˆœíšŒ (ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆë§ˆë‹¤ ë‹¤ë¦„)
            elif text_to_delete:
                expr_fields = ["text", "page_content", "content", "question"]
                esc = _escape(text_to_delete)
                for f in expr_fields:
                    try:
                        vectorstore.delete(expr=f'{f} == "{esc}"')
                        print(f"[DEBUG] delete by expr ok: {f} == \"{esc}\"")
                        break
                    except Exception as e:
                        print(f"[DEBUG] delete by expr ì‹¤íŒ¨({f}): {e}")

            # ìƒˆ ë¬¸ì„œ ì¶”ê°€
            vectorstore.add_documents([Document(
                page_content=q,
                metadata={
                    # ì°¸ê³ ìš© fingerprint(ìŠ¤í‚¤ë§ˆ í•„ë“œëŠ” ì•„ë‹˜)
                    "doc_id": did,
                    "options": json.dumps(opts, ensure_ascii=False),
                    "answer":      meta.get("answer", "") or "",
                    "explanation": meta.get("explanation", "") or "",
                    "subject":     meta.get("subject", "") or "",
                },
            )])

        if exact:
            meta = exact.metadata.copy()
            updated = False

            new_answer      = _clean_str(state.get("generated_answer"))
            new_explanation = _clean_str(state.get("generated_explanation"))
            new_subject     = _clean_str(state.get("generated_subject"))

            for k, new_val in [("answer", new_answer),
                            ("explanation", new_explanation),
                            ("subject", new_subject)]:
                cur_val   = meta.get(k)
                cur_blank = _is_blank(cur_val)
                new_blank = _is_blank(new_val)
                print(f"[DEBUG] {k}: current={repr(cur_val)} (blank={cur_blank}) "
                    f"new={repr(new_val)} (blank={new_blank})")
                if cur_blank and not new_blank:
                    meta[k] = new_val
                    updated = True

            if updated:
                # PK ì¶”ì¶œ ì‹œë„ (í™˜ê²½ì— ë”°ë¼ 'pk'/'id'/'_id' ë“±ì¼ ìˆ˜ ìˆìŒ)
                pk = None
                for k in ("pk", "id", "_id", "pk_id", "milvus_id"):
                    if k in exact.metadata:
                        pk = exact.metadata[k]; break
                if pk is not None:
                    upsert(meta, pk_to_delete=pk)
                else:
                    # PK ëª» ì°¾ìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ expr ì‚­ì œ ì‹œë„
                    upsert(meta, text_to_delete=q)
                print("âœ… ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ë¹ˆ ì»¬ëŸ¼ë§Œ ì±„ì›Œ ê°±ì‹ ")
            else:
                print("âš ï¸ ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ì¡´ì¬, ì €ì¥ ìƒëµ")
        else:
            upsert({
                "answer": _clean_str(state.get("generated_answer")),
                "explanation": _clean_str(state.get("generated_explanation")),
                "subject": _clean_str(state.get("generated_subject")),
            }, text_to_delete=q)
            print("ğŸ†• ì‹ ê·œ ë¬¸í•­ ì €ì¥")

        # ---------- ê²°ê³¼ ê¸°ë¡ ----------
        state.setdefault("results", []).append({
            "user_problem": q,
            "user_problem_options": opts,
            "generated_answer": state.get("generated_answer", ""),
            "generated_explanation": state.get("generated_explanation", ""),
            "generated_subject": state.get("generated_subject", ""),
            "validated": state.get("validated", False),
            "chat_history": state.get("chat_history", []),
        })
        return state




    def invoke(
            self, 
            user_input_txt: str,
            user_problem: str,
            user_problem_options: List[str],
            vectorstore: Optional[Milvus] = None,
            recursion_limit: int = 1000,
        ) -> Dict:

        # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if vectorstore is None:
            try:
                embedding_model = HuggingFaceEmbeddings(
                    model_name="jhgan/ko-sroberta-multitask",
                    model_kwargs={"device": "cpu"}
                )

                if "default" in connections.list_connections():
                    connections.disconnect("default")
                connections.connect(alias="default", host="localhost", port="19530")

                vectorstore = Milvus(
                    embedding_function=embedding_model,
                    collection_name="problems",
                    connection_args={"host": "localhost", "port": "19530"}
                )
                print("âœ… Milvus ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
                print("   - ë²¡í„°ìŠ¤í† ì–´ ì—†ì´ ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤.")
                vectorstore = None
        
        
        initial_state: SolutionState = {
            "user_input_txt": user_input_txt,

            "user_problem": user_problem,
            "user_problem_options": user_problem_options,

            "vectorstore": vectorstore,

            "retrieved_docs": [],
            "similar_questions_text": "",

            "generated_answer": "",
            "generated_explanation": "",
            "generated_subject": "",
            "validated": False,
            "retry_count": 0,

            "results": [],
            
            "chat_history": []
        }
        
        final_state = self.graph.invoke(initial_state, config={"recursion_limit": recursion_limit})
        
        # ê·¸ë˜í”„ ì‹œê°í™”
        # try:
        #     graph_image_path = "solution_agent_workflow.png"
        #     with open(graph_image_path, "wb") as f:
        #         f.write(self.graph.get_graph().draw_mermaid_png())
        #     print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # except Exception as e:
        #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        #     print("ì›Œí¬í”Œë¡œìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

        # ê²°ê³¼ í™•ì¸ ë° ë””ë²„ê¹…
        results = final_state.get("results", [])
        print(f"   - ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        
        if results:
            for i, result in enumerate(results):
                print(f"   - ê²°ê³¼ {i+1}: {result.get('question', '')[:30]}...")
        else:
            print("   âš ï¸ resultsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            print(f"   - final_state ë‚´ìš©: {final_state}")
        
        return final_state


if __name__ == "__main__":

    # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )

    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port":"19530"}
    )

    agent = SolutionAgent()

    # ê·¸ë˜í”„ ì‹œê°í™” (ì„ íƒ)
    # try:
    #     graph_image_path = "solution_agent_workflow.png"
    #     with open(graph_image_path, "wb") as f:
    #         f.write(agent.graph.get_graph().draw_mermaid_png())
    #     print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    #     print("ì›Œí¬í”Œë¡œìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

    user_input_txt = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
    user_problem = input("\nâ“ ì‚¬ìš©ì ë¬¸ì œ: ").strip()
    user_problem_options_raw = input("\nâ“ ì‚¬ìš©ì ë³´ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
    user_problem_options = [opt.strip() for opt in user_problem_options_raw.split(",") if opt.strip()]

    final_state = agent.execute(
        user_input_txt=user_input_txt,
        user_problem=user_problem,
        user_problem_options=user_problem_options,
    )

    # # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    # results = final_state.get("results", [])
    # results_data = {
    #     "timestamp": datetime.now().isoformat(),
    #     "user_input_txt": final_state.get("user_input_txt",""),
    #     "total_results": len(results),
    #     "results": results,
    # }

    # results_filename = os.path.join(f"solution_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    # os.makedirs(os.path.dirname(results_filename), exist_ok=True)
    # with open(results_filename, "w", encoding="utf-8") as f:
    #     json.dump(results_data, f, ensure_ascii=False, indent=2)
    # print(f"âœ… í•´ë‹µ ê²°ê³¼ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_filename}")
