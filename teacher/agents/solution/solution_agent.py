import os
from typing import TypedDict, List, Dict, Optional, Tuple, Any
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections, Collection, DataType
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re
from langchain_openai import ChatOpenAI
from ..base_agent import BaseAgent
from langchain_community.retrievers import BM25Retriever
import numpy as np
import asyncio, sys
from concurrent.futures import ThreadPoolExecutor
import copy
from datasets import Dataset, Features, Value, Sequence
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
import json
import os, json, glob
from datetime import datetime
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from pymilvus import connections, Collection
import numpy as _np
import pandas as _pd

try:
    from rank_bm25 import BM25Okapi  # optional fallback(bm25 ì¸ë±ìŠ¤ ì—†ì´ í›„ë³´êµ° ìœ„ì—ì„œ sparse ìŠ¤ì½”ì–´ë§)
    HAS_RANK_BM25 = True
except Exception:
    HAS_RANK_BM25 = False

try:
    from sentence_transformers import CrossEncoder
    HAS_CROSS_ENCODER = True
except Exception:
    HAS_CROSS_ENCODER = False



# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GROQAI_API_KEY = os.getenv("GROQAI_API_KEY", "")
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))

ragas_llm = ChatOpenAI(
    api_key=GROQAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    model=OPENAI_LLM_MODEL,
    temperature=LLM_TEMPERATURE,
    max_tokens=min(LLM_MAX_TOKENS, 2048),
)

ragas_emb = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={"device": "cpu"}
)

# âœ… ìƒíƒœ ì •ì˜
class SolutionState(TypedDict):
    # ì‚¬ìš©ì ì…ë ¥
    user_input_txt: str

    # ë¬¸ì œë¦¬ìŠ¤íŠ¸, ë¬¸ì œ, ë³´ê¸°
    user_problem: str
    user_problem_options: List[str]
    
    vectorstore_p: Milvus
    vectorstore_c: Milvus

    retrieved_docs: List[Document]
    problems_contexts_text : str
    concept_contexts: List[Document]
    concept_contexts_text: str

    # ë¬¸ì œ í•´ë‹µ/í’€ì´/ê³¼ëª© ìƒì„±
    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    generated_subject: str

    results: List[Dict]
    validated: bool
    retry_count: int             # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    chat_history: List[str]
    
class SolutionAgent(BaseAgent):
    """ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        # --- í•˜ì´ë¸Œë¦¬ë“œ/ë¦¬ë­í¬ íŒŒë¼ë¯¸í„° ---
        self.RETRIEVAL_FETCH_K = int(os.getenv("RETRIEVAL_FETCH_K", "30"))
        self.HYBRID_TOPK       = int(os.getenv("HYBRID_TOPK", "12"))
        self.RERANK_TOPK       = int(os.getenv("RERANK_TOPK", "5"))
        self.HYBRID_ALPHA      = float(os.getenv("HYBRID_ALPHA", "0.5"))

        # --- ë°˜ë“œì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±í•´ ë‘ê¸° ---
        self.bm25_retriever = None      # â† ì—†ìœ¼ë©´ AttributeError
        self.reranker = None            # â† ë¦¬ë­ì»¤ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’
        self.rerank_model_name = os.getenv("RERANK_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")

        try:
            self.reranker = CrossEncoder(self.rerank_model_name, device=os.getenv("RERANK_DEVICE","cpu"))
            print(f"[Rerank] CrossEncoder loaded: {self.rerank_model_name}")
        except Exception as e:
            print(f"[Rerank] load skipped: {e}")

        # (ì„ íƒ) BM25 ë§ë­‰ì¹˜ê°€ ìˆë‹¤ë©´ ë¡œë“œ
        bm25_jsonl = os.getenv("BM25_CORPUS_JSONL")
        if bm25_jsonl and os.path.exists(bm25_jsonl):
            docs = []
            with open(bm25_jsonl, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                        docs.append(Document(page_content=obj.get("page_content",""),
                                             metadata=obj.get("metadata",{})))
                    except Exception:
                        pass
            if docs:
                self.bm25_retriever = BM25Retriever.from_documents(docs)
                print(f"[BM25] ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {len(docs)}")

        self.vectorstore_p = None
        self.vectorstore_c = None
        self.graph = self._create_graph()


    def _ensure_vectorstores(
        self,
        host: str = "localhost",
        port: str = "19530",
        coll_p: str = "problems",
        coll_c: str = "concept_summary",
        model_name: str = "jhgan/ko-sroberta-multitask",
    ):
        # 1) ì´ë²¤íŠ¸ ë£¨í”„ ë³´ì¥
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            if sys.platform.startswith("win"):
                try:
                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
                except Exception:
                    pass
            asyncio.set_event_loop(asyncio.new_event_loop())

        # 2) ë™ê¸° http ìŠ¤í‚´ìœ¼ë¡œ ì—°ê²°
        if "default" not in connections.list_connections():
            connections.connect(alias="default", uri=f"http://{host}:{port}")

        def infer_fields(coll_name: str):
            c = Collection(coll_name)
            vec_field, text_field, dim = None, None, None

            # ë²¡í„° í•„ë“œ(FLOAT_VECTOR) ì°¾ê¸°
            for f in c.schema.fields:
                if f.dtype == DataType.FLOAT_VECTOR and vec_field is None:
                    vec_field = f.name
                    try:
                        dim = int(f.params.get("dim") or 0)
                    except Exception:
                        dim = None

            # í…ìŠ¤íŠ¸ í•„ë“œëŠ” í›„ë³´êµ°ì—ì„œ ìš°ì„  ì„ íƒ, ì—†ìœ¼ë©´ ì²« VARCHAR ì‚¬ìš©
            candidates = ("text", "page_content", "content", "question", "item_title", "title")
            varchar_fields = [f.name for f in c.schema.fields if f.dtype == DataType.VARCHAR]
            for name in candidates:
                if name in varchar_fields:
                    text_field = name
                    break
            if text_field is None and varchar_fields:
                text_field = varchar_fields[0]

            if vec_field is None:
                raise RuntimeError(f"[Milvus] '{coll_name}'ì— FLOAT_VECTOR í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"[Milvus] '{coll_name}' â†’ text_field='{text_field}', vector_field='{vec_field}', dim={dim}")
            return text_field, vec_field, dim

        emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={"device": "cpu"})

        if self.vectorstore_p is None:
            txt_p, vec_p, _ = infer_fields(coll_p)
            self.vectorstore_p = Milvus(
                embedding_function=emb,
                collection_name=coll_p,
                connection_args={"uri": f"http://{host}:{port}"},
                text_field=txt_p,
                vector_field=vec_p,   # â† ìë™ ê°ì§€ëœ ì´ë¦„ ì‚¬ìš© (ëŒ€ê°œ 'vector')
            )
            print(f"âœ… Milvus '{coll_p}' ì—°ê²° OK (text_field={txt_p}, vector_field={vec_p})")

        if self.vectorstore_c is None:
            txt_c, vec_c, _ = infer_fields(coll_c)
            self.vectorstore_c = Milvus(
                embedding_function=emb,
                collection_name=coll_c,
                connection_args={"uri": f"http://{host}:{port}"},
                text_field=txt_c,     # concept_summaryë¼ë©´ ë³´í†µ 'content'
                vector_field=vec_c,   # â† ì—¬ê¸°ì„œ ë°˜ë“œì‹œ 'embedding'ìœ¼ë¡œ ì¡í ê²ƒ
            )
            print(f"âœ… Milvus '{coll_c}' ì—°ê²° OK (text_field={txt_c}, vector_field={vec_c})")

        
    @property
    def name(self) -> str:
        return "SolutionAgent"

    @property
    def description(self) -> str:
        return "ì‹œí—˜ë¬¸ì œë¥¼ ì¸ì‹í•˜ì—¬ ë‹µê³¼ í’€ì´, í•´ì„¤ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."

    def _llm(self, temperature: float = 0):
        return ChatOpenAI(
            api_key=GROQAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            model=OPENAI_LLM_MODEL,
            temperature=temperature,
            max_tokens=min(LLM_MAX_TOKENS, 2048),
        )
    
    def _build_concept_query(self, problem: str, options: List[str]) -> str:
        opts = "\n".join([f"{i+1}) {o}" for i, o in enumerate(options or [])])
        return f"{(problem or '').strip()}\n{opts}"
    
    @staticmethod
    def _safe_eval_metric(ds, metric, llm_obj, emb_obj, name: str) -> float:
        from ragas import evaluate
        try:
            res = evaluate(ds, metrics=[metric], llm=llm_obj, embeddings=emb_obj)
            score = 0.0
            if hasattr(res, "scores"):
                sc = res.scores.get(name, 0.0)
                if isinstance(sc, (list, tuple)):
                    score = float(sc[0]) if len(sc) else 0.0
                else:
                    try:
                        import numpy as _np, pandas as _pd
                        if isinstance(sc, _np.ndarray):
                            score = float(sc.item() if sc.size == 1 else sc[0])
                        elif isinstance(sc, _pd.Series):
                            score = float(sc.iloc[0])
                        else:
                            score = float(sc)
                    except Exception:
                        score = float(sc) if sc is not None else 0.0
            elif hasattr(res, "to_pandas"):
                df_res = res.to_pandas()
                if name in df_res.columns and len(df_res) > 0:
                    score = float(df_res[name].iloc[0])
            print(f"[RAGAS:{name}] {score:.3f}")
            return score
        except Exception as e:
            print(f"[RAGAS:{name}] ì‹¤íŒ¨ â†’ 0.0 ì²˜ë¦¬ ({repr(e)})")
            return 0.0

    
    #----------------------------------------create graph------------------------------------------------------

    def _create_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        # âœ… LangGraph êµ¬ì„±
        print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ê³µí†µ ì²˜ë¦¬
        graph.add_node("search_problems", self._search_similar_problems)
        graph.add_node("search_concepts", self._search_concepts_summary)
        graph.add_node("retrieve_parallel", self._retrieve_parallel)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        graph.add_node("store", self._store_to_vector_db)

        graph.set_entry_point("retrieve_parallel")
        graph.add_edge("retrieve_parallel", "generate_solution")
        graph.add_edge("generate_solution", "validate")
        graph.add_edge("store", END)

        graph.add_conditional_edges(
            "validate", 
            # ê²€ì¦ ì‹¤íŒ¨ â†’ retry<5ì´ë©´ back, ì•„ë‹ˆë©´ ê·¸ëƒ¥ storeë¡œ ì§„í–‰
            lambda s: "ok" if s["validated"] else ("back" if s.get("retry_count", 0) < 5 else "force_store"),
            {"ok": "store", "back": "generate_solution", "force_store": "store"}
        )

        return graph.compile()
    
    #----------------------------------------nodes------------------------------------------------------

    def _search_similar_problems(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        print(state["user_problem"], state["user_problem_options"])
            
        vectorstore_p = state.get("vectorstore_p")

        if vectorstore_p is None:
            print("âš ï¸ vectorstore_pì—†ìŒ â†’ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ê±´ë„ˆëœ€")
            state["retrieved_docs"] = []
            state["problems_contexts_text"] = ""
            return state

        # q = state["user_problem"]
        q = self._build_concept_query(state.get("user_problem",""), state.get("user_problem_options", []))

        # ---------- (1) Dense í›„ë³´ ë„‰ë„‰íˆ ìˆ˜ì§‘ ----------
        try:
            dense_scored = vectorstore_p.similarity_search_with_score(q, k=self.RETRIEVAL_FETCH_K)
            dense_docs = [d for d, _ in dense_scored]
            dense_scores = {id(d): float(s) for d, s in dense_scored}
            print(f"[Dense] fetched: {len(dense_docs)}")
        except Exception as e:
            print(f"[Dense] similarity_search_with_score ì‹¤íŒ¨ â†’ {e} â†’ score ì—†ì´ fallback")
            dense_docs = vectorstore_p.similarity_search(q, k=self.RETRIEVAL_FETCH_K)
            dense_scores = {id(d): 1.0/(r+1) for r, d in enumerate(dense_docs)}

        # ---------- (2) Sparse í›„ë³´(BM25) ê²°í•© ----------
        sparse_docs = []
        sparse_scores = {}

        if self.bm25_retriever is not None:
            try:
                sparse_docs = self.bm25_retriever.get_relevant_documents(q)[:self.RETRIEVAL_FETCH_K]
                for r, d in enumerate(sparse_docs):
                    sparse_scores[id(d)] = 1.0/(r+1)
                print(f"[BM25] fetched: {len(sparse_docs)}")
            except Exception as e:
                print(f"[BM25] ì‹¤íŒ¨ â†’ {e}")

        elif HAS_RANK_BM25 and dense_docs:
            try:
                def tok(s: str) -> List[str]:
                    return re.findall(r"[ê°€-í£A-Za-z0-9_]+", (s or "").lower())
                corpus_toks = [tok(d.page_content) for d in dense_docs]
                bm25 = BM25Okapi(corpus_toks)
                q_scores = bm25.get_scores(tok(q))
                if q_scores is not None and len(q_scores) == len(dense_docs):
                    min_s, max_s = float(min(q_scores)), float(max(q_scores))
                    rng = (max_s - min_s) or 1.0
                    for d, s in zip(dense_docs, q_scores):
                        sparse_scores[id(d)] = (float(s) - min_s) / rng
                print(f"[BM25-lite] computed over dense pool: {len(dense_docs)}")
            except Exception as e:
                print(f"[BM25-lite] ì‹¤íŒ¨ â†’ {e}")

        # ---------- (3) Dense + Sparse ì•™ìƒë¸” ----------
        def _safe_meta_str(md: Dict[str, Any]) -> str:
            try:
                norm = {
                    str(k): (
                        v.item() if hasattr(v, "item") else (
                            str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v
                        )
                    )
                    for k, v in (md or {}).items()
                }
                return json.dumps(norm, ensure_ascii=False, sort_keys=True)
            except Exception:
                try:
                    return str({k: str(v) for k, v in (md or {}).items()})
                except Exception:
                    return ""

        def key_of(doc: Document) -> Tuple[str, str]:
            return ((doc.page_content or "")[:150], _safe_meta_str(doc.metadata)[:150])

        pool: Dict[Tuple[str,str], Dict[str, Any]] = {}
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            wd = 1.0/(r+1)
            wd = max(wd, dense_scores.get(id(d), 0.0))
            pool[k]["dense"] = max(pool[k]["dense"], wd)

        for r, d in enumerate(sparse_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            ws = 1.0/(r+1)
            ws = max(ws, sparse_scores.get(id(d), 0.0))
            pool[k]["sparse"] = max(pool[k]["sparse"], ws)

        alpha = self.HYBRID_ALPHA
        scored = []
        for k, v in pool.items():
            score = alpha * v["dense"] + (1.0 - alpha) * v["sparse"]
            scored.append((v["doc"], score))

        scored.sort(key=lambda x: x[1], reverse=True)
        hybrid_top = [d for d, _ in scored[:self.HYBRID_TOPK]]
        print(f"[Hybrid] pool={len(pool)} â†’ top{self.HYBRID_TOPK} ì„ ì •")

        # ---------- (4) Cross-Encoder rerank ----------
        try:
            if self.reranker is not None and len(hybrid_top) > 0:
                pairs = [[q, d.page_content] for d in hybrid_top]
                scores = self.reranker.predict(pairs)
                order = sorted(range(len(hybrid_top)), key=lambda i: float(scores[i]), reverse=True)
                reranked = [hybrid_top[i] for i in order[:self.RERANK_TOPK]]
                print(f"[Rerank] ìµœì¢… top{self.RERANK_TOPK} (CrossEncoder)")
            else:
                reranked = hybrid_top[:self.RERANK_TOPK]
                print("[Rerank] ì‚¬ìš© ì•ˆ í•¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        except Exception as e:
            print(f"[Rerank] ì‹¤íŒ¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©: {e}")
            reranked = hybrid_top[:self.RERANK_TOPK]

        # ---------- (4.5) Near-duplicate ì œê±° & ìƒí•œ 3ê°œ ----------
        MAX_KEEP = getattr(self, "PROBLEM_MAXK", 3)              # ìµœì¢… ìœ ì§€ ê°œìˆ˜ (ê¸°ë³¸ 3)
        SIM_THRESHOLD = getattr(self, "PROBLEM_SIM_THRESHOLD", 0.82)  # ìœ ì‚¬ë„ ì„ê³„ê°’

        def _norm_text(s: str) -> str:
            s = (s or "").lower().strip()
            s = re.sub(r"\s+", " ", s)
            return s

        def _tokens(s: str) -> set:
            return set(re.findall(r"[ê°€-í£a-z0-9]{2,}", _norm_text(s)))

        def _char_ngrams(s: str, n: int = 5) -> set:
            t = _norm_text(s)
            return set(t[i:i+n] for i in range(max(0, len(t) - n + 1)))

        def _jaccard(a: set, b: set) -> float:
            if not a or not b:
                return 0.0
            inter = len(a & b)
            if inter == 0:
                return 0.0
            return inter / float(len(a | b))

        def _similar(a_txt: str, b_txt: str) -> float:
            A_tok, B_tok = _tokens(a_txt), _tokens(b_txt)
            A_ng,  B_ng  = _char_ngrams(a_txt, 5), _char_ngrams(b_txt, 5)
            j_tok = _jaccard(A_tok, B_tok)
            j_ng  = _jaccard(A_ng,  B_ng)
            return 0.5 * (j_tok + j_ng)

        # ë¬¸ì œ ë³¸ë¬¸(content) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°(ë³´ê¸°/ë©”íƒ€ëŠ” ë³´ì¡°)
        deduped = []
        for d in reranked:
            txt = (d.page_content or "").strip()
            if not txt:
                continue
            is_dup = False
            for kept in deduped:
                kept_txt = (kept.page_content or "").strip()
                if _similar(txt, kept_txt) >= SIM_THRESHOLD:
                    is_dup = True
                    break
            if not is_dup:
                deduped.append(d)
            if len(deduped) >= MAX_KEEP:
                break

        print(f"[Dedup(problems)] reranked={len(reranked)} â†’ deduped={len(deduped)} (max={MAX_KEEP}, thr={SIM_THRESHOLD})")
        results = deduped  # ìµœì¢… ë¬¸ì„œë“¤ (ìµœëŒ€ 3ê°œ)

        # ---------- (5) ê¸°ì¡´ í¬ë§·/ì €ì¥ ----------
        similar_questions = []
        for i, doc in enumerate(results, start=1):
            metadata = doc.metadata or {}
            options = json.loads(metadata.get("options", "[]" )) if isinstance(metadata.get("options"), str) else (metadata.get("options", []) or [])
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")
            subject = metadata.get("subject", "ê¸°íƒ€")

            # ì •ë‹µ ë²ˆí˜¸ â†’ í…ìŠ¤íŠ¸
            answer_text = ""
            try:
                answer_idx = int(answer) - 1
                if 0 <= answer_idx < len(options):
                    answer_text = options[answer_idx]
            except Exception:
                pass

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer} ({answer_text})
                í’€ì´: {explanation}
                ê³¼ëª©: {subject}
                """
            similar_questions.append(formatted)

        state["retrieved_docs"] = results
        state["problems_contexts_text"] = "\n\n".join(similar_questions)

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ (dense fetch={len(dense_docs)}, hybrid_pool={len(pool)})")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state

    
    def _search_concepts_summary(self, state: SolutionState) -> SolutionState:
        print("\nğŸ“š [1-í™•ì¥] ê°œë… ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘")

        vectorstore_c = state.get("vectorstore_c")
        if vectorstore_c is None:
            print("âš ï¸ vectorstore_c ì—†ìŒ â†’ ê°œë… ê²€ìƒ‰ ê±´ë„ˆëœ€")
            state["concept_contexts"], state["concept_contexts_text"] = [], ""
            return state

        q = self._build_concept_query(
            state.get("user_problem", ""),
            state.get("user_problem_options", []),
        )

        # ---------- (1) Dense í›„ë³´ ë„‰ë„‰íˆ ìˆ˜ì§‘ ----------
        try:
            dense_scored = vectorstore_c.similarity_search_with_score(q, k=self.RETRIEVAL_FETCH_K)
            dense_docs   = [d for d, _ in dense_scored]
            dense_scores = {id(d): float(s) for d, s in dense_scored}
            print(f"[Dense(concepts)] fetched: {len(dense_docs)}")
        except Exception as e:
            print(f"[Dense(concepts)] similarity_search_with_score ì‹¤íŒ¨ â†’ {e} â†’ score ì—†ì´ fallback")
            dense_docs   = vectorstore_c.similarity_search(q, k=self.RETRIEVAL_FETCH_K)
            dense_scores = {id(d): 1.0/(r+1) for r, d in enumerate(dense_docs)}

        # ---------- (2) Sparse í›„ë³´ ê²°í•© (BM25-lite over dense pool) ----------
        # ê°œë… ì½”í¼ìŠ¤ìš© ë³„ë„ BM25 ì¸ë±ìŠ¤ê°€ ì—†ë‹¤ë©´, dense í›„ë³´êµ° ìœ„ì—ì„œë§Œ BM25 ì ìˆ˜ ê·¼ì‚¬
        sparse_scores = {}
        try:
            if dense_docs and HAS_RANK_BM25:
                def tok(s: str) -> List[str]:
                    return re.findall(r"[ê°€-í£A-Za-z0-9_]+", (s or "").lower())
                corpus_toks = [tok(d.page_content) for d in dense_docs]
                bm25 = BM25Okapi(corpus_toks)
                q_scores = bm25.get_scores(tok(q))
                # 0~1 ì •ê·œí™”
                if q_scores is not None and len(q_scores) == len(dense_docs):
                    min_s, max_s = float(min(q_scores)), float(max(q_scores))
                    rng = (max_s - min_s) or 1.0
                    for d, s in zip(dense_docs, q_scores):
                        sparse_scores[id(d)] = (float(s) - min_s) / rng
                print(f"[BM25-lite(concepts)] computed over dense pool: {len(dense_docs)}")
            else:
                print("[BM25-lite(concepts)] ê±´ë„ˆëœ€ (dense_docs ë¹„ì—ˆê±°ë‚˜ rank_bm25 ë¯¸ì„¤ì¹˜)")
        except Exception as e:
            print(f"[BM25-lite(concepts)] ì‹¤íŒ¨ â†’ {e}")

        # ---------- (3) Dense + Sparse ì•™ìƒë¸” ----------
        def _safe_meta_str(md: Dict[str, Any]) -> str:
            try:
                norm = {
                    str(k): (
                        v.item() if hasattr(v, "item") else (
                            str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v
                        )
                    )
                    for k, v in (md or {}).items()
                }
                return json.dumps(norm, ensure_ascii=False, sort_keys=True)
            except Exception:
                try:
                    return str({k: str(v) for k, v in (md or {}).items()})
                except Exception:
                    return ""

        def key_of(doc: Document) -> Tuple[str, str]:
            return ((doc.page_content or "")[:150], _safe_meta_str(doc.metadata)[:150])

        pool: Dict[Tuple[str, str], Dict[str, Any]] = {}
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            wd = max(1.0/(r+1), dense_scores.get(id(d), 0.0))
            pool[k]["dense"] = max(pool[k]["dense"], wd)

        # BM25-lite ì ìˆ˜ë§Œ ì¡´ì¬ (ë³„ë„ sparse_docs ì—†ìŒ)
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            if k not in pool:
                pool[k] = {"doc": d, "dense": 0.0, "sparse": 0.0}
            ws = max(1.0/(r+1), sparse_scores.get(id(d), 0.0))
            pool[k]["sparse"] = max(pool[k]["sparse"], ws)

        alpha = self.HYBRID_ALPHA
        scored = []
        for k, v in pool.items():
            score = alpha * v["dense"] + (1.0 - alpha) * v["sparse"]
            scored.append((v["doc"], score))

        scored.sort(key=lambda x: x[1], reverse=True)
        hybrid_top = [d for d, _ in scored[:self.HYBRID_TOPK]]
        print(f"[Hybrid(concepts)] pool={len(pool)} â†’ top{self.HYBRID_TOPK} ì„ ì •")

        # ---------- (4) Cross-Encoder rerank ----------
        try:
            if self.reranker is not None and len(hybrid_top) > 0:
                pairs  = [[q, d.page_content] for d in hybrid_top]
                scores = self.reranker.predict(pairs)
                order  = sorted(range(len(hybrid_top)), key=lambda i: float(scores[i]), reverse=True)
                reranked = [hybrid_top[i] for i in order[:self.RERANK_TOPK]]
                print(f"[Rerank(concepts)] ìµœì¢… top{self.RERANK_TOPK} (CrossEncoder)")
            else:
                reranked = hybrid_top[:self.RERANK_TOPK]
                print("[Rerank(concepts)] ì‚¬ìš© ì•ˆ í•¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        except Exception as e:
            print(f"[Rerank(concepts)] ì‹¤íŒ¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©: {e}")
            reranked = hybrid_top[:self.RERANK_TOPK]

        # ---------- (4.5) Near-duplicate ì œê±° & ìƒí•œ 3ê°œ ----------
        #   - ê±°ì˜ ê°™ì€ ë¬¸ë‹¨ì´ ì¤‘ë³µë˜ëŠ” í˜„ìƒ ë°©ì§€
        #   - í† í° Jaccard ì™€ 5-gram Jaccard ì˜ í‰ê·  ìœ ì‚¬ë„ê°€ threshold ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        MAX_KEEP = getattr(self, "CONCEPT_MAXK", 3)  # ê¸°ë³¸ 3ê°œë¡œ ì œí•œ
        SIM_THRESHOLD = getattr(self, "CONCEPT_SIM_THRESHOLD", 0.82)

        def _norm_text(s: str) -> str:
            s = (s or "").lower().strip()
            s = re.sub(r"\s+", " ", s)
            return s

        def _tokens(s: str) -> set:
            return set(re.findall(r"[ê°€-í£a-z0-9]{2,}", _norm_text(s)))

        def _char_ngrams(s: str, n: int = 5) -> set:
            t = _norm_text(s)
            return set(t[i:i+n] for i in range(max(0, len(t) - n + 1)))

        def _jaccard(a: set, b: set) -> float:
            if not a or not b:
                return 0.0
            inter = len(a & b)
            if inter == 0:
                return 0.0
            return inter / float(len(a | b))

        def _similar(a_txt: str, b_txt: str) -> float:
            A_tok, B_tok = _tokens(a_txt), _tokens(b_txt)
            A_ng,  B_ng  = _char_ngrams(a_txt, 5), _char_ngrams(b_txt, 5)
            j_tok = _jaccard(A_tok, B_tok)
            j_ng  = _jaccard(A_ng,  B_ng)
            return 0.5 * (j_tok + j_ng)

        deduped = []
        for d in reranked:
            txt = (d.metadata.get("content") if d.metadata else None) or d.page_content or ""
            if not txt.strip():
                continue
            is_dup = False
            for kept in deduped:
                kept_txt = (kept.metadata.get("content") if kept.metadata else None) or kept.page_content or ""
                if _similar(txt, kept_txt) >= SIM_THRESHOLD:
                    is_dup = True
                    break
            if not is_dup:
                deduped.append(d)
            if len(deduped) >= MAX_KEEP:
                break

        print(f"[Dedup(concepts)] reranked={len(reranked)} â†’ deduped={len(deduped)} (max={MAX_KEEP}, thr={SIM_THRESHOLD})")

        final_docs = deduped  # ìµœì¢… ì‚¬ìš© ë¬¸ì„œ(ìµœëŒ€ 3ê°œ)

        # ---------- (5) LLM í”„ë¡¬í”„íŠ¸ìš© ì •ë¦¬ ----------
        chunks, cleaned_docs = [], []
        for idx, d in enumerate(final_docs, start=1):
            md = d.metadata or {}
            content = (md.get("content") or d.page_content or "").strip()
            subject = (md.get("subject") or "").strip()
            if not content and d.page_content:
                content = d.page_content.strip()

            cleaned = Document(page_content=content, metadata={"subject": subject})
            cleaned_docs.append(cleaned)

            chunks.append(f"ê³¼ëª©: {subject}\në‚´ìš©: {content}")
            print(f" - [{idx}] subject='{subject}' content={content[:30]}...")

        state["concept_contexts"] = cleaned_docs
        state["concept_contexts_text"] = "\n\n".join(chunks)
        print(f"ğŸ“š ê°œë… ì»¨í…ìŠ¤íŠ¸ {len(cleaned_docs)}ê°œ ìˆ˜ì§‘")
        return state

    
    def _retrieve_parallel(self, state: SolutionState) -> SolutionState:
        # stateë¥¼ ë³µì‚¬í•´ì„œ ê° ì‘ì—…ì´ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì •í•˜ë„ë¡ í•¨
        s1 = copy.deepcopy(state)
        s2 = copy.deepcopy(state)

        with ThreadPoolExecutor(max_workers=2) as ex:
            f_sim = ex.submit(self._search_similar_problems, s1)
            f_con = ex.submit(self._search_concepts_summary, s2)
            r_sim = f_sim.result()
            r_con = f_con.result()

        # ê²°ê³¼ í•©ì¹˜ê¸°
        state["retrieved_docs"]        = r_sim.get("retrieved_docs", [])
        state["problems_contexts_text"]= r_sim.get("problems_contexts_text", "")
        state["concept_contexts"]      = r_con.get("concept_contexts", [])
        state["concept_contexts_text"] = r_con.get("concept_contexts_text", "")

        # ë””ë²„ê·¸ ë¡œê·¸
        print(f"[Parallel] similar_problems={len(state['retrieved_docs'])}, "
            f"similar_concepts={len(state['concept_contexts'])}")
        return state



    def _generate_solution(self, state: SolutionState) -> SolutionState:

        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm_gen = self._llm(0.5)  

        problems_ctx = state.get("problems_contexts_text", "")
        concept_ctx  = state.get("concept_contexts_text", "")

        def preview_context(ctx_text: str, label: str):
            print(f"{label} ì „ì²´ ê¸¸ì´: {len(ctx_text)}")
            if not ctx_text.strip():
                print(f"{label}: (ë¹„ì–´ ìˆìŒ)")
                return
            # ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë‘ ì¤„ ê°œí–‰ ê¸°ì¤€)
            blocks = [b.strip() for b in ctx_text.split("\n\n") if b.strip()]
            for i, b in enumerate(blocks[:3], 1):   # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
                first_line = b.splitlines()[0]
                print(f" - {label} {i}: {first_line[:120]}...")  # ì• 120ìë§Œ
            if len(blocks) > 3:
                print(f" ... (ì´ {len(blocks)}ê°œ ì¤‘ ìƒìœ„ 3ê°œë§Œ í‘œì‹œ)")

        # ì¶œë ¥
        preview_context(problems_ctx, "ìœ ì‚¬ë¬¸ì œ")
        preview_context(concept_ctx, "ê°œë…ì»¨í…ìŠ¤íŠ¸")

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_input_txt']}

            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {problems_ctx}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ê´€ë ¨ëœ ê°œë… ìš”ì•½:
            {concept_ctx}


            1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œì˜ **ì •ë‹µ**ì„ ì˜ ë³´ê¸° ë²ˆí˜¸ë¥¼ ì •ë‹µìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
            3. ì´ ë¬¸ì œì˜ ê³¼ëª©ì„ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ê³¼ëª© 5ê°œ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒìœ¼ë¡œ ì§€ì •í•´ ì£¼ì„¸ìš”.
                [ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„, ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•, í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©, ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬]
                (ìœ ì‚¬ë¬¸ì œì™€ ê°œë… ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ì˜ ê³¼ëª©ì„ ì°¸ê³ í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
            ê³¼ëª©: ...
        """

        response = llm_gen.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        subject_match = re.search(r"ê³¼ëª©:\s*(.+)", result)
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["generated_subject"] = subject_match.group(1).strip() if subject_match else "ê¸°íƒ€"

        state["chat_history"].append(f"Q: {state['user_input_txt']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state


    def _validate_solution(self, state: SolutionState) -> SolutionState:
        """
        RAGAS ê²€ì¦ (ì»¨í…ìŠ¤íŠ¸ ë¬´ê°€ê³µ):
        - faithfulness / answer_relevancy
        - ê²€ì¦ìš© LLMì€ ë„‰ë„‰í•œ max_tokens/timeout
        - ìŠ¤í‚¤ë§ˆ ëª…ì‹œ + ì§€í‘œ ë¶„ë¦¬ í‰ê°€
        """
        print("\nğŸ§ [3ë‹¨ê³„] RAGAS ê¸°ë°˜ ì •í•©ì„± ê²€ì¦ ì‹œì‘ (ì»¨í…ìŠ¤íŠ¸ ë¬´ê°€ê³µ)")

        # --- í•„ìš”í•œ import (íŒŒì¼ ìƒë‹¨ì— ì´ë¯¸ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°) ---
        import os
        from datasets import Dataset, Features, Value, Sequence
        from ragas import evaluate
        from ragas.metrics import faithfulness, answer_relevancy
        from langchain_openai import ChatOpenAI
        from langchain_huggingface import HuggingFaceEmbeddings

        # ì„ê³„ê°’
        f_min = float(os.getenv("VALIDATE_FAITHFULNESS_MIN", "0.65"))
        a_min = float(os.getenv("VALIDATE_ANS_REL_MIN", "0.55"))

        # ì§ˆì˜/ë‹µë³€ í…ìŠ¤íŠ¸
        question = (state.get("user_input_txt") or "").strip()
        prob = (state.get("user_problem") or "").strip()
        opts = state.get("user_problem_options") or []
        if opts:
            opts_blob = "\n".join(f"{i+1}) {o}" for i, o in enumerate(opts))
            question = f"{question}\n\n[ë¬¸ì œ]\n{prob}\n\n[ë³´ê¸°]\n{opts_blob}"

        answer = (
            f"ì •ë‹µ: {state.get('generated_answer','')}\n"
            f"í’€ì´: {state.get('generated_explanation','')}\n"
            f"ê³¼ëª©: {state.get('generated_subject','')}"
        )

        # ì»¨í…ìŠ¤íŠ¸(ë¬´ê°€ê³µ)
        raw_context_candidates = [
            state.get("generation_context_text"),
            state.get("problems_contexts_text"),
            state.get("concept_contexts_text"),
            state.get("extra_context_text"),
        ]
        ctxs = [c for c in raw_context_candidates if isinstance(c, str) and len(c) > 0]

        if not ctxs:
            print("âš ï¸ RAGAS ê²€ì¦ ìŠ¤í‚µ: ìƒì„± ì‹œ ì „ë‹¬ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
            state["validated"] = False
            state["retry_count"] = state.get("retry_count", 0) + 1
            return state

        print(f"[Validate] using {len(ctxs)} raw context block(s) exactly as used for generation")

        # ---------- í‰ê°€ ì…ë ¥ (ìŠ¤í‚¤ë§ˆ ëª…ì‹œ) ----------
        features = Features({
            "question": Value("string"),
            "answer":   Value("string"),
            "contexts": Sequence(Value("string")),
        })
        # ë¬¸ìì—´ ê°•ì œ
        ctxs = [str(c) for c in ctxs]
        if any(len(c.strip()) == 0 for c in ctxs):
            print("[Validate] ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ì´ í¬í•¨ë˜ì–´ ìˆì–´ RAGASê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        data = {"question":[question], "answer":[answer], "contexts":[ctxs]}
        ds = Dataset.from_dict(data, features=features)

        # ---------- ê²€ì¦ìš© LLM/ì„ë² ë”© ----------
        VALIDATION_LLM_MODEL = os.getenv("VALIDATION_LLM_MODEL", os.getenv("OPENAI_LLM_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct"))
        VALIDATION_BASE_URL  = os.getenv("VALIDATION_BASE_URL",  os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"))
        VALIDATION_API_KEY   = os.getenv("VALIDATION_API_KEY",   os.getenv("GROQAI_API_KEY", ""))

        VALIDATION_MAX_TOKENS = int(os.getenv("VALIDATION_LLM_MAX_TOKENS", "4096"))
        VALIDATION_TIMEOUT    = int(os.getenv("VALIDATION_LLM_TIMEOUT", "120"))

        validation_llm = ChatOpenAI(
            api_key=VALIDATION_API_KEY,
            base_url=VALIDATION_BASE_URL,
            model=VALIDATION_LLM_MODEL,
            temperature=0.0,
            max_tokens=VALIDATION_MAX_TOKENS,
            timeout=VALIDATION_TIMEOUT,
        )
        validation_emb = HuggingFaceEmbeddings(
            model_name=os.getenv("VALIDATION_EMB_MODEL", "jhgan/ko-sroberta-multitask"),
            model_kwargs={"device": "cpu"}
        )

        # ---------- âœ… ì§€í‘œ ë¶„ë¦¬ í‰ê°€ë§Œ ì‹¤í–‰ (ë°°ì¹˜ í‰ê°€ ì œê±°) ----------
        print(f"[RAGAS] evaluating 1 sample with thresholds f>={f_min}, a>={a_min} (tokens={VALIDATION_MAX_TOKENS}, timeout={VALIDATION_TIMEOUT}s)")
        f = self._safe_eval_metric(ds, faithfulness,     validation_llm, validation_emb, "faithfulness")
        a = self._safe_eval_metric(ds, answer_relevancy, validation_llm, validation_emb, "answer_relevancy")

        print(f"[RAGAS] faithfulness={f:.3f}, answer_relevancy={a:.3f}")
        state["validated"] = bool((f >= f_min) and (a >= a_min))

        # ì¬ì‹œë„ ì •ì±…
        if not state["validated"]:
            state["retry_count"] = state.get("retry_count", 0) + 1
            if state["retry_count"] >= 5:
                print("âš ï¸ RAGAS ì„ê³„ ë¯¸ë‹¬ 5íšŒ â†’ ê²°ê³¼ë¥¼ ì €ì¥ ë‹¨ê³„ë¡œ ê°•ì œ ì§„í–‰")
            else:
                print(f"âš ï¸ RAGAS ì„ê³„ ë¯¸ë‹¬ â†’ ì¬ìƒì„± ì‹œë„ ({state['retry_count']}/5)")
        else:
            print("âœ… RAGAS ê²€ì¦ í†µê³¼")

        return state



    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:

        # if not state.get("validated", False):
        #     print("âš ï¸ ê²€ì¦ ì‹¤íŒ¨ ìƒíƒœ â†’ ë²¡í„°DB ì €ì¥ì„ ê±´ë„ˆë›°ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #     # (ì„ íƒ) ê²°ê³¼ ë¡œê·¸ëŠ” ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ìœ ì§€, ì™„ì „ ìŠ¤í‚µí•˜ë ¤ë©´ ì´ ë¸”ë¡ì„ ì§€ì›Œë„ ë¨
        #     state.setdefault("results", []).append({
        #         "user_problem": state.get("user_problem", "") or "",
        #         "user_problem_options": state.get("user_problem_options", []) or [],
        #         "generated_answer": state.get("generated_answer", ""),
        #         "generated_explanation": state.get("generated_explanation", ""),
        #         "generated_subject": state.get("generated_subject", ""),
        #         "validated": False,
        #         "chat_history": state.get("chat_history", []),
        #     })
        #     return state
        
        # vectorstore_p = state.get("vectorstore_p")
        # q    = state.get("user_problem", "") or ""
        # opts = state.get("user_problem_options", []) or []

        # from langchain_core.documents import Document
        # import json, hashlib

        # # ---------- helpers ----------
        # norm = lambda s: " ".join((s or "").split()).strip()

        # def parse_opts(v):
        #     if isinstance(v, str):
        #         try: v = json.loads(v)
        #         except: v = [v]
        #     return [norm(str(x)) for x in (v or [])]

        # def doc_id_of(q, opts):
        #     base = norm(q) + "||" + "||".join(parse_opts(opts))
        #     return hashlib.sha1(base.encode()).hexdigest()

        # def _clean_str(v):
        #     if isinstance(v, (bytes, bytearray)):
        #         try: v = v.decode("utf-8", "ignore")
        #         except Exception: v = str(v)
        #     if isinstance(v, str):
        #         s = v.strip()
        #         if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
        #             try:
        #                 u = json.loads(s)
        #                 if isinstance(u, str): s = u.strip()
        #             except Exception:
        #                 pass
        #         if s.lower() in ("", "null", "none"):
        #             return ""
        #         return s
        #     return v

        # def _is_blank(v):
        #     v = _clean_str(v)
        #     if v is None: return True
        #     if isinstance(v, str): return v == ""
        #     try: return len(v) == 0
        #     except Exception: return False

        # def _escape(s: str) -> str:
        #     # Milvus exprìš© ì´ìŠ¤ì¼€ì´í”„
        #     return s.replace("\\", "\\\\").replace('"', r"\"")

        # did = doc_id_of(q, opts)

        # # ---------- ì™„ì „ ì¼ì¹˜ 1ê°œë§Œ: retrieved_docs[0] ----------
        # docs = state.get("retrieved_docs", []) or []
        # exact = None
        # if docs:
        #     d = docs[0]
        #     same_q = norm(d.page_content) == norm(q)
        #     same_o = parse_opts(d.metadata.get("options", "[]")) == parse_opts(opts)
        #     print("[DEBUG] exact-match?", same_q and same_o,
        #         "| Q:", repr(norm(d.page_content)), "==", repr(norm(q)),
        #         "| OPTS:", parse_opts(d.metadata.get("options", "[]")), "==", parse_opts(opts))
        #     if same_q and same_o:
        #         exact = d

        # # ---------- ì‚­ì œâ†’ì¶”ê°€ (upsert) ----------
        # def upsert(meta, pk_to_delete=None, text_to_delete=None):
        #     if not vectorstore_p:
        #         print("âš ï¸ vectorstore_p ì—†ìŒ â†’ ì €ì¥ ìŠ¤í‚µ(ê²°ê³¼ë§Œ ê¸°ë¡)")
        #         return
        #     # 1) PKë¡œ ì‚­ì œ (ê°€ì¥ ì•ˆì „)
        #     if pk_to_delete is not None:
        #         try:
        #             vectorstore_p.delete([pk_to_delete])
        #             print(f"[DEBUG] delete by PK ok: {pk_to_delete}")
        #         except Exception as e:
        #             print(f"[DEBUG] delete(ids=[{pk_to_delete}]) ì‹¤íŒ¨: {e}")

        #     # 2) expr ì‚­ì œ: í•„ë“œëª… í›„ë³´ ìˆœíšŒ (ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆë§ˆë‹¤ ë‹¤ë¦„)
        #     elif text_to_delete:
        #         expr_fields = ["text", "page_content", "content", "question"]
        #         esc = _escape(text_to_delete)
        #         for f in expr_fields:
        #             try:
        #                 vectorstore_p.delete(expr=f'{f} == "{esc}"')
        #                 print(f"[DEBUG] delete by expr ok: {f} == \"{esc}\"")
        #                 break
        #             except Exception as e:
        #                 print(f"[DEBUG] delete by expr ì‹¤íŒ¨({f}): {e}")

        #     # ìƒˆ ë¬¸ì„œ ì¶”ê°€
        #     vectorstore_p.add_documents([Document(
        #         page_content=q,
        #         metadata={
        #             # ì°¸ê³ ìš© fingerprint(ìŠ¤í‚¤ë§ˆ í•„ë“œëŠ” ì•„ë‹˜)
        #             "doc_id": did,
        #             "options": json.dumps(opts, ensure_ascii=False),
        #             "answer":      meta.get("answer", "") or "",
        #             "explanation": meta.get("explanation", "") or "",
        #             "subject":     meta.get("subject", "") or "",
        #         },
        #     )])

        # if exact:
        #     meta = exact.metadata.copy()
        #     updated = False

        #     new_answer      = _clean_str(state.get("generated_answer"))
        #     new_explanation = _clean_str(state.get("generated_explanation"))
        #     new_subject     = _clean_str(state.get("generated_subject"))

        #     for k, new_val in [("answer", new_answer),
        #                     ("explanation", new_explanation),
        #                     ("subject", new_subject)]:
        #         cur_val   = meta.get(k)
        #         cur_blank = _is_blank(cur_val)
        #         new_blank = _is_blank(new_val)
        #         print(f"[DEBUG] {k}: current={repr(cur_val)} (blank={cur_blank}) "
        #             f"new={repr(new_val)} (blank={new_blank})")
        #         if cur_blank and not new_blank:
        #             meta[k] = new_val
        #             updated = True

        #     if updated:
        #         # PK ì¶”ì¶œ ì‹œë„ (í™˜ê²½ì— ë”°ë¼ 'pk'/'id'/'_id' ë“±ì¼ ìˆ˜ ìˆìŒ)
        #         pk = None
        #         for k in ("pk", "id", "_id", "pk_id", "milvus_id"):
        #             if k in exact.metadata:
        #                 pk = exact.metadata[k]; break
        #         if pk is not None:
        #             upsert(meta, pk_to_delete=pk)
        #         else:
        #             # PK ëª» ì°¾ìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ expr ì‚­ì œ ì‹œë„
        #             upsert(meta, text_to_delete=q)
        #         print("âœ… ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ë¹ˆ ì»¬ëŸ¼ë§Œ ì±„ì›Œ ê°±ì‹ ")
        #     else:
        #         print("âš ï¸ ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ì¡´ì¬, ì €ì¥ ìƒëµ")
        # else:
        #     upsert({
        #         "answer": _clean_str(state.get("generated_answer")),
        #         "explanation": _clean_str(state.get("generated_explanation")),
        #         "subject": _clean_str(state.get("generated_subject")),
        #     }, text_to_delete=q)
        #     print("ğŸ†• ì‹ ê·œ ë¬¸í•­ ì €ì¥")

        # ---------- ê²°ê³¼ ê¸°ë¡ ----------
        state.setdefault("results", []).append({
            "user_problem":state.get("user_problem", "") or "",
            "user_problem_options": state.get("user_problem_options", []) or [],
            "generated_answer": state.get("generated_answer", ""),
            "generated_explanation": state.get("generated_explanation", ""),
            "generated_subject": state.get("generated_subject", ""),
            "validated": state.get("validated", False),
            "chat_history": state.get("chat_history", []),
        })
        return state

    def invoke(
            self, 
            user_input_txt: str,
            user_problem: str,
            user_problem_options: List[str],
            vectorstore_p: Optional[Milvus] = None,
            vectorstore_c: Optional[Milvus] = None,
            recursion_limit: int = 1000,
        ) -> Dict:  

        # # 1) ì™¸ë¶€ì—ì„œ í•˜ë‚˜ë¼ë„ ì•ˆ ë„˜ê²¼ìœ¼ë©´ ë‚´ë¶€ ë””í´íŠ¸ ì¤€ë¹„
        # if vectorstore_p is None or vectorstore_c is None:
        #     self._ensure_vectorstores()

        # # 2) ìµœì¢…ìœ¼ë¡œ ì“¸ ë²¡í„°ìŠ¤í† ì–´ ê²°ì • (ì™¸ë¶€ > ë‚´ë¶€)
        # vs_p = vectorstore_p or self.vectorstore_p
        # vs_c = vectorstore_c or self.vectorstore_c

        # # (ì„ íƒ) ì•ˆì „ì¥ì¹˜: ê·¸ë˜ë„ Noneì´ë©´ ê²½ê³ ë§Œ ì°ê³  ê³„ì†
        # if vs_p is None:
        #     print("âš ï¸ vectorstore_pê°€ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        # if vs_c is None:
        #     print("âš ï¸ vectorstore_cê°€ ì—†ìŠµë‹ˆë‹¤. ê°œë… ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        initial_state: SolutionState = {
            "user_input_txt": user_input_txt,

            "user_problem": user_problem,
            "user_problem_options": user_problem_options,

            "vectorstore_p": vectorstore_p,
            "vectorstore_c": vectorstore_c,

            "retrieved_docs": [],
            "problems_contexts_text": "",
            "concept_contexts": [],
            "concept_contexts_text": "",

            "generated_answer": "",
            "generated_explanation": "",
            "generated_subject": "",
            "validated": False,
            "retry_count": 0,

            "results": [],
            
            "chat_history": []
        }
        
        final_state = self.graph.invoke(initial_state, config={"recursion_limit": recursion_limit})
        
        # ê·¸ë˜í”„ ì‹œê°í™”
        # try:
        #     graph_image_path = "solution_agent_workflow.png"
        #     with open(graph_image_path, "wb") as f:
        #         f.write(self.graph.get_graph().draw_mermaid_png())
        #     print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # except Exception as e:
        #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        #     print("ì›Œí¬í”Œë¡œìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

        # ê²°ê³¼ í™•ì¸ ë° ë””ë²„ê¹…
        results = final_state.get("results", [])
        print(f"   - ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        
        if results:
            for i, result in enumerate(results):
                print(f"   - ê²°ê³¼ {i+1}: {result.get('question', '')[:30]}...")
        else:
            print("   âš ï¸ resultsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            print(f"   - final_state ë‚´ìš©: {final_state}")
        
        return final_state


# ====== replace the entire __main__ block in solution_agent.py ======
if __name__ == "__main__":
    

    # ----------------------------
    # ê³ ì • ì‹¤í–‰ íŒŒë¼ë¯¸í„° (ì›í•˜ë©´ ì—¬ê¸°ë§Œ ìˆ˜ì •)
    # ----------------------------
    JSON_DIR        = os.getenv("PROBLEMS_JSON_DIR", "./teacher/exam/test_parsed_exam_json")  # í´ë” ê²½ë¡œ
    MILVUS_HOST     = os.getenv("MILVUS_HOST", "localhost")
    MILVUS_PORT     = os.getenv("MILVUS_PORT", "19530")
    PROBLEMS_COLL   = os.getenv("PROBLEMS_COLL", "problems")
    CONCEPT_COLL    = os.getenv("CONCEPT_COLL", "concept_summary")
    INSTRUCTION     = os.getenv("AGENT_INSTRUCTION", "ì •ë‹µ ë²ˆí˜¸ì™€ í’€ì´, ê³¼ëª©ì„ ì•Œë ¤ì¤˜.")  # â† input() ì œê±°
    RECURSION_LIMIT = int(os.getenv("AGENT_RECURSION_LIMIT", "200"))
    ONLY_INDEX      = int(os.getenv("AGENT_ONLY_INDEX", "0"))  # 0ì´ë©´ ì „ì²´, 1 ì´ìƒì´ë©´ í•´ë‹¹ ë¬¸ì œ(1-based)

    # --- app.py ì°¸ê³ í•œ ë²¡í„° ì—°ê²° í•¨ìˆ˜ ---
    def init_vectorstore(host: str, port: str, coll: str,
                         *, text_field: str | None = None,
                         vector_field: str | None = None,
                         metric_type: str | None = None) -> Milvus:
        emb = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={"device": "cpu"},
        )
        if "default" not in connections.list_connections():
            connections.connect(alias="default", host=host, port=port)

        actual_metric = metric_type
        try:
            col = Collection(coll)
            if col.indexes:
                params = col.indexes[0].params or {}
                actual_metric = params.get("metric_type") or params.get("METRIC_TYPE") or actual_metric
        except Exception:
            pass
        if not actual_metric:
            actual_metric = "L2"

        kwargs = {
            "embedding_function": emb,
            "collection_name": coll,
            "connection_args": {"host": host, "port": port},
            "search_params": {"metric_type": actual_metric, "params": {"nprobe": 10}},
        }
        if text_field is not None:
            kwargs["text_field"] = text_field
        if vector_field is not None:
            kwargs["vector_field"] = vector_field
        return Milvus(**kwargs)

    # --- JSON í´ë” ë‚´ íŒŒì¼ ëª©ë¡ ---
    if not os.path.isdir(JSON_DIR):
        raise FileNotFoundError(f"ë¬¸ì œ JSON í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JSON_DIR}")
    json_files = sorted(glob.glob(os.path.join(JSON_DIR, "*.json")))
    if not json_files:
        raise ValueError(f"{JSON_DIR} ì•ˆì— .json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- Milvus ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ---
    vectorstore_p = init_vectorstore(MILVUS_HOST, MILVUS_PORT, PROBLEMS_COLL)
    vectorstore_c = init_vectorstore(
        MILVUS_HOST, MILVUS_PORT, CONCEPT_COLL,
        text_field="content",
        vector_field="embedding",
    )

    agent = SolutionAgent()

    def run_one(p: dict) -> dict:
        return agent.invoke(
            user_input_txt=INSTRUCTION,
            user_problem=p.get("question", "") or "",
            user_problem_options=p.get("options", []) or [],
            vectorstore_p=vectorstore_p,
            vectorstore_c=vectorstore_c,
            recursion_limit=RECURSION_LIMIT,
        )

    # --- ê° íŒŒì¼ ìˆœíšŒ ì‹¤í–‰ ---
    for jf in json_files:
        print(f"\n=== JSON íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {jf} ===")
        with open(jf, "r", encoding="utf-8") as f:
            raw = json.load(f)

        # 1) íŒŒì¼ êµ¬ì¡°: dictì— "questions"ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©, ì•„ë‹ˆë©´ list ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if isinstance(raw, dict) and isinstance(raw.get("questions"), list):
            items = raw["questions"]
        elif isinstance(raw, list):
            items = raw
        else:
            raise ValueError(f"{jf}: ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON êµ¬ì¡° (list ë˜ëŠ” {{'questions':[...]}} )")

        # 2) âœ… ì¸ë±ì‹±ë§Œ: question / options ë‘ í•„ë“œë§Œ ë½‘ì•„ì„œ ì „ë‹¬
        #    - optionsê°€ listê°€ ì•„ë‹ˆê±°ë‚˜ ì—†ëŠ” í•­ëª©ì€ ê±´ë„ˆëœ€ (ë¶ˆí•„ìš”í•œ ì •ê·œí™”ëŠ” í•˜ì§€ ì•ŠìŒ)
        problems = []
        for it in items:
            if not isinstance(it, dict):
                continue
            q = it.get("question")
            opts = it.get("options")
            if isinstance(q, str) and isinstance(opts, list):
                problems.append({"question": q, "options": opts})

        if not problems:
            print(f"[WARN] {jf}: question/options í˜•ì‹ì˜ ë¬¸ì œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            continue

        print(f"[LOAD] {jf}: {len(problems)}ë¬¸í•­ (question/optionsë§Œ ì‚¬ìš©)")

        outputs = []
        if ONLY_INDEX and ONLY_INDEX > 0:
            idx = ONLY_INDEX
            if not (1 <= idx <= len(problems)):
                raise IndexError(f"--index={idx} ë²”ìœ„ ë²—ì–´ë‚¨ (1..{len(problems)}) in {jf}")
            res_state = run_one(problems[idx - 1])
            outputs.append((idx, (res_state.get("results") or [{}])[-1]))
        else:
            for i, p in enumerate(problems, 1):
                res_state = run_one(p)
                outputs.append((i, (res_state.get("results") or [{}])[-1]))

        # --- ì½˜ì†” ì¶œë ¥ ---
        print("\n================= ê²°ê³¼ =================")
        print(f"- ì‹¤í–‰ì‹œê°: {datetime.now().isoformat(timespec='seconds')}")
        print(f"- ì…ë ¥íŒŒì¼: {jf}")
        for i, r in outputs:
            print(f"\n# ê²°ê³¼ {i}")
            print(f"- ì •ë‹µ(ë²ˆí˜¸): {r.get('generated_answer','-')}")
            print(f"- ê³¼ëª©: {r.get('generated_subject','-')}")
            print(f"- í’€ì´:\n{r.get('generated_explanation','-')}")
        print("========================================\n")
