import os
from typing import TypedDict, List, Dict, Optional, Tuple, Any
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections, Collection, DataType
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re
from langchain_openai import ChatOpenAI
from ..base_agent import BaseAgent
from ..retrieve.retrieve_agent import retrieve_agent
from langchain_community.retrievers import BM25Retriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np
import asyncio, sys
from concurrent.futures import ThreadPoolExecutor
import copy

from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.types import Command
try:
    from langgraph.errors import NodeInterrupt
except Exception:
    NodeInterrupt = RuntimeError
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.tools import InjectedToolCallId, tool
from datetime import datetime

try:
    from rank_bm25 import BM25Okapi  # optional fallback(bm25 ì¸ë±ìŠ¤ ì—†ì´ í›„ë³´êµ° ìœ„ì—ì„œ sparse ìŠ¤ì½”ì–´ë§)
    HAS_RANK_BM25 = True
except Exception:
    HAS_RANK_BM25 = False

try:
    from sentence_transformers import CrossEncoder
    HAS_CROSS_ENCODER = True
except Exception:
    HAS_CROSS_ENCODER = False



# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GROQAI_API_KEY = os.getenv("GROQAI_API_KEY", "")
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))

    
# âœ… ìƒíƒœ ì •ì˜
class SolutionState(TypedDict):
    # ì‚¬ìš©ì ì…ë ¥
    user_input_txt: str

    # ë¬¸ì œë¦¬ìŠ¤íŠ¸, ë¬¸ì œ, ë³´ê¸°
    user_problem: str
    user_problem_options: List[str]
    
    # vectorstore_p: Milvus
    # vectorstore_c: Milvus

    retrieved_docs: List[Document]
    problems_contexts_text : str
    concept_contexts: List[Document]
    concept_contexts_text: str

    # ë¬¸ì œ í•´ë‹µ/í’€ì´/ê³¼ëª© ìƒì„±
    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    generated_subject: str

    results: List[Dict]
    validated: bool
    retry_count: int             # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    chat_history: List[str]
    
    # í’€ì´ í‰ê°€ ë° í”¼ë“œë°± ê´€ë ¨
    solution_score: float        # í’€ì´ í’ˆì§ˆ ì ìˆ˜ (0-100)
    feedback_analysis: str      # ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„ ê²°ê³¼
    needs_improvement: bool     # í’€ì´ ê°œì„  í•„ìš” ì—¬ë¶€
    improved_solution: str      # ê°œì„ ëœ í’€ì´
    search_results: str         # ê²€ìƒ‰ ê²°ê³¼
    
    # ì‚¬ìš©ì HITL ì…ë ¥
    user_feedback: str         # ì¬ê°œ ì‹œ ì£¼ì…ë˜ëŠ” ì‚¬ìš©ì í”¼ë“œë°±
    need_search: bool          # í”¼ë“œë°± ë¶„ì„ ê²°ê³¼: ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    needs_easier_explanation: bool  # í”¼ë“œë°± ë¶„ì„ ê²°ê³¼: ì‰¬ìš´ ì„¤ëª… í•„ìš” ì—¬ë¶€
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ê´€ë ¨
    test_mode: bool             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
    test_score: int             # í…ŒìŠ¤íŠ¸ìš© ê°•ì œ ì ìˆ˜
    test_feedback_type: str     # í…ŒìŠ¤íŠ¸ìš© ê°•ì œ í”¼ë“œë°± íƒ€ì…

class SolutionAgent(BaseAgent):
    """ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        # --- í•˜ì´ë¸Œë¦¬ë“œ/ë¦¬ë­í¬ íŒŒë¼ë¯¸í„° ---
        self.RETRIEVAL_FETCH_K = int(os.getenv("RETRIEVAL_FETCH_K", "30"))
        self.HYBRID_TOPK       = int(os.getenv("HYBRID_TOPK", "12"))
        self.RERANK_TOPK       = int(os.getenv("RERANK_TOPK", "5"))
        self.HYBRID_ALPHA      = float(os.getenv("HYBRID_ALPHA", "0.5"))

        # --- ë°˜ë“œì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±í•´ ë‘ê¸° ---
        self.bm25_retriever = None      # â† ì—†ìœ¼ë©´ AttributeError
        self.reranker = None            # â† ë¦¬ë­ì»¤ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’
        self.rerank_model_name = os.getenv("RERANK_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")

        try:
            self.reranker = CrossEncoder(self.rerank_model_name, device=os.getenv("RERANK_DEVICE","cpu"))
            print(f"[Rerank] CrossEncoder loaded: {self.rerank_model_name}")
        except Exception as e:
            print(f"[Rerank] load skipped: {e}")

        # (ì„ íƒ) BM25 ë§ë­‰ì¹˜ê°€ ìˆë‹¤ë©´ ë¡œë“œ
        bm25_jsonl = os.getenv("BM25_CORPUS_JSONL")
        if bm25_jsonl and os.path.exists(bm25_jsonl):
            docs = []
            with open(bm25_jsonl, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                        docs.append(Document(page_content=obj.get("page_content",""),
                                             metadata=obj.get("metadata",{})))
                    except Exception:
                        pass
            if docs:
                self.bm25_retriever = BM25Retriever.from_documents(docs)
                print(f"[BM25] ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {len(docs)}")

        self.vectorstore_p = None
        self.vectorstore_c = None
        
        # checkpointer ì¶”ê°€
        self.checkpointer = InMemorySaver()
        self.graph = self._create_graph()


    def _ensure_vectorstores(
        self,
        host: str = "localhost",
        port: str = "19530",
        coll_p: str = "problems",
        coll_c: str = "concept_summary",
        model_name: str = "jhgan/ko-sroberta-multitask",
    ):
        # 1) ì´ë²¤íŠ¸ ë£¨í”„ ë³´ì¥
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            if sys.platform.startswith("win"):
                try:
                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
                except Exception:
                    pass
            asyncio.set_event_loop(asyncio.new_event_loop())

        # 2) ë™ê¸° http ìŠ¤í‚´ìœ¼ë¡œ ì—°ê²°
        if "default" not in connections.list_connections():
            # gRPC ê¸°ë³¸(19530). RESTëŠ” 19531. ì—¬ê¸°ì„œëŠ” gRPC ë°©ì‹ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
            connections.connect(alias="default", host=host, port=port)

        def infer_fields(coll_name: str):
            c = Collection(coll_name)
            vec_field, text_field, dim = None, None, None

            # ë²¡í„° í•„ë“œ(FLOAT_VECTOR) ì°¾ê¸°
            for f in c.schema.fields:
                if f.dtype == DataType.FLOAT_VECTOR and vec_field is None:
                    vec_field = f.name
                    try:
                        dim = int(f.params.get("dim") or 0)
                    except Exception:
                        dim = None

            # í…ìŠ¤íŠ¸ í•„ë“œëŠ” í›„ë³´êµ°ì—ì„œ ìš°ì„  ì„ íƒ, ì—†ìœ¼ë©´ ì²« VARCHAR ì‚¬ìš©
            candidates = ("text", "page_content", "content", "question", "item_title", "title")
            varchar_fields = [f.name for f in c.schema.fields if f.dtype == DataType.VARCHAR]
            for name in candidates:
                if name in varchar_fields:
                    text_field = name
                    break
            if text_field is None and varchar_fields:
                text_field = varchar_fields[0]

            if vec_field is None:
                raise RuntimeError(f"[Milvus] '{coll_name}'ì— FLOAT_VECTOR í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"[Milvus] '{coll_name}' â†’ text_field='{text_field}', vector_field='{vec_field}', dim={dim}")
            return text_field, vec_field, dim

        emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={"device": "cpu"})

        if self.vectorstore_p is None:
            txt_p, vec_p, _ = infer_fields(coll_p)
            self.vectorstore_p = Milvus(
                embedding_function=emb,
                collection_name=coll_p,
                connection_args={"host": host, "port": port},
                text_field=txt_p,
                vector_field=vec_p,
            )
            print(f"âœ… Milvus '{coll_p}' ì—°ê²° OK (text_field={txt_p}, vector_field={vec_p})")

        if self.vectorstore_c is None:
            txt_c, vec_c, _ = infer_fields(coll_c)
            self.vectorstore_c = Milvus(
                embedding_function=emb,
                collection_name=coll_c,
                connection_args={"host": host, "port": port},
                text_field=txt_c,
                vector_field=vec_c,
            )
            print(f"âœ… Milvus '{coll_c}' ì—°ê²° OK (text_field={txt_c}, vector_field={vec_c})")

        
    @property
    def name(self) -> str:
        return "SolutionAgent"

    @property
    def description(self) -> str:
        return "ì‹œí—˜ë¬¸ì œë¥¼ ì¸ì‹í•˜ì—¬ ë‹µê³¼ í’€ì´, í•´ì„¤ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."

    def _llm(self, temperature: float = 0):
        return ChatOpenAI(
            api_key=GROQAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            model=OPENAI_LLM_MODEL,
            temperature=temperature,
        )
    
    def _evaluate_solution(self, state: SolutionState) -> SolutionState:
        """LLMì„ í™œìš©í•˜ì—¬ í’€ì´ì˜ ì •í™•ì„±ê³¼ ì„¤ëª…ì˜ ì¶©ë¶„í•¨ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        print("\nğŸ“Š [í‰ê°€] í’€ì´ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        
        evaluation_prompt = f"""
        ë‹¤ìŒ ë¬¸ì œì™€ í’€ì´ë¥¼ í‰ê°€í•˜ì—¬ 0-100ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ê°œì„ ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

        ë¬¸ì œ: {state.get('user_problem', '')}
        ë³´ê¸°: {state.get('user_problem_options', [])}
        ì •ë‹µ: {state.get('generated_answer', '')}
        í’€ì´: {state.get('generated_explanation', '')}

        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
        1. ì •í™•ì„± (40ì ): ì •ë‹µì´ ë§ëŠ”ì§€, ë…¼ë¦¬ê°€ ì˜¬ë°”ë¥¸ì§€
        2. ì„¤ëª…ì˜ ì¶©ë¶„í•¨ (30ì ): ë‹¨ê³„ë³„ ì„¤ëª…ì´ ì¶©ë¶„í•œì§€, ì´í•´í•˜ê¸° ì‰¬ìš´ì§€
        3. ìš©ì–´ ì„¤ëª… (20ì ): ì „ë¬¸ ìš©ì–´ë‚˜ ê°œë…ì— ëŒ€í•œ ì„¤ëª…ì´ ìˆëŠ”ì§€
        4. ì „ì²´ì ì¸ í’ˆì§ˆ (10ì ): ì „ì²´ì ìœ¼ë¡œ ì˜ ì‘ì„±ë˜ì—ˆëŠ”ì§€

        ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
        {{
            "score": ì ìˆ˜,
            "needs_improvement": true/false,
            "improvement_reasons": ["ê°œì„ ì´ í•„ìš”í•œ ì´ìœ ë“¤"],
            "evaluation_summary": "í‰ê°€ ìš”ì•½"
        }}

        ì ìˆ˜ê°€ 70ì  ë¯¸ë§Œì´ë©´ needs_improvementë¥¼ trueë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.
        """
        
        try:
            llm_evaluator = self._llm(0.3)
            evaluation_response = llm_evaluator.invoke(evaluation_prompt)
            
            # JSON íŒŒì‹± - ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            import json
            import re
            
            response_content = evaluation_response.content.strip()
            print(f"ğŸ” LLM í‰ê°€ ì‘ë‹µ: {response_content[:200]}...")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ê°ì²´ë§Œ ì°¾ê¸°
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = response_content
            
            try:
                evaluation_result = json.loads(json_str)
                print("âœ… JSON íŒŒì‹± ì„±ê³µ")
            except json.JSONDecodeError as json_err:
                print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {json_err}")
                # ê¸°ë³¸ê°’ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ ìƒì„±
                evaluation_result = {
                    "score": 70.0,
                    "needs_improvement": True,
                    "improvement_reasons": ["LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"],
                    "evaluation_summary": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
                }
            
            state["solution_score"] = evaluation_result.get("score", 70.0)
            state["needs_improvement"] = evaluation_result.get("needs_improvement", True)
            # state["needs_improvement"] = True
            
            print(f"ğŸ“Š í’€ì´ í‰ê°€ ì ìˆ˜: {state['solution_score']}/100")
            print(f"ğŸ“Š ê°œì„  í•„ìš”: {state['needs_improvement']}")
            
        except Exception as e:
            print(f"âš ï¸ í’€ì´ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["solution_score"] = 70.0
            state["needs_improvement"] = True
        
        return state

    def _collect_user_feedback(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ìë¡œë¶€í„° í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ’¬ [í”¼ë“œë°±] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì‹œì‘")
        
        print(f"ğŸ” ë””ë²„ê·¸ - needs_improvement: {state.get('needs_improvement', 'NOT_SET')}")
        print(f"ğŸ” ë””ë²„ê·¸ - needs_improvement íƒ€ì…: {type(state.get('needs_improvement'))}")
        print(f"ğŸ” ë””ë²„ê·¸ - needs_improvement == True: {state.get('needs_improvement') == True}")
        print(f"ğŸ” ë””ë²„ê·¸ - not needs_improvement: {not state.get('needs_improvement', False)}")
        
        if not state.get("needs_improvement", False):
            print("ğŸ’¬ ê°œì„ ì´ í•„ìš”í•˜ì§€ ì•Šì•„ í”¼ë“œë°± ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        # ì•„ì§ ì‚¬ìš©ì í”¼ë“œë°±ì´ ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨í•˜ì—¬ ì…ë ¥ì„ ë°›ëŠ”ë‹¤
        if not state.get("user_feedback"):
            print("ğŸ›‘ í”¼ë“œë°± ì…ë ¥ ëŒ€ê¸°: NodeInterrupt ë°œìƒ (user_feedback ì—†ìŒ)")
            raise NodeInterrupt({
                "message": "í’€ì´ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‰¬ìš´ ì„¤ëª…, ìš©ì–´ ì„¤ëª… ë“±)",
                "hint": "resume ì‹œ 'user_feedback' í‚¤ì— ë¬¸ìì—´ë¡œ ì „ë‹¬"
            })
        
        print("ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°±ì´ ì „ë‹¬ë˜ì–´ ìˆ˜ì§‘ ë‹¨ê³„ í†µê³¼")
        
        return {"feedback_analysis": state.get("feedback_analysis", "")}

    def _improve_solution(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ í’€ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤."""
        print("\nğŸ”§ [ê°œì„ ] í’€ì´ ê°œì„  ì‹œì‘")
        
        if not state.get("needs_improvement", False):
            print("ğŸ”§ ê°œì„ ì´ í•„ìš”í•˜ì§€ ì•Šì•„ í’€ì´ ê°œì„ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return state
        
        feedback = state.get("feedback_analysis", "understand")
        original_solution = state.get("generated_explanation", "")
        
        improvement_prompt = f"""
        ë‹¤ìŒ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ í’€ì´ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”.

        ì›ë³¸ í’€ì´: {original_solution}
        ì‚¬ìš©ì í”¼ë“œë°±: {feedback}

        í”¼ë“œë°± ë¶„ì„ ê²°ê³¼ì— ë”°ë¼:
        - "easier_explanation": ë” ì‰¬ìš´ ë‹¨ê³„ë³„ ì„¤ëª…ìœ¼ë¡œ ê°œì„ 
        - "term_explanation": ì „ë¬¸ ìš©ì–´ì™€ ê°œë…ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª… ì¶”ê°€
        - "term_easier_explanation": ìš©ì–´ ì„¤ëª…ê³¼ ì‰¬ìš´ ì„¤ëª… ëª¨ë‘ ê°œì„ 
        - "understand": ì›ë³¸ í’€ì´ ìœ ì§€

        ê°œì„ ëœ í’€ì´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        """
        
        try:
            llm_improver = self._llm(0.7)
            improvement_response = llm_improver.invoke(improvement_prompt)
            improved = improvement_response.content
            print("ğŸ”§ í’€ì´ ê°œì„  ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í’€ì´ ê°œì„  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            improved = original_solution
        
        return {"improved_solution": improved}
    
    #----------------------------------------create graph------------------------------------------------------

    def _create_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        # âœ… LangGraph êµ¬ì„±
        print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ê³µí†µ ì²˜ë¦¬
        graph.add_node("search_similarity", self._search_similar_problems)
        graph.add_node("search_concepts", self._search_concepts_summary)
        graph.add_node("retrieve_parallel", self._retrieve_parallel)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        graph.add_node("store", self._store_to_vector_db)
        
        # ìƒˆë¡œìš´ ë…¸ë“œë“¤
        graph.add_node("evaluate_solution", self._evaluate_solution)
        graph.add_node("collect_feedback", self._collect_user_feedback)
        graph.add_node("improve_solution", self._improve_solution)
        graph.add_node("search_additional_info", self._search_additional_info)
        graph.add_node("finalize_solution", self._finalize_solution)

        # ë„êµ¬ ë…¸ë“œ ì¶”ê°€ (ToolNode ëŒ€ì‹  ì¼ë°˜ ë…¸ë“œ ì‚¬ìš©)
        graph.add_node("user_feedback_tool", self._execute_user_feedback)

        # ì‹œì‘ì  ì„¤ì •
        graph.set_entry_point("retrieve_parallel")
        
        # ê¸°ë³¸ íë¦„
        graph.add_edge("retrieve_parallel", "generate_solution")
        graph.add_edge("generate_solution", "validate")
        
        # ê²€ì¦ í›„ ë¶„ê¸°
        graph.add_conditional_edges(
            "validate", 
            # ê²€ì¦ ì‹¤íŒ¨ â†’ retry<5ì´ë©´ back, ì•„ë‹ˆë©´ evaluateë¡œ ì§„í–‰
            lambda s: "ok" if s["validated"] else ("back" if s.get("retry_count", 0) < 5 else "evaluate"),
            {"ok": "evaluate_solution", "back": "generate_solution", "evaluate": "evaluate_solution"}
        )
        
        # í’€ì´ í‰ê°€ í›„ ë¶„ê¸°
        graph.add_conditional_edges(
            "evaluate_solution",
            lambda s: "needs_improvement" if s.get("needs_improvement", False) else "store",
            {"needs_improvement": "collect_feedback", "store": "store"}
        )
        
        # ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°ì˜ íë¦„: ë¶„ì„ â†’ (ê²€ìƒ‰) â†’ (ê°œì„ ) â†’ ì •ë¦¬
        graph.add_edge("collect_feedback", "user_feedback_tool")
        # user_feedback_tool ê²°ê³¼ì— ë”°ë¼ ì¡°ê±´ ë¶„ê¸°
        graph.add_conditional_edges(
            "user_feedback_tool",
            lambda s: (
                "search_then_improve" if s.get("need_search") and s.get("needs_easier_explanation") else
                ("search_only" if s.get("need_search") else ("improve_only" if s.get("needs_easier_explanation") else "skip"))
            ),
            {
                "search_then_improve": "search_additional_info",
                "search_only": "search_additional_info",
                "improve_only": "improve_solution",
                "skip": "finalize_solution",
            }
        )
        # ê²€ìƒ‰ í›„: needs_easier_explanationì´ë©´ ê°œì„ ìœ¼ë¡œ, ì•„ë‹ˆë©´ ê³§ë°”ë¡œ ì •ë¦¬
        graph.add_conditional_edges(
            "search_additional_info",
            lambda s: "improve_solution" if s.get("needs_easier_explanation") else "finalize_solution",
            {"improve_solution": "improve_solution", "finalize_solution": "finalize_solution"}
        )
        graph.add_edge("finalize_solution", "store")
        
        # ìµœì¢… ì €ì¥
        graph.add_edge("store", END)

        # ë…¸ë“œ ë‚´ë¶€ì—ì„œ ì§ì ‘ interruptë¥¼ ë°œìƒì‹œí‚¤ë¯€ë¡œ compile ì‹œ interruption ì„¤ì • ë¶ˆí•„ìš”
        return graph.compile(checkpointer=self.checkpointer)
    
    #----------------------------------------nodes------------------------------------------------------

    def _search_similar_problems(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        print(state["user_problem"], state["user_problem_options"])
            
        # ë²¡í„°ìŠ¤í† ì–´ëŠ” ìƒíƒœì—ì„œ ì–»ì§€ ì•Šê³ , ì¸ìŠ¤í„´ìŠ¤ ë©¤ë²„ë¥¼ ì‚¬ìš©
        vectorstore_p = getattr(self, "vectorstore_p", None)

        if vectorstore_p is None:
            print("âš ï¸ vectorstore_pì—†ìŒ â†’ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ê±´ë„ˆëœ€")
            state["retrieved_docs"] = []
            state["problems_contexts_text"] = ""
            return state

        # q = state["user_problem"]
        q = self._build_concept_query(state.get("user_problem",""), state.get("user_problem_options", []))

        # ---------- (1) Dense í›„ë³´ ë„‰ë„‰íˆ ìˆ˜ì§‘ ----------
        try:
            # ì ìˆ˜ í¬í•¨ ë²„ì „ì´ ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©(ì—†ìœ¼ë©´ ì•„ë˜ exceptì—ì„œ ëŒ€ì²´)
            dense_scored = vectorstore_p.similarity_search_with_score(q, k=self.RETRIEVAL_FETCH_K)
            dense_docs = [d for d, _ in dense_scored]
            dense_scores = {id(d): float(s) for d, s in dense_scored}
            # ì£¼ì˜: ì–´ë–¤ ë°±ì—”ë“œëŠ” "ì½”ì‚¬ì¸ê±°ë¦¬/ê±°ë¦¬" ë“± ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨. ë­í¬ ê¸°ë°˜ìœ¼ë¡œ ì¹˜í™˜í•´ ì•ˆì •í™”.
            print(f"[Dense] fetched: {len(dense_docs)}")
        except Exception as e:
            print(f"[Dense] similarity_search_with_score ì‹¤íŒ¨ â†’ {e} â†’ score ì—†ì´ fallback")
            dense_docs = vectorstore_p.similarity_search(q, k=self.RETRIEVAL_FETCH_K)
            dense_scores = {id(d): 1.0/(r+1) for r, d in enumerate(dense_docs)}  # ë­í¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜

        # ---------- (2) Sparse í›„ë³´(BM25) ê²°í•© ----------
        sparse_docs = []
        sparse_scores = {}  # id(doc) â†’ score (rank ê¸°ë°˜ ë˜ëŠ” ì ìˆ˜ ì •ê·œí™”)

        if self.bm25_retriever is not None:
            try:
                sparse_docs = self.bm25_retriever.get_relevant_documents(q)[:self.RETRIEVAL_FETCH_K]
                for r, d in enumerate(sparse_docs):
                    sparse_scores[id(d)] = 1.0/(r+1)  # ë­í¬ ê¸°ë°˜
                print(f"[BM25] fetched: {len(sparse_docs)}")
            except Exception as e:
                print(f"[BM25] ì‹¤íŒ¨ â†’ {e}")

        elif HAS_RANK_BM25 and dense_docs:
            # ë³„ë„ ì¸ë±ìŠ¤ê°€ ì—†ë‹¤ë©´, dense í›„ë³´êµ° ìœ„ì—ì„œë§Œ BM25 ê·¼ì‚¬ ìŠ¤ì½”ì–´ ê³„ì‚°
            try:
                def tok(s: str) -> List[str]:
                    return re.findall(r"[ê°€-í£A-Za-z0-9_]+", (s or "").lower())
                corpus_toks = [tok(d.page_content) for d in dense_docs]
                bm25 = BM25Okapi(corpus_toks)
                q_scores = bm25.get_scores(tok(q))
                # ì ìˆ˜ ì •ê·œí™” (0~1)
                if q_scores is not None and len(q_scores) == len(dense_docs):
                    min_s, max_s = float(min(q_scores)), float(max(q_scores))
                    rng = (max_s - min_s) or 1.0
                    for d, s in zip(dense_docs, q_scores):
                        sparse_scores[id(d)] = (float(s) - min_s) / rng
                print(f"[BM25-lite] computed over dense pool: {len(dense_docs)}")
            except Exception as e:
                print(f"[BM25-lite] ì‹¤íŒ¨ â†’ {e}")

        # ---------- (3) Dense + Sparse ì•™ìƒë¸” ----------
        # ë™ì¼ ë¬¸ì„œê°€ ì–‘ìª½ì— ì„ì—¬ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ content+metadata ì¼ë¶€ë¡œ í‚¤ë¥¼ ë§Œë“ ë‹¤
        def _safe_meta_str(md: Dict[str, Any]) -> str:
            try:
                # numpy íƒ€ì… ë“±ì€ strë¡œ ë³€í™˜
                norm = {str(k): (v.item() if isinstance(v, (np.generic,)) else (str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v))
                        for k, v in (md or {}).items()}
                return json.dumps(norm, ensure_ascii=False, sort_keys=True)
            except Exception:
                try:
                    return str({k: str(v) for k, v in (md or {}).items()})
                except Exception:
                    return ""

        def key_of(doc: Document) -> Tuple[str, str]:
            return ( (doc.page_content or "")[:150], _safe_meta_str(doc.metadata)[:150] )

        pool: Dict[Tuple[str,str], Dict[str, Any]] = {}
        # dense ìª½ë¶€í„° ì ì¬
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            # ë­í¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜(ì•ˆì „)
            wd = 1.0/(r+1)
            # ì ìˆ˜ ìˆìœ¼ë©´ ë‘˜ ì¤‘ í° ê²ƒì„ ì‚¬ìš©
            wd = max(wd, dense_scores.get(id(d), 0.0))
            pool[k]["dense"] = max(pool[k]["dense"], wd)

        # sparse ìª½ ë°˜ì˜
        for r, d in enumerate(sparse_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            ws = 1.0/(r+1)
            ws = max(ws, sparse_scores.get(id(d), 0.0))
            pool[k]["sparse"] = max(pool[k]["sparse"], ws)

        # ê°€ì¤‘í•©
        alpha = self.HYBRID_ALPHA  # 0~1, 1ì´ë©´ denseë§Œ
        scored = []
        for k, v in pool.items():
            score = alpha * v["dense"] + (1.0 - alpha) * v["sparse"]
            scored.append((v["doc"], score))

        # ìƒìœ„ K ì¶”ë¦¼
        scored.sort(key=lambda x: x[1], reverse=True)
        hybrid_top = [d for d, _ in scored[:self.HYBRID_TOPK]]
        print(f"[Hybrid] pool={len(pool)} â†’ top{self.HYBRID_TOPK} ì„ ì •")

        # ---------- (4) Cross-Encoder rerank ----------
        try:
            if self.reranker is not None and len(hybrid_top) > 0:
                pairs = [[q, d.page_content] for d in hybrid_top]
                scores = self.reranker.predict(pairs)  # shape: (len(hybrid_top),)
                order = sorted(range(len(hybrid_top)), key=lambda i: float(scores[i]), reverse=True)
                reranked = [hybrid_top[i] for i in order[:self.RERANK_TOPK]]
                print(f"[Rerank] ìµœì¢… top{self.RERANK_TOPK} (CrossEncoder)")
            else:
                reranked = hybrid_top[:self.RERANK_TOPK]
                print("[Rerank] ì‚¬ìš© ì•ˆ í•¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        except Exception as e:
            print(f"[Rerank] ì‹¤íŒ¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©: {e}")
            reranked = hybrid_top[:self.RERANK_TOPK]


        # ---------- (5) ê¸°ì¡´ í¬ë§·/ì €ì¥ ----------
        results = reranked  # ìµœì¢… ë¬¸ì„œë“¤
        similar_questions = []
        for i, doc in enumerate(results, start=1):
            metadata = doc.metadata or {}
            options = json.loads(metadata.get("options", "[]")) if isinstance(metadata.get("options"), str) else metadata.get("options", []) or []
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")
            subject = metadata.get("subject", "ê¸°íƒ€")

            # ì •ë‹µ ë²ˆí˜¸ â†’ í…ìŠ¤íŠ¸
            answer_text = ""
            try:
                answer_idx = int(answer) - 1
                if 0 <= answer_idx < len(options):
                    answer_text = options[answer_idx]
            except Exception:
                pass

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer} ({answer_text})
                í’€ì´: {explanation}
                ê³¼ëª©: {subject}
                """
            similar_questions.append(formatted)

        state["retrieved_docs"] = results
        state["problems_contexts_text"] = "\n\n".join(similar_questions)

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ (dense fetch={len(dense_docs)}, hybrid_pool={len(pool)})")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state
    
    def _search_concepts_summary(self, state: SolutionState) -> SolutionState:
        print("\nğŸ“š [1-í™•ì¥] ê°œë… ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘")

        vectorstore_c = getattr(self, "vectorstore_c", None)
        if vectorstore_c is None:
            print("âš ï¸ vectorstore_c ì—†ìŒ â†’ ê°œë… ê²€ìƒ‰ ê±´ë„ˆëœ€")
            state["concept_contexts"], state["concept_contexts_text"] = [], ""
            return state

        q = self._build_concept_query(
            state.get("user_problem", ""),
            state.get("user_problem_options", []),
        )

        # ---------- (1) Dense í›„ë³´ ë„‰ë„‰íˆ ìˆ˜ì§‘ ----------
        try:
            dense_scored = vectorstore_c.similarity_search_with_score(q, k=self.RETRIEVAL_FETCH_K)
            dense_docs   = [d for d, _ in dense_scored]
            dense_scores = {id(d): float(s) for d, s in dense_scored}
            print(f"[Dense(concepts)] fetched: {len(dense_docs)}")
        except Exception as e:
            print(f"[Dense(concepts)] similarity_search_with_score ì‹¤íŒ¨ â†’ {e} â†’ score ì—†ì´ fallback")
            dense_docs   = vectorstore_c.similarity_search(q, k=self.RETRIEVAL_FETCH_K)
            dense_scores = {id(d): 1.0/(r+1) for r, d in enumerate(dense_docs)}

        # ---------- (2) Sparse í›„ë³´ ê²°í•© (BM25-lite over dense pool) ----------
        # ê°œë… ì½”í¼ìŠ¤ìš© ë³„ë„ BM25 ì¸ë±ìŠ¤ê°€ ì—†ë‹¤ë©´, dense í›„ë³´êµ° ìœ„ì—ì„œë§Œ BM25 ì ìˆ˜ ê·¼ì‚¬
        sparse_scores = {}
        try:
            if dense_docs and HAS_RANK_BM25:
                def tok(s: str) -> List[str]:
                    return re.findall(r"[ê°€-í£A-Za-z0-9_]+", (s or "").lower())
                corpus_toks = [tok(d.page_content) for d in dense_docs]
                bm25 = BM25Okapi(corpus_toks)
                q_scores = bm25.get_scores(tok(q))
                # 0~1 ì •ê·œí™”
                if q_scores is not None and len(q_scores) == len(dense_docs):
                    min_s, max_s = float(min(q_scores)), float(max(q_scores))
                    rng = (max_s - min_s) or 1.0
                    for d, s in zip(dense_docs, q_scores):
                        sparse_scores[id(d)] = (float(s) - min_s) / rng
                print(f"[BM25-lite(concepts)] computed over dense pool: {len(dense_docs)}")
            else:
                print("[BM25-lite(concepts)] ê±´ë„ˆëœ€ (dense_docs ë¹„ì—ˆê±°ë‚˜ rank_bm25 ë¯¸ì„¤ì¹˜)")
        except Exception as e:
            print(f"[BM25-lite(concepts)] ì‹¤íŒ¨ â†’ {e}")

        # ---------- (3) Dense + Sparse ì•™ìƒë¸” ----------
        def _safe_meta_str(md: Dict[str, Any]) -> str:
            try:
                norm = {
                    str(k): (
                        v.item() if hasattr(v, "item") else (
                            str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v
                        )
                    )
                    for k, v in (md or {}).items()
                }
                return json.dumps(norm, ensure_ascii=False, sort_keys=True)
            except Exception:
                try:
                    return str({k: str(v) for k, v in (md or {}).items()})
                except Exception:
                    return ""

        def key_of(doc: Document) -> Tuple[str, str]:
            return ((doc.page_content or "")[:150], _safe_meta_str(doc.metadata)[:150])

        pool: Dict[Tuple[str, str], Dict[str, Any]] = {}
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            pool.setdefault(k, {"doc": d, "dense": 0.0, "sparse": 0.0})
            wd = max(1.0/(r+1), dense_scores.get(id(d), 0.0))
            pool[k]["dense"] = max(pool[k]["dense"], wd)

        # BM25-lite ì ìˆ˜ë§Œ ì¡´ì¬ (ë³„ë„ sparse_docs ì—†ìŒ)
        for r, d in enumerate(dense_docs):
            k = key_of(d)
            if k not in pool:
                pool[k] = {"doc": d, "dense": 0.0, "sparse": 0.0}
            ws = max(1.0/(r+1), sparse_scores.get(id(d), 0.0))
            pool[k]["sparse"] = max(pool[k]["sparse"], ws)

        alpha = self.HYBRID_ALPHA
        scored = []
        for k, v in pool.items():
            score = alpha * v["dense"] + (1.0 - alpha) * v["sparse"]
            scored.append((v["doc"], score))

        scored.sort(key=lambda x: x[1], reverse=True)
        hybrid_top = [d for d, _ in scored[:self.HYBRID_TOPK]]
        print(f"[Hybrid(concepts)] pool={len(pool)} â†’ top{self.HYBRID_TOPK} ì„ ì •")

        # ---------- (4) Cross-Encoder rerank ----------
        try:
            if self.reranker is not None and len(hybrid_top) > 0:
                pairs  = [[q, d.page_content] for d in hybrid_top]
                scores = self.reranker.predict(pairs)
                order  = sorted(range(len(hybrid_top)), key=lambda i: float(scores[i]), reverse=True)
                reranked = [hybrid_top[i] for i in order[:self.RERANK_TOPK]]
                print(f"[Rerank(concepts)] ìµœì¢… top{self.RERANK_TOPK} (CrossEncoder)")
            else:
                reranked = hybrid_top[:self.RERANK_TOPK]
                print("[Rerank(concepts)] ì‚¬ìš© ì•ˆ í•¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        except Exception as e:
            print(f"[Rerank(concepts)] ì‹¤íŒ¨ â†’ hybrid_top ê·¸ëŒ€ë¡œ ì‚¬ìš©: {e}")
            reranked = hybrid_top[:self.RERANK_TOPK]

        # ---------- (5) LLM í”„ë¡¬í”„íŠ¸ìš© ì •ë¦¬ ----------
        chunks, cleaned_docs = [], []
        for idx, d in enumerate(reranked, start=1):
            md = d.metadata or {}
            content = (md.get("content") or d.page_content or "").strip()
            subject = (md.get("subject") or "").strip()
            if not content and d.page_content:
                content = d.page_content.strip()

            cleaned = Document(page_content=content, metadata={"subject": subject})
            cleaned_docs.append(cleaned)

            chunks.append(f"ê³¼ëª©: {subject}\në‚´ìš©: {content}")
            print(f" - [{idx}] subject='{subject}' content={content[:30]}...")

        state["concept_contexts"] = cleaned_docs
        state["concept_contexts_text"] = "\n\n".join(chunks)
        print(f"ğŸ“š ê°œë… ì»¨í…ìŠ¤íŠ¸ {len(cleaned_docs)}ê°œ ìˆ˜ì§‘")
        return state

    
    def _retrieve_parallel(self, state: SolutionState) -> SolutionState:
        # stateë¥¼ ë³µì‚¬í•´ì„œ ê° ì‘ì—…ì´ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì •í•˜ë„ë¡ í•¨
        s1 = copy.deepcopy(state)
        s2 = copy.deepcopy(state)

        with ThreadPoolExecutor(max_workers=2) as ex:
            f_sim = ex.submit(self._search_similar_problems, s1)
            f_con = ex.submit(self._search_concepts_summary, s2)
            r_sim = f_sim.result()
            r_con = f_con.result()

        # ê²°ê³¼ í•©ì¹˜ê¸°
        state["retrieved_docs"]        = r_sim.get("retrieved_docs", [])
        state["problems_contexts_text"]= r_sim.get("problems_contexts_text", "")
        state["concept_contexts"]      = r_con.get("concept_contexts", [])
        state["concept_contexts_text"] = r_con.get("concept_contexts_text", "")

        # ë””ë²„ê·¸ ë¡œê·¸
        print(f"[Parallel] similar={len(state['retrieved_docs'])}, "
            f"concepts={len(state['concept_contexts'])}")
        return state



    def _generate_solution(self, state: SolutionState) -> SolutionState:

        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm_gen = self._llm(0.5)  

        problems_ctx = state.get("problems_contexts_text", "")
        print("ìœ ì‚¬ ë¬¸ì œë“¤ ê¸¸ì´:", len(problems_ctx))
        print("ìœ ì‚¬ ë¬¸ì œë“¤: ", problems_ctx[:500])
        concept_ctx = state.get("concept_contexts_text", "")
        print("ê°œë… ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´:", len(concept_ctx))
        print("ê°œë… ì»¨í…ìŠ¤íŠ¸: ", concept_ctx[:500])

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_input_txt']}

            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {problems_ctx}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ê´€ë ¨ëœ ê°œë… ìš”ì•½:
            {concept_ctx}


            1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œì˜ **ì •ë‹µ**ì„ ì˜ ë³´ê¸° ë²ˆí˜¸ë¥¼ ì •ë‹µìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
            3. ì´ ë¬¸ì œì˜ ê³¼ëª©ì„ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ê³¼ëª© 5ê°œ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒìœ¼ë¡œ ì§€ì •í•´ ì£¼ì„¸ìš”.
                [ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„, ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•, í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©, ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬]
                (ìœ ì‚¬ë¬¸ì œì™€ ê°œë… ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ì˜ ê³¼ëª©ì„ ì°¸ê³ í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
            ê³¼ëª©: ...
        """

        response = llm_gen.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        subject_match = re.search(r"ê³¼ëª©:\s*(.+)", result)
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["generated_subject"] = subject_match.group(1).strip() if subject_match else "ê¸°íƒ€"

        state["chat_history"].append(f"Q: {state['user_input_txt']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state

    def _search_additional_info(self, state: SolutionState) -> SolutionState:
        """í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print("\nğŸ” [ê²€ìƒ‰] ì¶”ê°€ ì •ë³´ ê²€ìƒ‰ ì‹œì‘")
        
        # ìš°ì„ ìˆœìœ„: need_search í”Œë˜ê·¸ê°€ Trueì¼ ë•Œë§Œ ê²€ìƒ‰ ìˆ˜í–‰
        if not state.get("need_search", False):
            print("ğŸ” ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {"search_results": ""}

        # ì‚¬ìš©ì í”¼ë“œë°±ì„ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰ ì§ˆì˜ì— ì‚¬ìš© (ì—†ìœ¼ë©´ ë¬¸ì œ/í’€ì´ ê¸°ë°˜ í´ë°±)
        feedback_query = (state.get("user_feedback") or "").strip()
        if feedback_query:
            search_query = feedback_query
        else:
            search_query = f"{state.get('user_problem', '')} {state.get('generated_explanation', '')}".strip()
        print(f"ğŸ” ê²€ìƒ‰ ì§ˆì˜: {search_query[:120]}")
        try:
            # 1) retrieve_agentë¡œ ì™¸ë¶€ ê²€ìƒ‰/ë³‘í•© ìˆ˜í–‰
            try:
                ra = retrieve_agent()
                r = ra.invoke({"retrieval_question": search_query})
                merged = r.get("merged_context") or r.get("answer") or ""
                if merged:
                    print("ğŸ” retrieve_agent ê²°ê³¼ ì‚¬ìš©")
                    return {"search_results": merged}
            except Exception as e:
                print(f"âš ï¸ retrieve_agent í˜¸ì¶œ ì‹¤íŒ¨: {e} â†’ ë²¡í„°ìŠ¤í† ì–´ë¡œ í´ë°±")

            # 2) í´ë°±: vectorstore_c ìœ ì‚¬ë„ ê²€ìƒ‰
            if state.get("vectorstore_c"):
                concept_results = state["vectorstore_c"].similarity_search(search_query, k=3)
                concept_texts = [doc.page_content for doc in concept_results]
                print(f"ğŸ” ê´€ë ¨ ê°œë… {len(concept_results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
                return {"search_results": "\n\n".join(concept_texts)}
            else:
                print("âš ï¸ ê°œë… ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {"search_results": ""}
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"search_results": ""}

    def _finalize_solution(self, state: SolutionState) -> SolutionState:
        """ìµœì¢… í’€ì´ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        print("\nâœ¨ [ì •ë¦¬] ìµœì¢… í’€ì´ ì •ë¦¬ ì‹œì‘")
        
        if state.get("needs_improvement", False) and state.get("improved_solution"):
            # ê°œì„ ëœ í’€ì´ê°€ ìˆëŠ” ê²½ìš°
            final_solution = state["improved_solution"]
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if state.get("search_results"):
                final_solution += f"\n\nğŸ“š ì¶”ê°€ ì°¸ê³  ìë£Œ:\n{state['search_results']}"
            
            state["generated_explanation"] = final_solution
            print("âœ¨ ê°œì„ ëœ í’€ì´ë¡œ ìµœì¢… ì •ë¦¬ ì™„ë£Œ")
        else:
            print("âœ¨ ì›ë³¸ í’€ì´ ìœ ì§€")
        
        return state
    
    def _build_concept_query(self, problem: str, options: List[str]) -> str:
        opts = "\n".join([f"{i+1}) {o}" for i, o in enumerate(options or [])])
        return f"{(problem or '').strip()}\n{opts}"
    
    def _execute_user_feedback(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ì—¬ 'ê²€ìƒ‰ í•„ìš”/ì‰¬ìš´ ì„¤ëª… í•„ìš”'ë¥¼ íŒì •í•©ë‹ˆë‹¤."""
        print("\nğŸ› ï¸ [ë„êµ¬] ì‚¬ìš©ì í”¼ë“œë°± LLM ë¶„ì„ ì‹œì‘")
        feedback = (state.get("user_feedback") or "").strip()
        if not feedback:
            print("ğŸ’¡ user_feedback ë¹„ì–´ìˆìŒ â†’ ê¸°ë³¸ê°’: easier_explanation")
            state["feedback_analysis"] = "easier_explanation"
            state["need_search"] = False
            state["needs_easier_explanation"] = True
            return state

        llm = self._llm(0)
        prompt = f"""
        ì•„ë˜ì˜ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¶„ì„í•´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
        {{
          "need_search": true/false,
          "needs_easier_explanation": true/false,
          "analysis": "ê°„ë‹¨í•œ ìš”ì•½"
        }}
        ì‚¬ìš©ì í”¼ë“œë°±: {feedback}
        """
        try:
            resp = llm.invoke(prompt)
            txt = (resp.content or "").strip()
            import re, json
            m = re.search(r"\{[\s\S]*\}", txt)
            js = json.loads(m.group(0) if m else txt)
            state["need_search"] = bool(js.get("need_search", False))
            state["needs_easier_explanation"] = bool(js.get("needs_easier_explanation", True))
            state["feedback_analysis"] = js.get("analysis", "")
        except Exception as e:
            print(f"âš ï¸ í”¼ë“œë°± ë¶„ì„ ì‹¤íŒ¨ â†’ ê¸°ë³¸ê°’ ì ìš©: {e}")
            state["need_search"] = True if any(k in feedback for k in ["ì°¾ì•„", "ê²€ìƒ‰", "ê·¼ê±°", "ì¶œì²˜"]) else False
            state["needs_easier_explanation"] = True if any(k in feedback for k in ["ì‰¬ìš´", "ê°„ë‹¨", "ì‰½ê²Œ"]) else False
            state["feedback_analysis"] = "fallback_analysis"

        print(f"ğŸ’¬ íŒì •: need_search={state['need_search']}, needs_easier_explanation={state['needs_easier_explanation']}")
        return state

    def resume_workflow(self, thread_id: str, user_feedback: str) -> Dict[str, Any]:
        """ì¤‘ë‹¨ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©ì í”¼ë“œë°±ê³¼ í•¨ê»˜ ì¬ê°œí•©ë‹ˆë‹¤."""
        print(f"\nğŸ”„ ì›Œí¬í”Œë¡œìš° ì¬ê°œ: {thread_id}")
        print(f"ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°±: {user_feedback}")
        
        # thread_config ìƒì„±
        thread_config = {"configurable": {"thread_id": thread_id}}
        
        # ì‚¬ìš©ì í”¼ë“œë°±ì„ ìƒíƒœì— ì¶”ê°€
        updated_state = {
            "user_feedback": user_feedback
        }
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.graph.update_state(thread_config, updated_state)
        
        # ì›Œí¬í”Œë¡œìš° ì¬ê°œ: ê¸°ì¡´ interrupt íë¦„ ìœ ì§€ (ìƒìœ„ì—ì„œ Command(resume) í˜¸ì¶œ ê°€ëŠ¥)
        # ì—¬ê¸°ì„œëŠ” ìƒíƒœë§Œ ì—…ë°ì´íŠ¸í•˜ê³ , ê·¸ë˜í”„ë¥¼ í•œ ìŠ¤í… ì§„í–‰ì‹œì¼œì„œ ë‹¤ìŒ ë…¸ë“œê°€ í”¼ë“œë°±ì„ ì½ë„ë¡ í•¨
        print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì¬ê°œ ì‹œì‘...")
        try:
            # invoke(None)ëŠ” ë™ì¼ ìŠ¤í…ì— ë˜ ë‹¤ë¥¸ ê°’ì„ ì£¼ì…í•  ìˆ˜ ìˆì–´ ì¶©ëŒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ stream í•œ í„´ìœ¼ë¡œ ì§„í–‰
            final_state = None
            for ev in self.graph.stream(None, thread_config, stream_mode="values"):
                final_state = ev
            print("âœ… ì›Œí¬í”Œë¡œìš° ì¬ê°œ ì™„ë£Œ")
            return final_state or {}
        except Exception as e:
            print(f"âš ï¸ ì›Œí¬í”Œë¡œìš° ì¬ê°œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "status": "resume_failed"}

    # âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

    def _validate_solution(self, state: SolutionState) -> SolutionState:
        print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
        
        llm = self._llm(0)

        validation_prompt = f"""
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_input_txt']}

        ë¬¸ì œ ì§ˆë¬¸: {state['user_problem']}
        ë¬¸ì œ ë³´ê¸°: {state['user_problem_options']}

        ìƒì„±ëœ ì •ë‹µ: {state['generated_answer']}
        ìƒì„±ëœ í’€ì´: {state['generated_explanation']}
        ìƒì„±ëœ ê³¼ëª©: {state['generated_subject']}

        ìƒì„±ëœ í•´ë‹µê³¼ í’€ì´, ê³¼ëª©ì´ ë¬¸ì œì™€ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
        ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        validation_response = llm.invoke(validation_prompt)
        result_text = validation_response.content.strip().lower()

        # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
        print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
        state["validated"] = "ë„¤" in result_text
        
        if not state["validated"]:
            state["retry_count"] = state.get("retry_count", 0) + 1
            if state["retry_count"] >= 5:
                print("âš ï¸ ê²€ì¦ 5íšŒ ì‹¤íŒ¨ â†’ ê·¸ë˜ë„ ê²°ê³¼ë¥¼ ì €ì¥ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            else:
                print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì¬ì‹œë„ {state['retry_count']}/5)")
        else:
            print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")

        return state


    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:

        # if not state.get("validated", False):
        #     print("âš ï¸ ê²€ì¦ ì‹¤íŒ¨ ìƒíƒœ â†’ ë²¡í„°DB ì €ì¥ì„ ê±´ë„ˆë›°ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #     # (ì„ íƒ) ê²°ê³¼ ë¡œê·¸ëŠ” ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ìœ ì§€, ì™„ì „ ìŠ¤í‚µí•˜ë ¤ë©´ ì´ ë¸”ë¡ì„ ì§€ì›Œë„ ë¨
        #     state.setdefault("results", []).append({
        #         "user_problem": state.get("user_problem", "") or "",
        #         "user_problem_options": state.get("user_problem_options", []) or [],
        #         "generated_answer": state.get("generated_answer", ""),
        #         "generated_explanation": state.get("generated_explanation", ""),
        #         "generated_subject": state.get("generated_subject", ""),
        #         "validated": False,
        #         "chat_history": state.get("chat_history", []),
        #     })
        #     return state
        
        # vectorstore_p = state.get("vectorstore_p")
        # q    = state.get("user_problem", "") or ""
        # opts = state.get("user_problem_options", []) or []

        # from langchain_core.documents import Document
        # import json, hashlib

        # # ---------- helpers ----------
        # norm = lambda s: " ".join((s or "").split()).strip()

        # def parse_opts(v):
        #     if isinstance(v, str):
        #         try: v = json.loads(v)
        #         except: v = [v]
        #     return [norm(str(x)) for x in (v or [])]

        # def doc_id_of(q, opts):
        #     base = norm(q) + "||" + "||".join(parse_opts(opts))
        #     return hashlib.sha1(base.encode()).hexdigest()

        # def _clean_str(v):
        #     if isinstance(v, (bytes, bytearray)):
        #         try: v = v.decode("utf-8", "ignore")
        #         except Exception: v = str(v)
        #     if isinstance(v, str):
        #         s = v.strip()
        #         if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
        #             try:
        #                 u = json.loads(s)
        #                 if isinstance(u, str): s = u.strip()
        #             except Exception:
        #                 pass
        #         if s.lower() in ("", "null", "none"):
        #             return ""
        #         return s
        #     return v

        # def _is_blank(v):
        #     v = _clean_str(v)
        #     if v is None: return True
        #     if isinstance(v, str): return v == ""
        #     try: return len(v) == 0
        #     except Exception: return False

        # def _escape(s: str) -> str:
        #     # Milvus exprìš© ì´ìŠ¤ì¼€ì´í”„
        #     return s.replace("\\", "\\\\").replace('"', r"\"")

        # did = doc_id_of(q, opts)

        # # ---------- ì™„ì „ ì¼ì¹˜ 1ê°œë§Œ: retrieved_docs[0] ----------
        # docs = state.get("retrieved_docs", []) or []
        # exact = None
        # if docs:
        #     d = docs[0]
        #     same_q = norm(d.page_content) == norm(q)
        #     same_o = parse_opts(d.metadata.get("options", "[]")) == parse_opts(opts)
        #     print("[DEBUG] exact-match?", same_q and same_o,
        #         "| Q:", repr(norm(d.page_content)), "==", repr(norm(q)),
        #         "| OPTS:", parse_opts(d.metadata.get("options", "[]")), "==", parse_opts(opts))
        #     if same_q and same_o:
        #         exact = d

        # # ---------- ì‚­ì œâ†’ì¶”ê°€ (upsert) ----------
        # def upsert(meta, pk_to_delete=None, text_to_delete=None):
        #     if not vectorstore_p:
        #         print("âš ï¸ vectorstore_p ì—†ìŒ â†’ ì €ì¥ ìŠ¤í‚µ(ê²°ê³¼ë§Œ ê¸°ë¡)")
        #         return
        #     # 1) PKë¡œ ì‚­ì œ (ê°€ì¥ ì•ˆì „)
        #     if pk_to_delete is not None:
        #         try:
        #             vectorstore_p.delete([pk_to_delete])
        #             print(f"[DEBUG] delete by PK ok: {pk_to_delete}")
        #         except Exception as e:
        #             print(f"[DEBUG] delete(ids=[{pk_to_delete}]) ì‹¤íŒ¨: {e}")

        #     # 2) expr ì‚­ì œ: í•„ë“œëª… í›„ë³´ ìˆœíšŒ (ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆë§ˆë‹¤ ë‹¤ë¦„)
        #     elif text_to_delete:
        #         expr_fields = ["text", "page_content", "content", "question"]
        #         esc = _escape(text_to_delete)
        #         for f in expr_fields:
        #             try:
        #                 vectorstore_p.delete(expr=f'{f} == "{esc}"')
        #                 print(f"[DEBUG] delete by expr ok: {f} == \"{esc}\"")
        #                 break
        #             except Exception as e:
        #                 print(f"[DEBUG] delete by expr ì‹¤íŒ¨({f}): {e}")

        #     # ìƒˆ ë¬¸ì„œ ì¶”ê°€
        #     vectorstore_p.add_documents([Document(
        #         page_content=q,
        #         metadata={
        #             # ì°¸ê³ ìš© fingerprint(ìŠ¤í‚¤ë§ˆ í•„ë“œëŠ” ì•„ë‹˜)
        #             "doc_id": did,
        #             "options": json.dumps(opts, ensure_ascii=False),
        #             "answer":      meta.get("answer", "") or "",
        #             "explanation": meta.get("explanation", "") or "",
        #             "subject":     meta.get("subject", "") or "",
        #         },
        #     )])

        # if exact:
        #     meta = exact.metadata.copy()
        #     updated = False

        #     new_answer      = _clean_str(state.get("generated_answer"))
        #     new_explanation = _clean_str(state.get("generated_explanation"))
        #     new_subject     = _clean_str(state.get("generated_subject"))

        #     for k, new_val in [("answer", new_answer),
        #                     ("explanation", new_explanation),
        #                     ("subject", new_subject)]:
        #         cur_val   = meta.get(k)
        #         cur_blank = _is_blank(cur_val)
        #         new_blank = _is_blank(new_val)
        #         print(f"[DEBUG] {k}: current={repr(cur_val)} (blank={cur_blank}) "
        #             f"new={repr(new_val)} (blank={new_blank})")
        #         if cur_blank and not new_blank:
        #             meta[k] = new_val
        #             updated = True

        #     if updated:
        #         # PK ì¶”ì¶œ ì‹œë„ (í™˜ê²½ì— ë”°ë¼ 'pk'/'id'/'_id' ë“±ì¼ ìˆ˜ ìˆìŒ)
        #         pk = None
        #         for k in ("pk", "id", "_id", "pk_id", "milvus_id"):
        #             if k in exact.metadata:
        #                 pk = exact.metadata[k]; break
        #         if pk is not None:
        #             upsert(meta, pk_to_delete=pk)
        #         else:
        #             # PK ëª» ì°¾ìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ expr ì‚­ì œ ì‹œë„
        #             upsert(meta, text_to_delete=q)
        #         print("âœ… ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ë¹ˆ ì»¬ëŸ¼ë§Œ ì±„ì›Œ ê°±ì‹ ")
        #     else:
        #         print("âš ï¸ ë™ì¼ ë¬¸í•­(ì™„ì „ ì¼ì¹˜) ì¡´ì¬, ì €ì¥ ìƒëµ")
        # else:
        #     upsert({
        #         "answer": _clean_str(state.get("generated_answer")),
        #         "explanation": _clean_str(state.get("generated_explanation")),
        #         "subject": _clean_str(state.get("generated_subject")),
        #     }, text_to_delete=q)
        #     print("ğŸ†• ì‹ ê·œ ë¬¸í•­ ì €ì¥")

        # ---------- ê²°ê³¼ ê¸°ë¡ ----------
        state.setdefault("results", []).append({
            "user_problem":state.get("user_problem", "") or "",
            "user_problem_options": state.get("user_problem_options", []) or [],
            "generated_answer": state.get("generated_answer", ""),
            "generated_explanation": state.get("generated_explanation", ""),
            "generated_subject": state.get("generated_subject", ""),
            "validated": state.get("validated", False),
            "chat_history": state.get("chat_history", []),
        })
        return state

    def invoke(
            self, 
            user_input_txt: str,
            user_problem: str,
            user_problem_options: List[str],
            vectorstore_p: Optional[Milvus] = None,
            vectorstore_c: Optional[Milvus] = None,
            recursion_limit: int = 1000,
            memory_key: Optional[str] = None,  # ìˆí…€ ë©”ëª¨ë¦¬ í‚¤ ì¶”ê°€
            user_feedback: Optional[str] = None,  # ìƒìœ„ ê·¸ë˜í”„ì—ì„œ ì „ë‹¬ëœ ì‚¬ìš©ì í”¼ë“œë°±(ì¬ê°œ ì‹œ)
        ) -> Dict:  

        # 1) ì™¸ë¶€ì—ì„œ í•˜ë‚˜ë¼ë„ ì•ˆ ë„˜ê²¼ìœ¼ë©´ ë‚´ë¶€ ë””í´íŠ¸ ì¤€ë¹„ (ë¯¸ë¦¬ ì—°ê²° ì‹œë„)
        if vectorstore_p is None or vectorstore_c is None:
            try:
                self._ensure_vectorstores()
            except Exception as e:
                print(f"âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨ â†’ ê²€ìƒ‰ ë¹„í™œì„±í™”: {e}")

        # 2) ìµœì¢…ìœ¼ë¡œ ì“¸ ë²¡í„°ìŠ¤í† ì–´ ê²°ì • (ì™¸ë¶€ > ë‚´ë¶€)
        vs_p = vectorstore_p or self.vectorstore_p
        vs_c = vectorstore_c or self.vectorstore_c

        # (ì„ íƒ) ì•ˆì „ì¥ì¹˜: ê·¸ë˜ë„ Noneì´ë©´ ê²½ê³ ë§Œ ì°ê³  ê³„ì†
        if vs_p is None:
            print("âš ï¸ vectorstore_pê°€ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        if vs_c is None:
            print("âš ï¸ vectorstore_cê°€ ì—†ìŠµë‹ˆë‹¤. ê°œë… ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        initial_state: SolutionState = {
            "user_input_txt": user_input_txt,

            "user_problem": user_problem,
            "user_problem_options": user_problem_options,

            # ë²¡í„°ìŠ¤í† ì–´ í•¸ë“¤ì€ ìƒíƒœì— ë„£ì§€ ì•ŠëŠ”ë‹¤(ë¹„ì§ë ¬í™” ê°ì²´)

            "retrieved_docs": [],
            "problems_contexts_text": "",
            "concept_contexts": [],
            "concept_contexts_text": "",

            "generated_answer": "",
            "generated_explanation": "",
            "generated_subject": "",
            "validated": False,
            "retry_count": 0,

            "results": [],
            
            "chat_history": [],
            "user_feedback": user_feedback or ""
        }
        
        try:
            # thread_idë¥¼ í¬í•¨í•œ config ìƒì„±
            thread_id = memory_key or f"thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            graph_config = {
                "recursion_limit": recursion_limit,
                "configurable": {
                    "thread_id": thread_id
                }
            }
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = self.graph.invoke(initial_state, config=graph_config)
            # ê·¸ë˜í”„ê°€ ì¸í„°ëŸ½íŠ¸ë¥¼ ìƒíƒœì— ê¸°ë¡í•˜ê³  ì¢…ë£Œí–ˆì„ ìˆ˜ ìˆìŒ â†’ ëª…ì‹œì ìœ¼ë¡œ ì˜ˆì™¸ë¡œ ì „íŒŒ
            if isinstance(final_state, dict) and final_state.get("__interrupt__"):
                raise RuntimeError("interrupt: user feedback required")
            return final_state
            
        except Exception as e:
            # interruptëŠ” ìƒìœ„(teacher_graph í…ŒìŠ¤íŠ¸)ì—ì„œ ì¡ë„ë¡ ê·¸ëŒ€ë¡œ ì „íŒŒ
            print(f"âš ï¸ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
            
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        

        # ê²°ê³¼ í™•ì¸ ë° ë””ë²„ê¹…
        results = final_state.get("results", [])
        print(f"   - ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        
        if results:
            for i, result in enumerate(results):
                print(f"   - ê²°ê³¼ {i+1}: {result.get('question', '')[:30]}...")
        else:
            print("   âš ï¸ resultsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            print(f"   - final_state ë‚´ìš©: {final_state}")
        
        return final_state


if __name__ == "__main__":

    # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )

    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore_p = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port":"19530"}
    )

    vectorstore_c = Milvus(
        embedding_function=embedding_model,
        collection_name="concept_summary",
        connection_args={"host": "localhost", "port":"19530"}
    )

    agent = SolutionAgent()

    # ê·¸ë˜í”„ ì‹œê°í™” (ì„ íƒ)
    # try:
    #     graph_image_path = "solution_agent_workflow.png"
    #     with open(graph_image_path, "wb") as f:
    #         f.write(agent.graph.get_graph().draw_mermaid_png())
    #     print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    #     print("ì›Œí¬í”Œë¡œìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

    user_input_txt = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
    user_problem = input("\nâ“ ì‚¬ìš©ì ë¬¸ì œ: ").strip()
    user_problem_options_raw = input("\nâ“ ì‚¬ìš©ì ë³´ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
    user_problem_options = [opt.strip() for opt in user_problem_options_raw.split(",") if opt.strip()]

    final_state = agent.invoke(
        user_input_txt=user_input_txt,
        user_problem=user_problem,
        user_problem_options=user_problem_options,
    )

    # # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    # results = final_state.get("results", [])
    # results_data = {
    #     "timestamp": datetime.now().isoformat(),
    #     "user_input_txt": final_state.get("user_input_txt",""),
    #     "total_results": len(results),
    #     "results": results,
    # }

    # results_filename = os.path.join(f"solution_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    # os.makedirs(os.path.dirname(results_filename), exist_ok=True)
    # with open(results_filename, "w", encoding="utf-8") as f:
    #     json.dump(results_data, f, ensure_ascii=False, indent=2)
    # print(f"âœ… í•´ë‹µ ê²°ê³¼ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_filename}")
