import os
from typing import TypedDict, List, Dict, Literal, Optional, Tuple, Any
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re
from langchain_openai import ChatOpenAI
from ..base_agent import BaseAgent
from datetime import datetime

# LangGraph HITL ê´€ë ¨ import ì¶”ê°€
from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import InMemorySaver

# ê²€ìƒ‰ ì—ì´ì „íŠ¸ import (retrieve_agent ì—°ë™)
try:
    from ..retrieve.retrieve_agent import retrieve_agent
    SEARCH_AGENT_AVAILABLE = True
    print("âœ… ê²€ìƒ‰ ì—ì´ì „íŠ¸ import ì„±ê³µ")
except ImportError as e:
    SEARCH_AGENT_AVAILABLE = False
    print(f"âš ï¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("   - ê²€ìƒ‰ ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))

# âœ… ìƒíƒœ ì •ì˜ (HITL ê¸°ëŠ¥ ì¶”ê°€)
class SolutionState(TypedDict):
    # ì‚¬ìš©ì ì…ë ¥
    user_input_txt: str

    # ë¬¸ì œë¦¬ìŠ¤íŠ¸, ë¬¸ì œ, ë³´ê¸°
    user_problem: str
    user_problem_options: List[str]
    
    # vectorstoreëŠ” ìƒíƒœì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì§ë ¬í™” ë¬¸ì œ ë°©ì§€)

    retrieved_docs: List[Document]
    similar_questions_text : str

    # ë¬¸ì œ í•´ë‹µ/í’€ì´/ê³¼ëª© ìƒì„±
    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    generated_subject: str

    # HITL ê´€ë ¨ ìƒíƒœ
    user_feedback: str           # ì‚¬ìš©ì í”¼ë“œë°±
    feedback_type: str           # í”¼ë“œë°± ìœ í˜• (comprehension, clarification, improvement)
    search_results: str          # ê²€ìƒ‰ ì—ì´ì „íŠ¸ ê²°ê³¼
    improved_explanation: str    # ê°œì„ ëœ í’€ì´
    interaction_count: int       # ìƒí˜¸ì‘ìš© íšŸìˆ˜
    max_interactions: int        # ìµœëŒ€ ìƒí˜¸ì‘ìš© íšŸìˆ˜
    
    # í’ˆì§ˆ í‰ê°€ ê´€ë ¨ ìƒíƒœ
    quality_scores: Dict[str, float]  # ì„¸ë¶€ í’ˆì§ˆ ì ìˆ˜ë“¤
    total_quality_score: float        # ì´ í’ˆì§ˆ ì ìˆ˜
    quality_threshold: float          # í’ˆì§ˆ ì„ê³„ê°’
    
    results: List[Dict]
    validated: bool
    retry_count: int             # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    chat_history: List[str]
    
class SolutionAgent(BaseAgent):
    """Human-in-the-Loopê°€ í¬í•¨ëœ ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self, max_interactions: int = 5, hitl_mode: str = "smart"):
        self.max_interactions = max_interactions
        self.hitl_mode = hitl_mode  # "auto", "manual", "smart"
        self.memory = self._create_custom_checkpointer()  # ì²´í¬í¬ì¸í„° ë¨¼ì € ì´ˆê¸°í™”
        self.graph = self._create_graph()  # ê·¸ ë‹¤ìŒ ê·¸ë˜í”„ ìƒì„±
        
    @property
    def name(self) -> str:
        return "SolutionAgent"

    @property
    def description(self) -> str:
        return "ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ í’€ì´ë¥¼ ê°œì„ í•˜ëŠ” ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."

    def _llm(self, temperature: float = 0):
        return ChatOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"),
            model=OPENAI_LLM_MODEL,
            temperature=temperature,
        )

    def _create_graph(self) -> StateGraph:
        """HITL ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        print("ğŸ“š HITL LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ê¸°ë³¸ ë…¸ë“œë“¤
        graph.add_node("search_similarity", self._search_similar_questions)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        
        # HITL ë…¸ë“œë“¤
        graph.add_node("collect_user_feedback", self._collect_user_feedback)
        graph.add_node("process_feedback", self._process_feedback)
        graph.add_node("search_additional_info", self._search_additional_info)
        graph.add_node("improve_explanation", self._improve_explanation)
        graph.add_node("store", self._store_to_vector_db)

        # ì›Œí¬í”Œë¡œìš° ì„¤ì •
        graph.set_entry_point("search_similarity")
        graph.add_edge("search_similarity", "generate_solution")
        graph.add_edge("generate_solution", "validate")
        
        # HITL ì¡°ê±´ë¶€ ì—£ì§€
        graph.add_conditional_edges(
            "validate", 
            self._route_after_validation,
            {"ok": "store", "feedback_needed": "collect_user_feedback", "retry": "generate_solution"}
        )
        
        graph.add_conditional_edges(
            "collect_user_feedback",
            self._route_after_feedback,
            {"continue": "store", "improve": "process_feedback", "search": "search_additional_info"}
        )
        
        graph.add_edge("process_feedback", "improve_explanation")
        graph.add_edge("search_additional_info", "improve_explanation")
        graph.add_edge("improve_explanation", "collect_user_feedback")

        return graph.compile(checkpointer=self.memory)
    
    def _route_after_validation(self, state: SolutionState) -> str:
        """ê²€ì¦ í›„ ë¼ìš°íŒ… ê²°ì • (HITL ëª¨ë“œì— ë”°ë¼)"""
        if state["validated"]:
            return "ok"
        elif state.get("retry_count", 0) < 3:
            return "retry"
        else:
            # HITL ëª¨ë“œì— ë”°ë¼ ê²°ì •
            if self.hitl_mode == "auto":
                return "ok"  # ìë™ ëª¨ë“œì—ì„œëŠ” ê²€ì¦ ì‹¤íŒ¨í•´ë„ í†µê³¼
            elif self.hitl_mode == "smart":
                # ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ: í’€ì´ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ê²°ì •
                return self._smart_hitl_decision(state)
            else:  # manual ëª¨ë“œ
                return "feedback_needed"
    
    def _smart_hitl_decision(self, state: SolutionState) -> str:
        """ìŠ¤ë§ˆíŠ¸ HITL: í’€ì´ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ HITL ì ìš© ì—¬ë¶€ ê²°ì •"""
        # ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰
        quality_score = self._evaluate_solution_quality(state)
        
        print(f"ğŸ“Š í’€ì´ í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}/100")
        
        # í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ HITL ì ìš© ì—¬ë¶€ ê²°ì •
        if quality_score >= 80:
            print("âœ… í’ˆì§ˆì´ ë†’ìŒ - ìë™ í†µê³¼")
            return "ok"
        elif quality_score >= 60:
            print("âš ï¸ í’ˆì§ˆì´ ë³´í†µ - HITL ì ìš©")
            return "feedback_needed"
        else:
            print("âŒ í’ˆì§ˆì´ ë‚®ìŒ - HITL í•„ìˆ˜ ì ìš©")
            return "feedback_needed"
    
    def _evaluate_solution_quality(self, state: SolutionState) -> float:
        """ë‹¤ì°¨ì› í’€ì´ í’ˆì§ˆ í‰ê°€ (0-100ì )"""
        llm = self._llm(0)
        
        # 1. ì •í™•ì„± í‰ê°€ (30ì )
        accuracy_score = self._evaluate_accuracy(state, llm)
        
        # 2. ì™„ì„±ë„ í‰ê°€ (25ì )
        completeness_score = self._evaluate_completeness(state, llm)
        
        # 3. ì´í•´ë„ í‰ê°€ (25ì )
        clarity_score = self._evaluate_clarity(state, llm)
        
        # 4. ë…¼ë¦¬ì„± í‰ê°€ (20ì )
        logic_score = self._evaluate_logic(state, llm)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = (
            accuracy_score * 0.30 +
            completeness_score * 0.25 +
            clarity_score * 0.25 +
            logic_score * 0.20
        )
        
        # í’ˆì§ˆ ì ìˆ˜ë“¤ì„ stateì— ì €ì¥
        state["quality_scores"] = {
            "accuracy": accuracy_score,
            "completeness": completeness_score,
            "clarity": clarity_score,
            "logic": logic_score
        }
        state["total_quality_score"] = total_score
        
        print(f"ğŸ“ˆ í’ˆì§ˆ ì„¸ë¶€ ì ìˆ˜:")
        print(f"   ì •í™•ì„±: {accuracy_score:.1f}/100 (ê°€ì¤‘ì¹˜: 30%)")
        print(f"   ì™„ì„±ë„: {completeness_score:.1f}/100 (ê°€ì¤‘ì¹˜: 25%)")
        print(f"   ì´í•´ë„: {clarity_score:.1f}/100 (ê°€ì¤‘ì¹˜: 25%)")
        print(f"   ë…¼ë¦¬ì„±: {logic_score:.1f}/100 (ê°€ì¤‘ì¹˜: 20%)")
        print(f"   ì´ì : {total_score:.1f}/100")
        
        return total_score
    
    def _evaluate_accuracy(self, state: SolutionState, llm) -> float:
        """ì •í™•ì„± í‰ê°€ (30ì )"""
        prompt = f"""
        ë‹¤ìŒ í’€ì´ì˜ ì •í™•ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì œ: {state['user_problem']}
        ë³´ê¸°: {state['user_problem_options']}
        ì •ë‹µ: {state['generated_answer']}
        í’€ì´: {state['generated_explanation']}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
        1. ì •ë‹µì´ ì˜¬ë°”ë¥¸ê°€? (10ì )
        2. í’€ì´ ê³¼ì •ì´ ì •í™•í•œê°€? (10ì )
        3. ê¸°ìˆ ì  ë‚´ìš©ì´ ì •í™•í•œê°€? (10ì )
        
        ê° í•­ëª©ë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ì´ì ì„ ê³„ì‚°í•˜ì—¬ 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        try:
            response = llm.invoke(prompt)
            score_text = response.content.strip()
            # ìˆ«ìë§Œ ì¶”ì¶œ
            score_match = re.search(r'(\d+)', score_text)
            if score_match:
                return min(100, max(0, float(score_match.group(1))))
            return 70  # ê¸°ë³¸ê°’
        except:
            return 70
    
    def _evaluate_completeness(self, state: SolutionState, llm) -> float:
        """ì™„ì„±ë„ í‰ê°€ (25ì )"""
        prompt = f"""
        ë‹¤ìŒ í’€ì´ì˜ ì™„ì„±ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì œ: {state['user_problem']}
        í’€ì´: {state['generated_explanation']}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
        1. í•µì‹¬ ê°œë…ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ”ê°€? (10ì )
        2. ë‹¨ê³„ë³„ ì„¤ëª…ì´ ì¶©ë¶„í•œê°€? (10ì )
        3. ì˜ˆì‹œë‚˜ ë¹„ìœ ê°€ ì ì ˆí•œê°€? (5ì )
        
        ê° í•­ëª©ë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ì´ì ì„ ê³„ì‚°í•˜ì—¬ 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        try:
            response = llm.invoke(prompt)
            score_text = response.content.strip()
            score_match = re.search(r'(\d+)', score_text)
            if score_match:
                return min(100, max(0, float(score_match.group(1))))
            return 70
        except:
            return 70
    
    def _evaluate_clarity(self, state: SolutionState, llm) -> float:
        """ì´í•´ë„ í‰ê°€ (25ì )"""
        prompt = f"""
        ë‹¤ìŒ í’€ì´ì˜ ì´í•´ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        í’€ì´: {state['generated_explanation']}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
        1. ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì½ê¸° ì‰¬ìš´ê°€? (10ì )
        2. ì „ë¬¸ ìš©ì–´ê°€ ì ì ˆíˆ ì„¤ëª…ë˜ì—ˆëŠ”ê°€? (10ì )
        3. ì „ì²´ì ì¸ íë¦„ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€? (5ì )
        
        ê° í•­ëª©ë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ì´ì ì„ ê³„ì‚°í•˜ì—¬ 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        try:
            response = llm.invoke(prompt)
            score_text = response.content.strip()
            score_match = re.search(r'(\d+)', score_text)
            if score_match:
                return min(100, max(0, float(score_match.group(1))))
            return 70
        except:
            return 70
    
    def _evaluate_logic(self, state: SolutionState, llm) -> float:
        """ë…¼ë¦¬ì„± í‰ê°€ (20ì )"""
        prompt = f"""
        ë‹¤ìŒ í’€ì´ì˜ ë…¼ë¦¬ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì œ: {state['user_problem']}
        í’€ì´: {state['generated_explanation']}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
        1. ë…¼ë¦¬ì  ì¶”ë¡ ì´ ì˜¬ë°”ë¥¸ê°€? (10ì )
        2. ì¸ê³¼ê´€ê³„ê°€ ëª…í™•í•œê°€? (10ì )
        
        ê° í•­ëª©ë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ì´ì ì„ ê³„ì‚°í•˜ì—¬ 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        try:
            response = llm.invoke(prompt)
            score_text = response.content.strip()
            score_match = re.search(r'(\d+)', score_text)
            if score_match:
                return min(100, max(0, float(score_match.group(1))))
            return 70
        except:
            return 70
    
    def _route_after_feedback(self, state: SolutionState) -> str:
        """ì‚¬ìš©ì í”¼ë“œë°± í›„ ë¼ìš°íŒ… ê²°ì •"""
        feedback = state.get("user_feedback", "").lower()
        
        if "ë§Œì¡±" in feedback or "ì¢‹ìŒ" in feedback or "ì´í•´" in feedback:
            return "continue"
        elif "ê²€ìƒ‰" in feedback or "ì°¾ì•„" in feedback or "ì„¤ëª…" in feedback:
            return "search"
        else:
            return "improve"
    
    #----------------------------------------nodes------------------------------------------------------

    def _search_similar_questions(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        print(state["user_problem"], state["user_problem_options"])
        
        # vectorstoreëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì•„ì•¼ í•¨ (ì§ë ¬í™” ë¬¸ì œ ë°©ì§€)
        vectorstore = getattr(self, '_current_vectorstore', None)
        if vectorstore is None:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì–´ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            state["retrieved_docs"] = []
            state["similar_questions_text"] = ""
            print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ (ê±´ë„ˆëœ€)")
            return state
        
        try:
            results = vectorstore.similarity_search(state["user_problem"], k=3)
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results = []
        
        similar_questions = []
        for i, doc in enumerate(results):
            metadata = doc.metadata
            options = json.loads(metadata.get("options", "[]"))
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")
            subject = metadata.get("subject", "ê¸°íƒ€")

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i+1}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer}
                í’€ì´: {explanation}
                ê³¼ëª©: {subject}
                """
            similar_questions.append(formatted)
        
        state["retrieved_docs"] = results
        state["similar_questions_text"] = "\n\n".join(similar_questions) 

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ.")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state

    def _generate_solution(self, state: SolutionState) -> SolutionState:
        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm_gen = self._llm(0.5)  

        similar_problems = state.get("similar_questions_text", "")
        print("ìœ ì‚¬ ë¬¸ì œë“¤:\n", similar_problems[:100])

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_input_txt']}

            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {similar_problems}

            1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œì˜ **ì •ë‹µ**ì˜ ë³´ê¸° ë²ˆí˜¸ë¥¼ ì •ë‹µìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
            3. ì´ ë¬¸ì œì˜ ê³¼ëª©ì„ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ê³¼ëª© 5ê°œ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒìœ¼ë¡œ ì§€ì •í•´ ì£¼ì„¸ìš”. ìœ ì‚¬ ë¬¸ì œë“¤ì˜ ê³¼ëª©ì„ ì°¸ê³ í•´ë„ ì¢‹ìŠµë‹ˆë‹¤. [ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„, ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•, í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©, ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬]

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
            ê³¼ëª©: ...
        """

        response = llm_gen.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        subject_match = re.search(r"ê³¼ëª©:\s*(.+)", result)
        
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["generated_subject"] = subject_match.group(1).strip() if subject_match else "ê¸°íƒ€"

        state["chat_history"].append(f"Q: {state['user_input_txt']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state

    # âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

    def _validate_solution(self, state: SolutionState) -> SolutionState:
        print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
        
        llm = self._llm(0)

        validation_prompt = f"""
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_input_txt']}

        ë¬¸ì œ ì§ˆë¬¸: {state['user_problem']}
        ë¬¸ì œ ë³´ê¸°: {state['user_problem_options']}

        ìƒì„±ëœ ì •ë‹µ: {state['generated_answer']}
        ìƒì„±ëœ í’€ì´: {state['generated_explanation']}
        ìƒì„±ëœ ê³¼ëª©: {state['generated_subject']}

        ìƒì„±ëœ í•´ë‹µê³¼ í’€ì´, ê³¼ëª©ì´ ë¬¸ì œì™€ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
        ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        validation_response = llm.invoke(validation_prompt)
        result_text = validation_response.content.strip().lower()

        # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
        print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
        
        # manual ëª¨ë“œì—ì„œëŠ” í•­ìƒ HITL ì ìš©
        if self.hitl_mode == "manual":
            state["validated"] = False
            print(" Manual ëª¨ë“œ: ê°•ì œë¡œ HITL ì ìš©")
        else:
            state["validated"] = "ë„¤" in result_text
        
        if not state["validated"]:
            state["retry_count"] = state.get("retry_count", 0) + 1
            print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì¬ì‹œë„ {state['retry_count']}/5)")
        else:
            print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
            
        return state

    def _collect_user_feedback(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ìë¡œë¶€í„° í’€ì´ì— ëŒ€í•œ í”¼ë“œë°± ìˆ˜ì§‘ (interrupt ì‚¬ìš©)"""
        print("\nğŸ’¬ [HITL] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘")
        
        # í˜„ì¬ í’€ì´ ìƒíƒœ í‘œì‹œ
        print(f"\nğŸ“ í˜„ì¬ í’€ì´:")
        print(f"ì •ë‹µ: {state['generated_answer']}")
        print(f"í’€ì´: {state['generated_explanation']}")
        print(f"ê³¼ëª©: {state['generated_subject']}")
        
        # interruptë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        feedback_query = {
            "query": "í’€ì´ì— ëŒ€í•œ ì˜ê²¬ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”",
            "examples": {
                "ì´í•´ë¨": ["ì´í•´ê°€ ë©ë‹ˆë‹¤", "ì¢‹ìŠµë‹ˆë‹¤", "ë§Œì¡±í•©ë‹ˆë‹¤", "ì¶©ë¶„í•©ë‹ˆë‹¤", "ê´œì°®ìŠµë‹ˆë‹¤"],
                "ë” ì‰¬ìš´ í’€ì´ í•„ìš”": ["ë” ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”", "ë³µì¡í•´ìš”", "ì–´ë ¤ì›Œìš”", "ê°„ë‹¨í•˜ê²Œ", "ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ"],
                "ìš©ì–´ ì„¤ëª… í•„ìš”": ["ì´ ìš©ì–´ê°€ ë­”ì§€ ëª¨ë¥´ê² ì–´ìš”", "ì„¤ëª…ì´ ë¶€ì¡±í•´ìš”", "ìš©ì–´ë¥¼ ë” ìì„¸íˆ", "ê°œë… ì„¤ëª… ì¶”ê°€"]
            },
            "current_problem": state['user_problem'],
            "current_answer": state['generated_answer'],
            "current_explanation": state['generated_explanation']
        }
        
        print("\nğŸ’­ í’€ì´ì— ëŒ€í•œ ì˜ê²¬ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”:")
        print("ì˜ˆì‹œ:")
        print("- 'ì´í•´ê°€ ë©ë‹ˆë‹¤', 'ì¢‹ìŠµë‹ˆë‹¤', 'ë§Œì¡±í•©ë‹ˆë‹¤' â†’ ì´í•´ë¨")
        print("- 'ë” ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”', 'ë³µì¡í•´ìš”', 'ì–´ë ¤ì›Œìš”' â†’ ë” ì‰¬ìš´ í’€ì´ í•„ìš”")
        print("- 'ì´ ìš©ì–´ê°€ ë­”ì§€ ëª¨ë¥´ê² ì–´ìš”', 'ì„¤ëª…ì´ ë¶€ì¡±í•´ìš”' â†’ ìš©ì–´ ì„¤ëª… í•„ìš”")
        
        # interruptë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì¼ì‹œ ì¤‘ë‹¨ ë° ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
        human_response = interrupt(feedback_query)
        
        # Command ê°ì²´ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
        if isinstance(human_response, dict) and "data" in human_response:
            user_input = human_response["data"]
        else:
            user_input = str(human_response)
        
        if not user_input:
            # ì…ë ¥ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
            user_input = "í’€ì´ë¥¼ ë” ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            print(f"âš ï¸ ì…ë ¥ì´ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {user_input}")
        
        state["user_feedback"] = user_input
        state["interaction_count"] = state.get("interaction_count", 0) + 1
        
        print(f"âœ… í”¼ë“œë°± ìˆ˜ì§‘ ì™„ë£Œ: {user_input}")
        
        return state

    def _route_after_feedback(self, state: SolutionState) -> str:
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ LLMì´ ë¶„ì„í•˜ì—¬ ë¼ìš°íŒ… ê²°ì •"""
        print(f"\nğŸ§  [HITL] ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„ ì¤‘...")
        
        llm = self._llm(0.1)  # ì¼ê´€ëœ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
        
        analysis_prompt = f"""
        ì‚¬ìš©ìê°€ ì œì‹œí•œ í’€ì´ì— ëŒ€í•œ í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

        ì‚¬ìš©ì í”¼ë“œë°±: {state['user_feedback']}

        ë¶„ë¥˜ ê¸°ì¤€:
        1. "ì´í•´ë¨" - ì‚¬ìš©ìê°€ í’€ì´ë¥¼ ì´í•´í–ˆê³  ë§Œì¡±í•˜ëŠ” ê²½ìš°
           ì˜ˆì‹œ: "ì´í•´ê°€ ë©ë‹ˆë‹¤", "ì¢‹ìŠµë‹ˆë‹¤", "ë§Œì¡±í•©ë‹ˆë‹¤", "ì¶©ë¶„í•©ë‹ˆë‹¤", "ê´œì°®ìŠµë‹ˆë‹¤"
        
        2. "ë” ì‰¬ìš´ í’€ì´ í•„ìš”" - ì‚¬ìš©ìê°€ í’€ì´ê°€ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ì–´ë µë‹¤ê³  ëŠë¼ëŠ” ê²½ìš°
           ì˜ˆì‹œ: "ë” ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”", "ë³µì¡í•´ìš”", "ì–´ë ¤ì›Œìš”", "ê°„ë‹¨í•˜ê²Œ", "ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ"
        
        3. "ìš©ì–´ ì„¤ëª… í•„ìš”" - ì‚¬ìš©ìê°€ íŠ¹ì • ìš©ì–´ë‚˜ ê°œë…ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°
           ì˜ˆì‹œ: "ì´ ìš©ì–´ê°€ ë­”ì§€ ëª¨ë¥´ê² ì–´ìš”", "ì„¤ëª…ì´ ë¶€ì¡±í•´ìš”", "ìš©ì–´ë¥¼ ë” ìì„¸íˆ", "ê°œë… ì„¤ëª… ì¶”ê°€"

        ìœ„ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:
        - ì´í•´ë¨
        - ë” ì‰¬ìš´ í’€ì´ í•„ìš”  
        - ìš©ì–´ ì„¤ëª… í•„ìš”
        """
        
        try:
            response = llm.invoke(analysis_prompt)
            feedback_type = response.content.strip()
            
            print(f"ğŸ“Š LLM ë¶„ì„ ê²°ê³¼: {feedback_type}")
            
            # ë¶„ë¥˜ ê²°ê³¼ë¥¼ stateì— ì €ì¥
            if "ì´í•´ë¨" in feedback_type:
                state["feedback_type"] = "comprehension"
                print("âœ… ì‚¬ìš©ì ì˜ë„: ì´í•´ë¨ â†’ ë°”ë¡œ ì €ì¥ìœ¼ë¡œ ì§„í–‰")
                return "continue"
            elif "ë” ì‰¬ìš´ í’€ì´ í•„ìš”" in feedback_type:
                state["feedback_type"] = "improvement"
                print("âš ï¸ ì‚¬ìš©ì ì˜ë„: ë” ì‰¬ìš´ í’€ì´ í•„ìš” â†’ í’€ì´ ê°œì„  ì§„í–‰")
                return "improve"
            elif "ìš©ì–´ ì„¤ëª… í•„ìš”" in feedback_type:
                state["feedback_type"] = "clarification"
                print("ğŸ” ì‚¬ìš©ì ì˜ë„: ìš©ì–´ ì„¤ëª… í•„ìš” â†’ ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰")
                return "search"
            else:
                # ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ê°œì„  ì§„í–‰
                print("âš ï¸ ëª…í™•í•˜ì§€ ì•Šì€ í”¼ë“œë°± â†’ ê¸°ë³¸ê°’ìœ¼ë¡œ ê°œì„  ì§„í–‰")
                state["feedback_type"] = "improvement"
                return "improve"
                
        except Exception as e:
            print(f"âš ï¸ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("âš ï¸ ê¸°ë³¸ê°’ìœ¼ë¡œ ê°œì„  ì§„í–‰")
            state["feedback_type"] = "improvement"
            return "improve"

    def _process_feedback(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¶„ì„í•˜ê³  ì²˜ë¦¬ ë°©í–¥ ê²°ì •"""
        print(f"\nğŸ”„ [HITL] í”¼ë“œë°± ì²˜ë¦¬: {state['feedback_type']}")
        
        feedback_type = state.get("feedback_type", "improvement")
        
        if feedback_type == "improvement":
            print("ğŸ“ í’€ì´ë¥¼ ë” ì‰½ê²Œ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤.")
        elif feedback_type == "clarification":
            print("ğŸ” íŠ¹ì • ìš©ì–´ì— ëŒ€í•´ ë” ìì„¸íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.")
        elif feedback_type == "search":
            print("ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê² ìŠµë‹ˆë‹¤.")
        
        return state

    def _search_additional_info(self, state: SolutionState) -> SolutionState:
        """ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ì •ë³´ ê²€ìƒ‰"""
        print(f"\nğŸ” [HITL] ì¶”ê°€ ì •ë³´ ê²€ìƒ‰ ì‹œì‘")
        
        if not SEARCH_AGENT_AVAILABLE:
            print("âš ï¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            state["search_results"] = "ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state
        
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± - ìœ ì € í”¼ë“œë°± í¬í•¨
            user_feedback = state.get('user_feedback', '')
            search_query = f"{state['user_problem']} {state['generated_explanation']} {user_feedback}"
            
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query[:100]}...")
            print(f"ğŸ’¬ ìœ ì € í”¼ë“œë°±: {user_feedback}")
            
            # ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‹¤í–‰ (invoke ë©”ì„œë“œ ì‚¬ìš©)
            search_results = retrieve_agent().invoke({
                "retrieval_question": search_query
            })
            
            if search_results and "answer" in search_results:
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                answer_content = search_results.get("answer", "")
                state["search_results"] = f"[ê²€ìƒ‰ê²°ê³¼]\n{answer_content}"
                print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: ë‹µë³€ ê¸¸ì´ {len(answer_content)}ì")
            else:
                state["search_results"] = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["search_results"] = f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
        return state

    def _improve_explanation(self, state: SolutionState) -> SolutionState:
        """ì‚¬ìš©ì í”¼ë“œë°±ê³¼ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’€ì´ ê°œì„ """
        print(f"\nâœ¨ [HITL] í’€ì´ ê°œì„  ì‹œì‘")
        
        llm = self._llm(0.3)
        feedback_type = state.get("feedback_type", "improvement")
        
        # í”¼ë“œë°± ìœ í˜•ì— ë”°ë¥¸ ê°œì„  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if feedback_type == "improvement":
            improvement_prompt = f"""
            ì›ë³¸ ë¬¸ì œ: {state['user_problem']}
            ì›ë³¸ ë³´ê¸°: {state['user_problem_options']}
            ì›ë³¸ ì •ë‹µ: {state['generated_answer']}
            ì›ë³¸ í’€ì´: {state['generated_explanation']}
            
            ì‚¬ìš©ì í”¼ë“œë°±: {state['user_feedback']}
            
            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’€ì´ë¥¼ ë” ì‰½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”:
            
            ê°œì„  ìš”êµ¬ì‚¬í•­:
            1. ë³µì¡í•œ ìš©ì–´ë¥¼ ì‰¬ìš´ ë§ë¡œ ë°”ê¾¸ê¸°
            2. ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê¸°
            3. ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨í•˜ê²Œ
            4. í•„ìš”ì‹œ ë¹„ìœ ë‚˜ ì˜ˆì‹œ ì¶”ê°€í•˜ê¸°
            
            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: [ì •ë‹µ]
            í’€ì´: [ê°œì„ ëœ í’€ì´]
            ê³¼ëª©: [ê³¼ëª©]
            ê°œì„ ì‚¬í•­: [ì–´ë–¤ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê°œì„ í–ˆëŠ”ì§€ ì„¤ëª…]
            """
            
        elif feedback_type == "clarification":
            improvement_prompt = f"""
            ì›ë³¸ ë¬¸ì œ: {state['user_problem']}
            ì›ë³¸ ë³´ê¸°: {state['user_problem_options']}
            ì›ë³¸ ì •ë‹µ: {state['generated_answer']}
            ì›ë³¸ í’€ì´: {state['generated_explanation']}
            
            ì‚¬ìš©ì í”¼ë“œë°±: {state['user_feedback']}
            
            ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼:
            {state.get('search_results', 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ')}
            
            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’€ì´ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:
            
            ê°œì„  ìš”êµ¬ì‚¬í•­:
            1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìš©ì–´ë‚˜ ê°œë…ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª… ì¶”ê°€
            2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ê´€ë ¨ ê°œë… ë³´ê°•
            3. ìš©ì–´ì˜ ì •ì˜ì™€ ì˜ˆì‹œ ì œê³µ
            4. ì „ì²´ì ì¸ ë§¥ë½ì—ì„œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…
            5. ì‚¬ìš©ì í”¼ë“œë°±ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ì„¤ëª…
             
            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: [ì •ë‹µ]
            í’€ì´: [ê°œì„ ëœ í’€ì´]
            ê³¼ëª©: [ê³¼ëª©]
            ê°œì„ ì‚¬í•­: [ì–´ë–¤ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê°œì„ í–ˆëŠ”ì§€ ì„¤ëª…]
            """
            
        else:  # ê¸°ë³¸ ê°œì„ 
            improvement_prompt = f"""
            ì›ë³¸ ë¬¸ì œ: {state['user_problem']}
            ì›ë³¸ ë³´ê¸°: {state['user_problem_options']}
            ì›ë³¸ ì •ë‹µ: {state['generated_answer']}
            ì›ë³¸ í’€ì´: {state['generated_explanation']}
            
            ì‚¬ìš©ì í”¼ë“œë°±: {state['user_feedback']}
            
            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’€ì´ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:
            
            ê°œì„  ìš”êµ¬ì‚¬í•­:
            1. ì‚¬ìš©ì í”¼ë“œë°±ì— ë§ê²Œ ì ì ˆíˆ ê°œì„ 
            2. ì´í•´í•˜ê¸° ì‰½ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…
            3. í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì •ë³´ë‚˜ ì˜ˆì‹œ ì œê³µ
            
            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: [ì •ë‹µ]
            í’€ì´: [ê°œì„ ëœ í’€ì´]
            ê³¼ëª©: [ê³¼ëª©]
            ê°œì„ ì‚¬í•­: [ì–´ë–¤ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê°œì„ í–ˆëŠ”ì§€ ì„¤ëª…]
            """
        
        try:
            response = llm.invoke(improvement_prompt)
            result = response.content.strip()
            
            # ê°œì„ ëœ ê²°ê³¼ íŒŒì‹±
            answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
            explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
            subject_match = re.search(r"ê³¼ëª©:\s*(.+)", result)
            improvement_match = re.search(r"ê°œì„ ì‚¬í•­:\s*(.+)", result, re.DOTALL)
            
            if answer_match:
                state["generated_answer"] = answer_match.group(1).strip()
            if explanation_match:
                state["improved_explanation"] = explanation_match.group(1).strip()
            if subject_match:
                state["generated_subject"] = subject_match.group(1).strip()
            if improvement_match:
                improvement_note = improvement_match.group(1).strip()
            else:
                improvement_note = f"ì‚¬ìš©ì í”¼ë“œë°±({feedback_type})ì— ë”°ë¼ í’€ì´ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤."
            
            # ê°œì„ ëœ í’€ì´ë¥¼ ë©”ì¸ í’€ì´ë¡œ ì„¤ì •
            if state.get("improved_explanation"):
                state["generated_explanation"] = state["improved_explanation"]
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ê°œì„  ê³¼ì • ì¶”ê°€
            state["chat_history"].append(f"ê°œì„ ({feedback_type}): {improvement_note}")
            
            print(f"âœ… í’€ì´ ê°œì„  ì™„ë£Œ: {improvement_note}")
            print(f"ğŸ“ ê°œì„  ìœ í˜•: {feedback_type}")
            
        except Exception as e:
            print(f"âš ï¸ í’€ì´ ê°œì„  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["improved_explanation"] = state["generated_explanation"]
        
        return state

    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:  
        print("\nğŸ§© [4ë‹¨ê³„] ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ì‹œì‘")

        # vectorstoreëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì•„ì•¼ í•¨ (ì§ë ¬í™” ë¬¸ì œ ë°©ì§€)
        vectorstore = getattr(self, '_current_vectorstore', None)
        if vectorstore is None:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            try:
                # ì¤‘ë³µ ë¬¸ì œ í™•ì¸
                similar = vectorstore.similarity_search(state["user_problem"], k=1)
                if similar and state["user_problem"].strip() in similar[0].page_content:
                    print("âš ï¸ ë™ì¼í•œ ë¬¸ì œê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥ ìƒëµ")
                else:
                    # ë¬¸ì œ, í•´ë‹µ, í’€ì´ë¥¼ ê°ê° metadataë¡œ ì €ì¥
                    doc = Document(
                        page_content=state["user_problem"],
                        metadata={
                            "options": json.dumps(state.get("user_problem_options", [])), 
                            "answer": state["generated_answer"],
                            "explanation": state["generated_explanation"],
                            "subject": state["generated_subject"],
                        }
                    )
                    vectorstore.add_documents([doc])
                    print("âœ… ë¬¸ì œ+í•´ë‹µ+í’€ì´ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("   - ê²°ê³¼ë§Œ ì €ì¥í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

        # ê²°ê³¼ë¥¼ stateì— ì €ì¥ (í•­ìƒ ì‹¤í–‰)
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì‹œì‘:")
        print(f"   - í˜„ì¬ ë¬¸ì œ: {state['user_problem'][:50]}...")
        print(f"   - ìƒì„±ëœ ì •ë‹µ: {state['generated_answer'][:30]}...")
        print(f"   - ê²€ì¦ ìƒíƒœ: {state['validated']}")
        print(f"   - ìƒí˜¸ì‘ìš© íšŸìˆ˜: {state.get('interaction_count', 0)}")
        
        item = {
            "user_problem": state["user_problem"],
            "user_problem_options": state["user_problem_options"],
            "generated_answer": state["generated_answer"],
            "generated_explanation": state["generated_explanation"],
            "generated_subject": state["generated_subject"],
            "validated": state["validated"],
            "interaction_count": state.get("interaction_count", 0),
            "user_feedback": state.get("user_feedback", ""),
            "quality_scores": state.get("quality_scores", {}),
            "total_quality_score": state.get("total_quality_score", 0.0),
            "chat_history": state.get("chat_history", [])
        }
        
        state["results"].append(item)
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(state['results'])}ê°œ")
        
        return state

    def invoke(
            self, 
            user_input_txt: str,
            user_problem: str,
            user_problem_options: List[str],
            vectorstore: Optional[Milvus] = None,
            recursion_limit: int = 1000,
        ) -> Dict:
        
        print(f"ğŸš€ HITL ëª¨ë“œ: {self.hitl_mode}")
        
        # ë‹¨ì¼ ë¬¸ì œ ì²˜ë¦¬
        return self._process_single_problem(
            user_input_txt, user_problem, user_problem_options, vectorstore, recursion_limit
        )
    
    def invoke_batch(
            self,
            problems: List[Dict[str, Any]],  # [{"problem": "...", "options": [...], "input_txt": "..."}]
            vectorstore: Optional[Milvus] = None,
            recursion_limit: int = 1000,
            batch_feedback: bool = True,  # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í”¼ë“œë°± ìˆ˜ì§‘
        ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ë¬¸ì œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (HITL ìµœì í™”)
        
        Args:
            problems: ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
            vectorstore: ë²¡í„°ìŠ¤í† ì–´
            recursion_limit: ì¬ê·€ ì œí•œ
            batch_feedback: ë°°ì¹˜ ë‹¨ìœ„ í”¼ë“œë°± ì—¬ë¶€
        """
        print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(problems)}ê°œ ë¬¸ì œ, HITL ëª¨ë“œ: {self.hitl_mode}")
        
        if self.hitl_mode == "auto":
            # ìë™ ëª¨ë“œ: ëª¨ë“  ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            return self._process_batch_auto(problems, vectorstore, recursion_limit)
        elif batch_feedback and self.hitl_mode in ["manual", "smart"]:
            # ë°°ì¹˜ í”¼ë“œë°± ëª¨ë“œ: ëª¨ë“  ë¬¸ì œ ì²˜ë¦¬ í›„ í•œ ë²ˆì— í”¼ë“œë°±
            return self._process_batch_with_feedback(problems, vectorstore, recursion_limit)
        else:
            # ê°œë³„ HITL ëª¨ë“œ: ë¬¸ì œë³„ë¡œ ê°œë³„ ì²˜ë¦¬
            return self._process_batch_individual(problems, vectorstore, recursion_limit)
    
    def _process_single_problem(
            self,
            user_input_txt: str,
            user_problem: str,
            user_problem_options: List[str],
            vectorstore: Optional[Milvus] = None,
            recursion_limit: int = 1000,
        ) -> Dict:

        # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if vectorstore is None:
            try:
                embedding_model = HuggingFaceEmbeddings(
                    model_name="jhgan/ko-sroberta-multitask",
                    model_kwargs={"device": "cpu"}
                )

                if "default" in connections.list_connections():
                    connections.disconnect("default")
                connections.connect(alias="default", host="localhost", port="19530")

                vectorstore = Milvus(
                    embedding_function=embedding_model,
                    collection_name="problems",
                    connection_args={"host": "localhost", "port": "19530"}
                )
                print("âœ… Milvus ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
                print("   - ë²¡í„°ìŠ¤í† ì–´ ì—†ì´ ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤.")
                vectorstore = None
        
        # vectorstoreë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì • (ì§ë ¬í™” ë¬¸ì œ ë°©ì§€)
        self._current_vectorstore = vectorstore
        
        initial_state: SolutionState = {
            "user_input_txt": user_input_txt,
            "user_problem": user_problem,
            "user_problem_options": user_problem_options,
            "retrieved_docs": [],
            "similar_questions_text": "",
            "generated_answer": "",
            "generated_explanation": "",
            "generated_subject": "",
            "validated": False,
            "retry_count": 0,
            "user_feedback": "",
            "feedback_type": "",
            "search_results": "",
            "improved_explanation": "",
            "interaction_count": 0,
            "max_interactions": self.max_interactions,
            "quality_scores": {},
            "total_quality_score": 0.0,
            "quality_threshold": 80.0,  # ê¸°ë³¸ ì„ê³„ê°’
            "results": [],
            "chat_history": []
        }
        
        final_state = self.graph.invoke(
            initial_state, 
            config={
                "recursion_limit": recursion_limit,
                "configurable": {
                    "thread_id": "default",
                    "checkpoint_id": "default"
                }
            }
        )

        # ê²°ê³¼ í™•ì¸ ë° ë””ë²„ê¹…
        results = final_state.get("results", [])
        print(f"   - ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        
        if results:
            for i, result in enumerate(results):
                print(f"   - ê²°ê³¼ {i+1}: {result.get('user_problem', '')[:30]}...")
                print(f"     ìƒí˜¸ì‘ìš© íšŸìˆ˜: {result.get('interaction_count', 0)}")
        else:
            print("   âš ï¸ resultsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            print(f"   - final_state ë‚´ìš©: {final_state}")
        
        return final_state
    
    def _process_batch_auto(self, problems: List[Dict[str, Any]], vectorstore, recursion_limit) -> Dict[str, Any]:
        """ìë™ ëª¨ë“œ: ëª¨ë“  ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬ (HITL ì—†ìŒ)"""
        print("ğŸ¤– ìë™ ëª¨ë“œ: ëª¨ë“  ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        results = []
        for i, problem in enumerate(problems):
            print(f"\nğŸ“ ë¬¸ì œ {i+1}/{len(problems)} ì²˜ë¦¬ ì¤‘...")
            
            # HITLì„ ë¹„í™œì„±í™”í•˜ê³  ìë™ ì²˜ë¦¬
            original_mode = self.hitl_mode
            self.hitl_mode = "auto"
            
            try:
                result = self._process_single_problem(
                    problem.get("input_txt", ""),
                    problem.get("problem", ""),
                    problem.get("options", []),
                    vectorstore,
                    recursion_limit
                )
                results.append(result)
            finally:
                self.hitl_mode = original_mode
        
        return {
            "mode": "auto",
            "total_problems": len(problems),
            "processed_problems": len(results),
            "results": results
        }
    
    def _process_batch_with_feedback(self, problems: List[Dict[str, Any]], vectorstore, recursion_limit) -> Dict[str, Any]:
        """ë°°ì¹˜ í”¼ë“œë°± ëª¨ë“œ: ëª¨ë“  ë¬¸ì œ ì²˜ë¦¬ í›„ í•œ ë²ˆì— í”¼ë“œë°± ìˆ˜ì§‘"""
        print("ğŸ’¬ ë°°ì¹˜ í”¼ë“œë°± ëª¨ë“œ: ëª¨ë“  ë¬¸ì œë¥¼ ì²˜ë¦¬í•œ í›„ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬
        batch_results = []
        for i, problem in enumerate(problems):
            print(f"\nğŸ“ ë¬¸ì œ {i+1}/{len(problems)} ìë™ ì²˜ë¦¬ ì¤‘...")
            
            # ì„ì‹œë¡œ ìë™ ëª¨ë“œë¡œ ì„¤ì •
            original_mode = self.hitl_mode
            self.hitl_mode = "auto"
            
            try:
                result = self._process_single_problem(
                    problem.get("input_txt", ""),
                    problem.get("problem", ""),
                    problem.get("options", []),
                    vectorstore,
                    recursion_limit
                )
                batch_results.append({
                    "problem_data": problem,
                    "result": result,
                    "needs_improvement": not result.get("validated", False)
                })
            finally:
                self.hitl_mode = original_mode
        
        # 2ë‹¨ê³„: ê°œì„ ì´ í•„ìš”í•œ ë¬¸ì œë“¤ë§Œ ì‚¬ìš©ìì—ê²Œ í”¼ë“œë°± ìš”ì²­
        improvement_candidates = [r for r in batch_results if r["needs_improvement"]]
        
        if improvement_candidates:
            print(f"\nğŸ” {len(improvement_candidates)}ê°œ ë¬¸ì œì— ëŒ€í•´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ë¬¸ì œë¥¼ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤.")
            
            # ì‚¬ìš©ìì—ê²Œ ë°°ì¹˜ í”¼ë“œë°± ìš”ì²­
            self._collect_batch_feedback(improvement_candidates)
            
            # í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œ ê°œì„ 
            for candidate in improvement_candidates:
                self._improve_problem_with_feedback(candidate)
        else:
            print("âœ… ëª¨ë“  ë¬¸ì œê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return {
            "mode": "batch_feedback",
            "total_problems": len(problems),
            "auto_processed": len(batch_results) - len(improvement_candidates),
            "improved_with_feedback": len(improvement_candidates),
            "results": batch_results
        }
    
    def _process_batch_individual(self, problems: List[Dict[str, Any]], vectorstore, recursion_limit) -> Dict[str, Any]:
        """ê°œë³„ HITL ëª¨ë“œ: ë¬¸ì œë³„ë¡œ ê°œë³„ HITL ì²˜ë¦¬"""
        print("ğŸ‘¤ ê°œë³„ HITL ëª¨ë“œ: ê° ë¬¸ì œë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        results = []
        for i, problem in enumerate(problems):
            print(f"\nğŸ“ ë¬¸ì œ {i+1}/{len(problems)} HITL ì²˜ë¦¬ ì¤‘...")
            
            result = self._process_single_problem(
                problem.get("input_txt", ""),
                problem.get("problem", ""),
                problem.get("options", []),
                vectorstore,
                recursion_limit
            )
            results.append(result)
        
        return {
            "mode": "individual_hitl",
            "total_problems": len(problems),
            "results": results
        }
    
    def _collect_batch_feedback(self, improvement_candidates: List[Dict]) -> None:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""
        print(f"\nğŸ’¬ {len(improvement_candidates)}ê°œ ë¬¸ì œì— ëŒ€í•œ ë°°ì¹˜ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        # ë¬¸ì œ ìš”ì•½ ì œê³µ
        for i, candidate in enumerate(improvement_candidates):
            problem = candidate["problem_data"]
            result = candidate["result"]
            print(f"\në¬¸ì œ {i+1}: {problem.get('problem', '')[:50]}...")
            print(f"í˜„ì¬ í’€ì´: {result.get('generated_explanation', '')[:100]}...")
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì•¼ í•¨
        print("\nğŸ’­ ì „ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì„ ê°œì„ í•˜ë©´ ì¢‹ì„ì§€ í”¼ë“œë°±ì„ ì£¼ì„¸ìš”.")
        print("ì˜ˆì‹œ: 'í’€ì´ë¥¼ ë” ì‰½ê²Œ', 'ìš©ì–´ ì„¤ëª… ì¶”ê°€', 'ì „ì²´ì ìœ¼ë¡œ ë§Œì¡±' ë“±")
        
        # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ìë™ í”¼ë“œë°±
        batch_feedback = "í’€ì´ë¥¼ ë” ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³ , ì¤‘ìš”í•œ ìš©ì–´ì— ëŒ€í•œ ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
        print(f"ğŸ“ ë°°ì¹˜ í”¼ë“œë°±: {batch_feedback}")
        
        # ê° ë¬¸ì œì— ë°°ì¹˜ í”¼ë“œë°± ì ìš©
        for candidate in improvement_candidates:
            candidate["batch_feedback"] = batch_feedback
    
    def _improve_problem_with_feedback(self, candidate: Dict) -> None:
        """ë°°ì¹˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°œë³„ ë¬¸ì œ ê°œì„ """
        problem = candidate["problem_data"]
        result = candidate["result"]
        batch_feedback = candidate.get("batch_feedback", "")
        
        print(f"\nâœ¨ ë¬¸ì œ ê°œì„  ì¤‘: {problem.get('problem', '')[:50]}...")
        
        # LLMì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ í’€ì´ ê°œì„ 
        llm = self._llm(0.3)
        
        improvement_prompt = f"""
        ë‹¤ìŒ ë¬¸ì œì˜ í’€ì´ë¥¼ ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ê°œì„ í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì œ: {problem.get('problem', '')}
        ë³´ê¸°: {problem.get('options', [])}
        í˜„ì¬ í’€ì´: {result.get('generated_explanation', '')}
        
        ì‚¬ìš©ì í”¼ë“œë°±: {batch_feedback}
        
        ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ í’€ì´ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”.
        
        ì¶œë ¥ í˜•ì‹:
        ê°œì„ ëœ í’€ì´: [ê°œì„ ëœ í’€ì´ ë‚´ìš©]
        ê°œì„  ì‚¬í•­: [ì–´ë–¤ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê°œì„ í–ˆëŠ”ì§€ ì„¤ëª…]
        """
        
        try:
            response = llm.invoke(improvement_prompt)
            improved_explanation = response.content.strip()
            
            # ê°œì„ ëœ í’€ì´ íŒŒì‹±
            explanation_match = re.search(r"ê°œì„ ëœ í’€ì´:\s*(.+)", improved_explanation, re.DOTALL)
            if explanation_match:
                candidate["improved_explanation"] = explanation_match.group(1).strip()
                print("âœ… ë¬¸ì œ ê°œì„  ì™„ë£Œ")
            else:
                candidate["improved_explanation"] = result.get("generated_explanation", "")
                print("âš ï¸ í’€ì´ ê°œì„  ì‹¤íŒ¨, ì›ë³¸ ìœ ì§€")
                
        except Exception as e:
            print(f"âš ï¸ í’€ì´ ê°œì„  ì¤‘ ì˜¤ë¥˜: {e}")
            candidate["improved_explanation"] = result.get("generated_explanation", "")

    def _create_custom_checkpointer(self):
        """vectorstore ì§ë ¬í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸í„°"""
        from langgraph.checkpoint.memory import InMemorySaver
        
        # ê¸°ë³¸ InMemorySaver ì‚¬ìš© (ì•ˆì •ì ì´ê³  í˜¸í™˜ì„± ì¢‹ìŒ)
        return InMemorySaver()


if __name__ == "__main__":
    print("ğŸš€ HITL í”¼ë“œë°± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # HITL ëª¨ë“œ ì„ íƒ
    print("\nğŸ¯ HITL ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. auto - ìë™ ëª¨ë“œ (HITL ì—†ìŒ)")
    print("2. smart - ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ (í’ˆì§ˆì— ë”°ë¼ ìë™ ê²°ì •)")
    print("3. manual - ìˆ˜ë™ ëª¨ë“œ (í•­ìƒ HITL)")
    
    mode_choice = input("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1-3, ê¸°ë³¸ê°’: 3): ").strip()
    
    if mode_choice == "1":
        hitl_mode = "auto"
    elif mode_choice == "2":
        hitl_mode = "smart"
    else:
        hitl_mode = "manual"
    
    print(f"\nâœ… ì„ íƒëœ ëª¨ë“œ: {hitl_mode}")
    
    # ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì‹œë„ (ì„ íƒì‚¬í•­)
    vectorstore = None
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={"device": "cpu"}
        )

        if "default" in connections.list_connections():
            connections.disconnect("default")
        connections.connect(alias="default", host="localhost", port="19530")

        vectorstore = Milvus(
            embedding_function=embedding_model,
            collection_name="problems",
            connection_args={"host": "localhost", "port":"19530"}
        )
        print("âœ… Milvus ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
        print("   - ë²¡í„°ìŠ¤í† ì–´ ì—†ì´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        vectorstore = None
    
    agent = SolutionAgent(max_interactions=5, hitl_mode=hitl_mode)

    # í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ê°’ ì œê³µ
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”:")
    
    user_input_txt = input("â“ ì‚¬ìš©ì ì§ˆë¬¸ (ê¸°ë³¸ê°’: í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤): ").strip()
    if not user_input_txt:
        user_input_txt = "í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤"
    
    user_problem = input("â“ ì‚¬ìš©ì ë¬¸ì œ (ê¸°ë³¸ê°’: í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œì˜ ì°¨ì´ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?): ").strip()
    if not user_problem:
        user_problem = "í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œì˜ ì°¨ì´ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?"
    
    user_problem_options_raw = input("â“ ì‚¬ìš©ì ë³´ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸ê°’: í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§€ë©° ìŠ¤ë ˆë“œëŠ” í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•œë‹¤,í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œëŠ” ëª¨ë‘ ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§„ë‹¤,í”„ë¡œì„¸ìŠ¤ëŠ” ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ê³  ìŠ¤ë ˆë“œëŠ” ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§„ë‹¤,í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œëŠ” ëª¨ë‘ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•œë‹¤): ").strip()
    if not user_problem_options_raw:
        user_problem_options_raw = "í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§€ë©° ìŠ¤ë ˆë“œëŠ” í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•œë‹¤,í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œëŠ” ëª¨ë‘ ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§„ë‹¤,í”„ë¡œì„¸ìŠ¤ëŠ” ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ê³  ìŠ¤ë ˆë“œëŠ” ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê°€ì§„ë‹¤,í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œëŠ” ëª¨ë‘ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•œë‹¤"
    
    user_problem_options = [opt.strip() for opt in user_problem_options_raw.split(",") if opt.strip()]

    print(f"\nğŸš€ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
    print(f"ëª¨ë“œ: {hitl_mode}")
    print(f"ë¬¸ì œ: {user_problem}")
    print(f"ë³´ê¸° ìˆ˜: {len(user_problem_options)}ê°œ")
    
    try:
        final_state = agent.invoke(
            user_input_txt=user_input_txt,
            user_problem=user_problem,
            user_problem_options=user_problem_options,
            vectorstore=vectorstore,
        )

        print(f"\n" + "=" * 50)
        print(f"ğŸ¯ ìµœì¢… ê²°ê³¼:")
        print(f"ë¬¸ì œ: {final_state.get('user_problem', '')}")
        print(f"ì •ë‹µ: {final_state.get('generated_answer', '')}")
        print(f"í’€ì´: {final_state.get('generated_explanation', '')}")
        print(f"ê³¼ëª©: {final_state.get('generated_subject', '')}")
        print(f"ìƒí˜¸ì‘ìš© íšŸìˆ˜: {final_state.get('interaction_count', 0)}")
        print(f"ì‚¬ìš©ì í”¼ë“œë°±: {final_state.get('user_feedback', '')}")
        print(f"í”¼ë“œë°± ìœ í˜•: {final_state.get('feedback_type', '')}")
        
        # í’ˆì§ˆ ì ìˆ˜ ì¶œë ¥
        quality_scores = final_state.get('quality_scores', {})
        if quality_scores:
            print(f"\nğŸ“Š í’ˆì§ˆ ì ìˆ˜:")
            for key, score in quality_scores.items():
                print(f"  {key}: {score:.1f}/100")
            print(f"  ì´ì : {final_state.get('total_quality_score', 0):.1f}/100")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶œë ¥
        chat_history = final_state.get('chat_history', [])
        if chat_history:
            print(f"\nğŸ’¬ ìƒí˜¸ì‘ìš© íˆìŠ¤í† ë¦¬:")
            for i, chat in enumerate(chat_history, 1):
                print(f"  {i}. {chat[:100]}...")
                
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"\nğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ 'python test_hitl_feedback.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")