# -*- coding: utf-8 -*-
"""
PDF ì „ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ ëª¨ë“ˆ (ê°œì„ íŒ)
teacher_graph.pyì—ì„œ PDF ê´€ë ¨ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì„
- ë²ˆí˜¸ ìƒí•œ ì œê±°(ë˜ëŠ” ìƒí–¥)ë¡œ 100ë¬¸í•­ ì´ìƒ ëŒ€ì‘
- ë¬¸ì œ ê²½ê³„(## 31. ë“±) ë³´ê°•
- ê³¼ë„í•œ ë¸”ë¡ ê¸¸ì´ ì»·ìœ¼ë¡œ LLM JSON ì˜ë¦¼ ì™„í™”
- í´ë°± íŠ¸ë¦¬ê±° ê°•í™”(ê¸°ëŒ€ì¹˜ ëŒ€ë¹„ ë¶€ì¡± ì‹œ 2ë‹¨ ì „ì²´â†’ì¼ê´„ LLM, í•„ìš” ì‹œ í˜ì´ì§€ ë°°ì¹˜ í´ë°±)
- Docling .text ë¶„ê¸° ì œê±°
"""

import os
import re
import json
from typing import List, Dict, Optional
from dotenv import load_dotenv

load_dotenv()

from docling.document_converter import DocumentConverter
from langchain_openai import ChatOpenAI

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))

# ------------ ìœ í‹¸: ê¸°ëŒ€ ë¬¸ì œ ìˆ˜ ì¶”ì • ------------
_HEADER_NUM_PAT = re.compile(r'^\s*(?:##\s*)?(?:ë¬¸ì œ\s*)?(\d+)\s*\.', re.UNICODE)


def _estimate_expected_count_from_text(text: str, upper: int = 100) -> int:
    nums = []
    for ln in text.split("\n"):
        m = _HEADER_NUM_PAT.match(ln.strip())
        if m:
            try:
                n = int(m.group(1))
                if 1 <= n <= upper:
                    nums.append(n)
            except Exception:
                pass
    return max(nums) if nums else 0


class PDFPreprocessor:
    """PDF íŒŒì¼ ì „ì²˜ë¦¬ ë° ë¬¸ì œ ì¶”ì¶œ í´ë˜ìŠ¤"""

    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ê¶Œí•œ ë¬¸ì œ í•´ê²°
        os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
        os.environ["HF_HOME"] = os.getenv("HF_HOME", "C:\\temp\\huggingface_cache")

        # cv2 setNumThreads ë¬¸ì œ í•´ê²°
        try:
            import cv2  # noqa: F401

            if not hasattr(cv2, "setNumThreads"):
                # setNumThreadsê°€ ì—†ìœ¼ë©´ ë”ë¯¸ í•¨ìˆ˜ ì¶”ê°€
                cv2.setNumThreads = lambda x: None  # type: ignore[attr-defined]
        except ImportError:
            pass

    # -------------------- í¸ì˜ ì¶”ì¶œ --------------------

    def extract_pdf_paths(self, text: str) -> List[str]:
        """PDF íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ"""
        pdf_patterns = [
            r"([^\s]+\.pdf)",  # ê¸°ë³¸ .pdf íŒŒì¼ ê²½ë¡œ
            r"([C-Z]:[\\\/][^\\\/\s]*\.pdf)",  # Windows ì ˆëŒ€ ê²½ë¡œ
            r"([\.\/][^\\\/\s]*\.pdf)",  # ìƒëŒ€ ê²½ë¡œ
        ]
        pdf_paths = []
        for pattern in pdf_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            pdf_paths.extend(matches)
        return list(set(pdf_paths))  # ì¤‘ë³µ ì œê±°

    def extract_problem_range(self, text: str) -> Optional[Dict]:
        """ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„ ì¶”ì¶œ"""
        patterns = [
            r"(\d+)ë²ˆë§Œ",
            r"(\d+)ë²ˆ\s*í’€",
            r"(\d+)\s*[-~]\s*(\d+)ë²ˆ",
            r"(\d+)ë²ˆë¶€í„°\s*(\d+)ë²ˆ",
            r"(\d+(?:\s*,\s*\d+)*)ë²ˆ",
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                groups = match.groups()
                if len(groups) == 1:
                    if "," in groups[0]:
                        numbers = [int(x.strip()) for x in groups[0].split(",")]
                        return {"type": "specific", "numbers": numbers}
                    else:
                        return {"type": "single", "number": int(groups[0])}
                elif len(groups) == 2:
                    start, end = int(groups[0]), int(groups[1])
                    return {"type": "range", "start": start, "end": end}
        return None

    def determine_problem_source(self, text: str) -> Optional[str]:
        """ë¬¸ì œ ì†ŒìŠ¤ ê²°ì •"""
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in ["pdf", "íŒŒì¼", "ë¬¸ì„œ"]):
            return "pdf_extracted"
        elif any(keyword in text_lower for keyword in ["ê¸°ì¡´", "shared", "ì €ì¥ëœ", "ì´ì „"]):
            return "shared"
        if self.extract_pdf_paths(text):
            return "pdf_extracted"
        return None

    # -------------------- í•µì‹¬ íŒŒì´í”„ë¼ì¸ --------------------

    def extract_problems_from_pdf(self, file_paths: List[str]) -> List[Dict]:
        """PDF íŒŒì¼ì—ì„œ ë¬¸ì œ ì¶”ì¶œ (Docling ì‚¬ìš©) + í´ë°± ê°•í™”"""
        try:
            print("ğŸ”§ DocumentConverter ì´ˆê¸°í™” ì¤‘...")
            converter = DocumentConverter()
            try:
                converter.config.image_processing = False
                print("âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™” ì„¤ì •")
            except Exception:
                print("âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë³€ê²½ ë¶ˆê°€")
            try:
                converter.config.text_extraction_priority = "text"
                print("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ì„¤ì •")
            except Exception:
                print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ì„¤ì • ë¶ˆê°€")
            print("âœ… DocumentConverter ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ DocumentConverter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            return []

        # LLM ì„¤ì •
        llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"),
            model=OPENAI_LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
        )

        all_problems: List[Dict] = []

        for path in file_paths:
            try:
                print(f"ğŸ“– íŒŒì¼ ì²˜ë¦¬ ì¤‘: {path}")
                doc_result = converter.convert(path)

                # ë°©ë²• 1: ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ
                raw_text = doc_result.document.export_to_markdown()
                print(f"ğŸ“ [ë°©ë²•1] ë§ˆí¬ë‹¤ìš´ ê¸¸ì´: {len(raw_text)}")

                # ë°©ë²• 2/3 ì œê±°: DoclingDocumentì— .text ì—†ìŒ / pages ìš”ì†Œê°€ intë˜ëŠ” ê²½ìš° ìˆìŒ
                # -> ì•ˆì •ì„± ìœ„í•´ ì œê±°(ë¡œê·¸ ì†ŒìŒ ë°©ì§€)

                # ë°©ë²• 4: ë§ˆí¬ë‹¤ìš´ HTML ì£¼ì„/íƒœê·¸ ì œê±°
                if raw_text.startswith("<!--"):
                    try:
                        cleaned = re.sub(r"<!--.*?-->", "", raw_text, flags=re.DOTALL)
                        cleaned = re.sub(r"<[^>]+>", "", cleaned)
                        cleaned = re.sub(r"^\s*-\s*", "", cleaned, flags=re.MULTILINE)
                        cleaned = "\n".join(
                            line for line in cleaned.split("\n") if line.strip()
                        )
                        if cleaned and len(cleaned) > 50:
                            raw_text = cleaned
                            print("âœ… HTML íƒœê·¸/ì£¼ì„ ì œê±° ì±„íƒ")
                    except Exception as e:
                        print(f"âš ï¸ HTML ì •ë¦¬ ì‹¤íŒ¨: {e}")

                if not raw_text.strip() or raw_text.startswith("<!--"):
                    print("âŒ Doclingìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    continue

                raw_text = self.normalize_docling_markdown(raw_text)
                print(f"ğŸ“Š ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)}")

                # ê¸°ëŒ€ ë¬¸ì œ ìˆ˜(í—¤ë” ê¸°ë°˜) ì¶”ì •
                expected = _estimate_expected_count_from_text(raw_text)
                if expected:
                    print(f"ğŸ“ˆ í—¤ë” ê¸°ë°˜ ê¸°ëŒ€ ë¬¸ì œ ìˆ˜(ì¶”ì •): ì•½ {expected}ë¬¸í•­")

                # 1) ê¸°ë³¸ ë¶„í•  â†’ LLM ë¸”ë¡ íŒŒì‹±
                blocks = self._process_pdf_text(raw_text, path)
                print(f"ğŸ“ 1ì°¨ ë¶„í•  ë¸”ë¡ ìˆ˜: {len(blocks)}")

                successful_parses = 0
                local_problems: List[Dict] = []

                for i, block in enumerate(blocks):
                    blk = block.strip()
                    if len(blk) < 20:
                        continue
                    try:
                        # ê³¼ë„í•œ ë¸”ë¡ ê¸¸ì´ ì»·(LLM ì•ˆì „ ê°€ë“œ)
                        if len(blk) > 4000:
                            blk = self._smart_truncate_block(blk)
                        problem = self._parse_block_with_llm(blk, llm)
                        if problem:
                            local_problems.append(problem)
                            successful_parses += 1
                    except Exception as e:
                        print(f"âš ï¸ ë¸”ë¡ {i+1} íŒŒì‹± ì‹¤íŒ¨: {e}")

                # --- í´ë°± ì¡°ê±´ íŒë‹¨ ê°•í™” ---
                need_fallback = False
                if successful_parses == 0:
                    need_fallback = True
                elif len(blocks) > 0 and successful_parses / max(1, len(blocks)) < 0.2:
                    need_fallback = True
                # ê¸°ëŒ€ì¹˜ 70% ë¯¸ë§Œì´ë©´ í´ë°±
                if expected and len(local_problems) < int(0.7 * expected):
                    need_fallback = True

                if need_fallback:
                    print("ğŸ” í´ë°± ë°œë™: 2ë‹¨ ì¬ì •ë ¬ í›„ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ LLMìœ¼ë¡œ ì¼ê´„ ì¶”ì¶œ (ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ ë°°ì¹˜)")

                    reordered_text = self._reorder_two_columns_with_pdfminer(path)
                    if not reordered_text.strip():
                        print("âš ï¸ 2ë‹¨ ì¬ì •ë ¬ í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ì›ë³¸ìœ¼ë¡œ ì¼ê´„ ì¶”ì¶œ ì‹œë„")
                        reordered_text = raw_text
                    reordered_text = self.normalize_docling_markdown(reordered_text)

                    # (1) ì „ì²´ ì¼ê´„
                    batch = self._parse_whole_text_with_llm(reordered_text, llm)

                    # (2) ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ ë°°ì¹˜
                    if not batch:
                        print("ğŸ” í´ë°± 2ë‹¨ê³„: í˜ì´ì§€ ë°°ì¹˜ LLM ì¶”ì¶œ ì‹œë„")
                        try:
                            batch = self._parse_by_pages_with_llm(doc_result, llm)
                        except Exception as e:
                            print(f"âš ï¸ í˜ì´ì§€ ë°°ì¹˜ í´ë°± ì‹¤íŒ¨: {e}")

                    if batch:
                        print(f"âœ… í´ë°± ì¶”ì¶œ ì„±ê³µ: {len(batch)}ê°œ")
                        local_problems = batch
                    else:
                        print("âŒ í´ë°± ì¶”ì¶œ ì‹¤íŒ¨ â†’ ê¸°ì¡´ ë¶€ë¶„ ì„±ê³¼ ìœ ì§€")
                else:
                    print("âœ… 1ì°¨ ë¸”ë¡ íŒŒì‹± ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì—¬ í´ë°± ìƒëµ")

                all_problems.extend(local_problems)
                print(f"ğŸ“Š ëˆ„ì  ë¬¸ì œ ìˆ˜: {len(all_problems)}")

            except Exception as e:
                print(f"âŒ íŒŒì¼ {path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"ğŸ¯ ì´ {len(all_problems)}ê°œ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
        return all_problems

    # -------------------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬/ë¶„í•  --------------------

    def _process_pdf_text(self, raw_text: str, pdf_path: str) -> List[str]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ 1ë‹¨/2ë‹¨ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬"""
        print("ğŸ” [ë ˆì´ì•„ì›ƒ ë¶„ì„] 1ë‹¨/2ë‹¨ êµ¬ì¡° íŒŒì•… ì¤‘...")

        blocks = self._split_problem_blocks(raw_text)

        # 1ë‹¨ íŒŒì‹± ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ 2ë‹¨ êµ¬ì¡°ë¡œ ì¬ì‹œë„
        if len(blocks) <= 2:
            print("âš ï¸ 1ë‹¨ íŒŒì‹± ê²°ê³¼ ë¶€ì¡± - 2ë‹¨ êµ¬ì¡°ë¡œ ì¬ì‹œë„")
            try:
                reordered_text = self._reorder_two_columns_with_pdfminer(pdf_path)
                reordered_text = self.normalize_docling_markdown(reordered_text)
                blocks = self._split_problem_blocks(reordered_text)
                print(f"ğŸ”„ 2ë‹¨ ì¬ì •ë ¬ í›„: {len(blocks)}ê°œ ë¸”ë¡")

                if len(blocks) <= 2:
                    print("âš ï¸ 2ë‹¨ íŒŒì‹±ë„ ë¶€ì¡± - ìˆ«ì í—¤ë” í´ë°± ì‚¬ìš©")
                    blocks = self._split_problem_blocks_without_keyword(reordered_text)
                    print(f"ğŸ”„ í´ë°± í›„: {len(blocks)}ê°œ ë¸”ë¡")
            except Exception as e:
                print(f"âš ï¸ 2ë‹¨ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                blocks = self._split_problem_blocks_without_keyword(raw_text)

        return blocks

    def _reorder_two_columns_with_pdfminer(self, pdf_path: str) -> str:
        """PDFMinerë¥¼ ì‚¬ìš©í•˜ì—¬ 2ë‹¨ PDFë¥¼ 1ë‹¨ìœ¼ë¡œ ì¬ì •ë ¬"""
        try:
            from pdfminer.high_level import extract_pages
            from pdfminer.layout import LTTextContainer

            print("ğŸ”„ [2ë‹¨ ì¬ì •ë ¬] PDFMinerë¡œ ì¢Œìš° ì»¬ëŸ¼ ì¬ì •ë ¬ ì¤‘...")

            pages_text = []
            for page_layout in extract_pages(pdf_path):
                left, right = [], []
                xs = []
                for el in page_layout:
                    if isinstance(el, LTTextContainer):
                        xs.append(el.bbox[0])

                if not xs:
                    continue

                sorted_xs = sorted(xs)
                mid = sorted_xs[len(sorted_xs) // 2]

                for el in page_layout:
                    if isinstance(el, LTTextContainer):
                        (x0, y0, x1, y1) = el.bbox
                        text = el.get_text().strip()
                        if text:
                            (left if x0 < mid else right).append((y1, text))

                left.sort(key=lambda t: -t[0])
                right.sort(key=lambda t: -t[0])

                page_text = "".join(t for _, t in left) + "\n" + "".join(t for _, t in right)
                pages_text.append(page_text)

            result = "\n\n".join(pages_text)
            print(f"âœ… [2ë‹¨ ì¬ì •ë ¬ ì™„ë£Œ] ì´ {len(pages_text)}í˜ì´ì§€ ì²˜ë¦¬")
            return result

        except ImportError:
            print("âš ï¸ PDFMinerê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - 2ë‹¨ ì¬ì •ë ¬ ë¶ˆê°€")
            return ""
        except Exception as e:
            print(f"âš ï¸ 2ë‹¨ ì¬ì •ë ¬ ì‹¤íŒ¨: {e}")
            return ""

    def _split_problem_blocks_without_keyword(self, text: str) -> List[str]:
        """ë¬¸ì œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ì‹œí—˜ì§€ì—ì„œ ë²ˆí˜¸(1., 2., â€¦)ë§Œìœ¼ë¡œ ë¬¸í•­ ë‹¨ìœ„ë¥¼ ë¶„í• """
        print("ğŸ”„ [í´ë°± íŒŒì‹±] ë¬¸ì œ í‚¤ì›Œë“œ ì—†ì´ ë²ˆí˜¸ë§Œìœ¼ë¡œ ë¶„í•  ì‹œë„")

        text = self.normalize_docling_markdown(text)
        lines = text.split("\n")
        n = len(lines)

        _QHEAD_CAND = re.compile(r"(?m)^\s*(\d{1,3})[.)]\s+\S")
        candidates = []

        for i, ln in enumerate(lines):
            m = _QHEAD_CAND.match(ln or "")
            if m:
                num = int(m.group(1))
                if not re.match(r"^\s*\d+\)\s*", ln):
                    if len(ln.strip()) > 10:
                        candidates.append((i, num))
                        print(f"ğŸ” [í´ë°±] ë¼ì¸ {i}: '{ln[:50]}...' â†’ í›„ë³´ ë²ˆí˜¸ {num}")
                else:
                    print(f"ğŸ” [í´ë°±] ë¼ì¸ {i}: '{ln[:50]}...' â†’ ë³´ê¸° ë²ˆí˜¸ë¡œ íŒë‹¨í•˜ì—¬ ì œì™¸")

        print(f"ğŸ” [í´ë°±] ì´ í›„ë³´ ìˆ˜: {len(candidates)}")

        headers = []
        prev_num = 0
        last_header_idx = -9999

        for i, num in candidates:
            if num == prev_num + 1:
                headers.append(i)
                prev_num = num
                last_header_idx = i
                print(f"âœ… [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ìˆœì°¨ ì¦ê°€ë¡œ í—¤ë” ì„ íƒ")
                continue

            if num == 1:
                window = "\n".join(lines[max(0, i - 3) : i + 1])
                if (i - last_header_idx) >= 8 or re.search(r"(â… |â…¡|III|ê³¼ëª©|íŒŒíŠ¸|SECTION)", window):
                    headers.append(i)
                    prev_num = 1
                    last_header_idx = i
                    print(f"âœ… [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ì„¹ì…˜ ë¦¬ì…‹ìœ¼ë¡œ í—¤ë” ì„ íƒ")
                else:
                    print(f"âŒ [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ì„¹ì…˜ ë¦¬ì…‹ ì¡°ê±´ ë¶ˆì¶©ì¡± (ê±°ë¦¬: {i - last_header_idx})")

        if not headers:
            print("âŒ [í´ë°±] í—¤ë”ê°€ í•˜ë‚˜ë„ ì„ íƒë˜ì§€ ì•ŠìŒ - ë‹¨ìˆœ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„")
            simple_pattern = re.compile(r"(?m)^\s*(\d{1,3})\.\s+")
            for i, ln in enumerate(lines):
                if simple_pattern.match(ln or "") and not re.match(r"^\s*\d+\)\s*", ln):
                    headers.append(i)
                    print(f"ğŸ“Œ [í´ë°±] ë¼ì¸ {i}: '{ln[:30]}...' â†’ í—¤ë” ì¶”ê°€")

            if not headers:
                print("âŒ [í´ë°± ì‹¤íŒ¨] ì „ì²´ë¥¼ 1ê°œ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬")
                return [text] if text.strip() else []

        headers.append(n)
        blocks = []
        for a, b in zip(headers[:-1], headers[1:]):
            blk = "\n".join(lines[a:b]).strip()
            if blk:
                blocks.append(blk)
                print(f"ğŸ“¦ [í´ë°±] ë¸”ë¡ {len(blocks)}: ë¼ì¸ {a}-{b-1} ({len(blk)}ì)")

        print(f"ğŸ¯ [í´ë°±] ìµœì¢… ë¸”ë¡ ìˆ˜: {len(blocks)}")
        return blocks

    def _split_problem_blocks(self, raw_text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì œ ë¸”ë¡ìœ¼ë¡œ ë¶„í•  (ì •êµí•œ ê°œì„ ëœ ë¡œì§)"""
        print("ğŸ” [êµ¬ì¡° ë¶„ì„] ì •êµí•œ ë¬¸ì œ í—¤ë” ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹± ë°©ì‹ ê²°ì •")

        lines = raw_text.split("\n")

        # ì •êµí•œ ë¬¸ì œ í—¤ë” íŒ¨í„´ë“¤ (ìš°ì„ ìˆœìœ„ ìƒë‹¨ì— ë²”ìš©í˜• ì¶”ê°€)
        problem_header_patterns = [
            r"^\s*(?:#+\s*)?ë¬¸ì œ?\s*(\d+)\s*\.\s*",  # "## 31.", "ë¬¸ì œ 31.", "31."
            r"^\s*#+\s*ë¬¸ì œ\s*(\d+)\s*\.\s*",
            r"^\s*ë¬¸ì œ\s*(\d+)\s*\.\s*",
            r"^\s*(\d+)\s*\.\s*[^ê°€-í£]*[ê°€-í£]",
            r"^\s*(\d+)\s*\.\s*\S",
            r"^\s*Q\s*(\d+)\s*\.\s*",
            r"^\s*\[(\d+)\]\s*",
            r"^\s*#+\s*.*?(\d+)\s*\.\s*[ê°€-í£]",
            r"^\s*#+\s*[^ê°€-í£]*(\d+)\.\s*[ê°€-í£]",
            r"^\s*-\s*[^ê°€-í£]*(\d+)\.\s*[ê°€-í£]",
            r"^\s*[^ê°€-í£]*(\d+)\.\s*[ê°€-í£]",
        ]

        option_patterns = [
            r"^\s*(\d+)\.\s*\1\.\s*",
            r"^\s*(\d+)\s*[)]\s*",
            r"^\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*",
            r"^\s*[ê°€-í•˜]\s*[)]\s*",
            r"^\s*[A-E]\s*[)]\s*",
            r"^\s*-\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*",
            r"^\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*[ê°€-í£]",
        ]

        problem_headers = []
        seen_numbers = set()

        for i, line in enumerate(lines):
            s = line.strip()
            if not s:
                continue

            # ë³´ê¸° ë²ˆí˜¸ ë¨¼ì € ê±¸ëŸ¬ë‚´ê¸°
            if any(re.match(p, s) for p in option_patterns):
                continue

            for pattern in problem_header_patterns:
                match = re.match(pattern, s)
                if match:
                    try:
                        problem_num = int(match.group(1))
                    except Exception:
                        continue

                    # ë²ˆí˜¸ ìƒí•œ ì™„í™” (ë˜ëŠ” ì œê±°). ì•ˆì „í•˜ê²Œ 300ìœ¼ë¡œ ë‘ .
                    if 1 <= problem_num <= 100 and problem_num not in seen_numbers:
                        problem_headers.append((i, problem_num, s))
                        seen_numbers.add(problem_num)
                        print(
                            f"âœ… [ë¬¸ì œ í—¤ë” ë°œê²¬] ë¼ì¸ {i+1}: '{s[:80]}...' (ë¬¸ì œ {problem_num}ë²ˆ)"
                        )
                    break

        if not problem_headers:
            print("âš ï¸ ë¬¸ì œ í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì „ì²´ë¥¼ 1ê°œ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬")
            return [raw_text] if raw_text.strip() else []

        problem_headers.sort(key=lambda x: x[1])

        # ëˆ„ë½ ë²ˆí˜¸ (ìƒí•œì€ í˜„ì¬ ë¬¸ì„œì—ì„œ ìµœëŒ€ ë²ˆí˜¸ ê¸°ì¤€)
        found_numbers = {h[1] for h in problem_headers}
        max_num = max(found_numbers) if found_numbers else 0
        expected_range = set(range(1, max_num + 1))
        missing_numbers = expected_range - found_numbers
        if missing_numbers:
            print(f"âš ï¸ ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸: {sorted(missing_numbers)}")

        # ë¬¸ì œ ë¸”ë¡ ìƒì„±
        blocks = []
        for idx, (header_idx, problem_num, header_text) in enumerate(problem_headers):
            start_line = header_idx
            if idx + 1 < len(problem_headers):
                next_header_idx = problem_headers[idx + 1][0]
                end_line = next_header_idx
            else:
                end_line = len(lines)

            problem_text = "\n".join(lines[start_line:end_line]).strip()

            # ë³µí•© ë¬¸ì œ ë¶„ë¦¬ ë¡œì§ ìœ ì§€
            sub_blocks = self._split_composite_problem(problem_text, problem_num)
            if len(sub_blocks) > 1:
                print(f"ğŸ”§ ë¬¸ì œ {problem_num}ë²ˆ ë³µí•© ë¬¸ì œ ë¶„ë¦¬: {len(sub_blocks)}ê°œ ë¸”ë¡")
                blocks.extend(sub_blocks)
            else:
                blocks.append(problem_text)

            print(
                f"ğŸ“¦ ë¬¸ì œ {problem_num}ë²ˆ: ë¼ì¸ {start_line+1}-{end_line} ({len(problem_text)}ì)"
            )
            print(f"   í—¤ë”: '{header_text[:50]}...'")

        print(f"âœ… ì´ {len(blocks)}ê°œ ë¬¸ì œ ë¸”ë¡ ìƒì„± ì™„ë£Œ")

        # ëˆ„ë½ ë¬¸ì œ ì¶”ê°€ íƒìƒ‰
        if missing_numbers and len(blocks) < max_num:
            print(f"ğŸ”„ ëˆ„ë½ëœ ë¬¸ì œ {len(missing_numbers)}ê°œ ì¶”ê°€ ì‹œë„ ì¤‘...")
            add = self._find_missing_problems(lines, missing_numbers)
            if add:
                blocks.extend(add)
                print(f"âœ… ì¶”ê°€ ë¬¸ì œ {len(add)}ê°œ ë°œê²¬ - ì´ {len(blocks)}ê°œ")

        return blocks

    # -------------------- LLM íŒŒì„œ --------------------

    def _parse_block_with_llm(self, block_text: str, llm) -> Optional[Dict]:
        """LLMìœ¼ë¡œ ë¸”ë¡ì„ ë¬¸ì œ í˜•íƒœë¡œ íŒŒì‹±"""
        cleaned_text = self._clean_problem_block(block_text)

        sys_prompt = (
            "ë„ˆëŠ” ì‹œí—˜ ë¬¸ì œ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
            "ë¬¸ì œ ì§ˆë¬¸ê³¼ ë³´ê¸°ë¥¼ êµ¬ë¶„í•´ì„œ questionê³¼ options ë°°ì—´ë¡œ ì¶œë ¥í•œë‹¤. "
            "optionsëŠ” ë³´ê¸° í•­ëª©ë§Œ í¬í•¨í•˜ê³ , ì„¤ëª…/í•´ì„¤/ì •ë‹µ ë“±ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. "
            "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ ë¬¸ì¥ì´ë‚˜ ì½”ë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ."
        )

        user_prompt = (
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸í•­ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ, ì •í™•íˆ ì¶”ì¶œí•´ JSONìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.\n"
            'ìš”êµ¬ ìŠ¤í‚¤ë§ˆ: {"question":"...","options":["...","..."]}\n'
            "ê·œì¹™:\n"
            "- ë¬¸ì œ ì§ˆë¬¸ì—ì„œ ë²ˆí˜¸(ì˜ˆ: 'ë¬¸ì œ 1.' ë“±)ì™€ ë¶ˆí•„ìš”í•œ ë¨¸ë¦¬ê¸€ì€ ì œê±°.\n"
            "- ì˜µì…˜ì€ ì‹¤ì œ ë³´ê¸° ê°œìˆ˜ì— ë§ì¶¤.\n"
            "- ë³´ê¸° ë²ˆí˜¸(â‘ , â‘¡, â‘¢, â‘£ ë“±)ëŠ” ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ì¶”ì¶œ.\n"
            "- ë¬¸ì œê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ nullì„ ë°˜í™˜.\n"
            "- ì‘ë‹µì€ ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ ê²ƒ.\n"
            f"í…ìŠ¤íŠ¸:\n{cleaned_text[:1500]}"
        )

        try:
            response = llm.invoke(
                [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ]
            )
            content = response.content.strip()
            print(f"ğŸ” LLM ì‘ë‹µ ì›ë³¸: {content[:200]}...")

            json_content = self._extract_json_from_response(content)
            if not json_content:
                print("âŒ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                return None

            try:
                data = json.loads(json_content)
            except json.JSONDecodeError:
                fixed = self._fix_json_format(json_content)
                try:
                    data = json.loads(fixed)
                    print("âœ… JSON ìˆ˜ì • í›„ íŒŒì‹± ì„±ê³µ")
                except json.JSONDecodeError as e2:
                    print(f"âŒ JSON ìˆ˜ì • í›„ì—ë„ íŒŒì‹± ì‹¤íŒ¨: {e2}")
                    return None

            if isinstance(data, dict) and "question" in data and "options" in data:
                if data["question"].strip() and isinstance(data["options"], list) and len(data["options"]) > 0:
                    print("âœ… ë¬¸ì œ íŒŒì‹± ì„±ê³µ")
                    return data
                else:
                    print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° êµ¬ì¡°")
            else:
                print("âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½")

        except Exception as e:
            print(f"âš ï¸ LLM íŒŒì‹± ì‹¤íŒ¨: {e}")

        return None

    def _parse_whole_text_with_llm(self, full_text: str, llm) -> Optional[List[Dict]]:
        """2ë‹¨ ì¬ì •ë ¬(ë˜ëŠ” ì›ë¬¸) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— LLMì— ë„£ì–´ ì¼ê´„ ì¶”ì¶œ"""
        cleaned = self.normalize_docling_markdown(full_text)

        sys_prompt = (
            "ë„ˆëŠ” ì‹œí—˜ì§€ì—ì„œ ë¬¸í•­ì„ êµ¬ì¡°í™”í•´ ì¶”ì¶œí•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
            "ë¬¸í•­ì˜ ì§ˆë¬¸ê³¼ ë³´ê¸°ë§Œì„ ë½‘ì•„ë‚´ê³ , í•´ì„¤/ì •ë‹µ/ì¶œì²˜ ë“±ì€ ì œì™¸í•œë‹¤. "
            "ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•œë‹¤."
        )
        user_prompt = (
            "ë‹¤ìŒ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸í•­ì„ ìµœëŒ€í•œ ëˆ„ë½ ì—†ì´ ì¶”ì¶œí•´ì¤˜.\n"
            'ìš”êµ¬ ìŠ¤í‚¤ë§ˆ(ë°°ì—´): [{"question":"...","options":["...","..."]}, ...]\n'
            "ê·œì¹™:\n"
            "- ì§ˆë¬¸ì— ë¶™ì€ ë²ˆí˜¸(ì˜ˆ: '1.', 'ë¬¸ì œ 1.') ë“± ë¨¸ë¦¬ê¸€ì€ ì œê±°.\n"
            "- ë³´ê¸° ë²ˆí˜¸ í‘œê¸°(â‘ , â‘¡, 1), ê°€) ë“±)ëŠ” ì œê±°í•˜ê³  ë‚´ìš©ë§Œ.\n"
            "- ë³´ê¸° ê°œìˆ˜ëŠ” ì›ë¬¸ì— ë§ì¶¤.\n"
            "- í•´ì„¤/ì •ë‹µ/í’€ì´/ì„¤ëª… ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ.\n"
            "- JSON ì™¸ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.\n"
            f"í…ìŠ¤íŠ¸:\n{cleaned[:40000]}"
        )

        try:
            resp = llm.invoke(
                [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ]
            )
            content = resp.content.strip()
            print(f"ğŸ” [ì¼ê´„] LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")

            json_part = self._extract_json_from_response(content)
            if not json_part:
                print("âŒ [ì¼ê´„] JSON ì¶”ì¶œ ì‹¤íŒ¨")
                return None

            try:
                data = json.loads(json_part)
            except json.JSONDecodeError:
                fixed = self._fix_json_format(json_part)
                try:
                    data = json.loads(fixed)
                except json.JSONDecodeError as e2:
                    print(f"âŒ [ì¼ê´„] JSON íŒŒì‹± ì‹¤íŒ¨: {e2}")
                    return None

            if isinstance(data, list):
                cleaned_list: List[Dict] = []
                for idx, item in enumerate(data, 1):
                    if (
                        isinstance(item, dict)
                        and isinstance(item.get("question", ""), str)
                        and item.get("question", "").strip()
                        and isinstance(item.get("options", []), list)
                        and len(item.get("options", [])) > 0
                    ):
                        cleaned_list.append(
                            {
                                "question": item["question"].strip(),
                                "options": [str(opt).strip() for opt in item["options"] if str(opt).strip()],
                            }
                        )
                    else:
                        print(f"âš ï¸ [ì¼ê´„] í•­ëª© {idx} ë¬´ì‹œ(ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜)")
                return cleaned_list if cleaned_list else None

            print("âŒ [ì¼ê´„] ìµœì¢… êµ¬ì¡°ê°€ ë°°ì—´ì´ ì•„ë‹˜")
            return None

        except Exception as e:
            print(f"âš ï¸ [ì¼ê´„] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _parse_by_pages_with_llm(self, doc_result, llm, max_pages: int = 9999) -> Optional[List[Dict]]:
        """í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ì¼ê´„ ì¶”ì¶œ (í´ë°± 2ë‹¨ê³„)"""
        items: List[Dict] = []
        pages = getattr(doc_result.document, "pages", []) or []
        for pidx, page in enumerate(pages[:max_pages], start=1):
            # í˜ì´ì§€ ì˜¤ë¸Œì íŠ¸ì— ë”°ë¼ text/export_to_markdown ì ‘ê·¼ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            page_text = ""
            if hasattr(page, "text") and isinstance(page.text, str):
                page_text = page.text
            elif hasattr(page, "export_to_markdown"):
                try:
                    page_text = page.export_to_markdown()
                except Exception:
                    page_text = ""
            if not page_text:
                continue

            chunk = self.normalize_docling_markdown(str(page_text))
            # ê¸¸ë©´ ë¶„í•  í˜¸ì¶œ
            chunks = [chunk[i : i + 8000] for i in range(0, len(chunk), 8000)]
            for c in chunks:
                batch = self._parse_whole_text_with_llm(c, llm)
                if batch:
                    items.extend(batch)

        return items if items else None

    # -------------------- JSON í›„ì²˜ë¦¬/ì •ë¦¬ --------------------

    def _extract_json_from_response(self, content: str) -> Optional[str]:
        """LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì •í™•íˆ ì¶”ì¶œ"""
        code_block_match = re.search(r"```(?:json)?\s*(.*?)\s*```", content, re.DOTALL)
        if code_block_match:
            return code_block_match.group(1).strip()

        json_match = re.search(r"\{.*?\}", content, re.DOTALL)
        if json_match:
            json_text = json_match.group(0)
            if self._is_valid_json_structure(json_text):
                return json_text

        array_match = re.search(r"\[.*?\]", content, re.DOTALL)
        if array_match:
            array_text = array_match.group(0)
            if self._is_valid_json_structure(array_text):
                return array_text

        if '"question"' in content and '"options"' in content:
            return self._construct_json_from_parts(content)

        return None

    def _is_valid_json_structure(self, text: str) -> bool:
        """JSON êµ¬ì¡°ê°€ ìœ íš¨í•œì§€ ê¸°ë³¸ ê²€ì‚¬"""
        brace_count = text.count("{") - text.count("}")
        bracket_count = text.count("[") - text.count("]")
        if brace_count != 0 or bracket_count != 0:
            return False
        if not (text.startswith("{") and text.endswith("}")) and not (
            text.startswith("[") and text.endswith("]")
        ):
            return False
        return True

    def _construct_json_from_parts(self, content: str) -> Optional[str]:
        """LLM ì‘ë‹µì—ì„œ questionê³¼ options ë¶€ë¶„ì„ ì°¾ì•„ JSON êµ¬ì„±"""
        try:
            question_match = re.search(r'"question"\s*:\s*"([^"]*)"', content)
            if not question_match:
                return None
            question = question_match.group(1)

            options_match = re.search(r'"options"\s*:\s*\[(.*?)\]', content, re.DOTALL)
            if not options_match:
                return None
            options_text = options_match.group(1)

            options = []
            option_matches = re.findall(r'"([^"]*)"', options_text)
            for opt in option_matches:
                if opt.strip():
                    options.append(opt.strip())

            if not options:
                return None

            json_data = {"question": question, "options": options}
            return json.dumps(json_data, ensure_ascii=False)

        except Exception as e:
            print(f"âš ï¸ JSON êµ¬ì„± ì‹¤íŒ¨: {e}")
            return None

    # -------------------- ë¸”ë¡ ì „ì²˜ë¦¬/ë³´ì¡° --------------------

    def _clean_problem_block(self, block_text: str) -> str:
        """ë¬¸ì œ ë¸”ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ì—¬ íŒŒì‹±ì— ì í•©í•˜ê²Œ ë§Œë“¦"""
        lines = block_text.split("\n")
        cleaned_lines = []
        for line in lines:
            s = line.strip()
            if not s:
                continue
            s = re.sub(r"<!--.*?-->", "", s)
            s = re.sub(r"<[^>]+>", "", s)
            s = re.sub(r"^\d+\.\s*", "", s)  # ë²ˆí˜¸ ì œê±°
            s = re.sub(r"^#+\s*", "", s)  # MD í—¤ë” ì œê±°
            if s:
                cleaned_lines.append(s)
        return "\n".join(cleaned_lines)

    def _fix_json_format(self, content: str) -> str:
        """JSON í˜•ì‹ì„ ìˆ˜ì •í•˜ì—¬ íŒŒì‹± ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦ (ê°•í™”)"""
        print(f"ğŸ”§ JSON ìˆ˜ì • ì „: {content[:100]}...")
        content = re.sub(r",\s*}", "}", content)
        content = re.sub(r",\s*]", "]", content)
        content = re.sub(r"\n|\r", " ", content)
        content = re.sub(r"\s+", " ", content)

        # ì¤‘ê´„í˜¸ ë° ëŒ€ê´„í˜¸ ë²”ìœ„ ì •ë¦¬
        if content.count("{") > 0 and content.count("}") > 0:
            start = content.find("{")
            end = content.rfind("}") + 1
            content = content[start:end]
        if content.count("[") > 0 and content.count("]") > 0:
            start = content.find("[")
            end = content.rfind("]") + 1
            content = content[start:end]

        print(f"ğŸ”§ JSON ìˆ˜ì • í›„: {content[:100]}...")
        return content

    def _split_composite_problem(self, block_text: str, problem_num: int) -> List[str]:
        """ë³µí•© ë¬¸ì œë¥¼ ê°œë³„ ë¬¸ì œë¡œ ë¶„ë¦¬"""
        lines = block_text.split("\n")
        sub_blocks = []
        current_block = []
        current_problem_num = problem_num

        for line in lines:
            s = line.strip()
            m = re.match(r"^(\d+)\.\s*", s)
            if m:
                new_num = int(m.group(1))
                if current_block:
                    sub_text = "\n".join(current_block).strip()
                    if sub_text and len(sub_text) > 20:
                        sub_blocks.append(sub_text)
                        print(
                            f"   ğŸ“ í•˜ìœ„ ë¸”ë¡ {len(sub_blocks)}: ë¬¸ì œ {current_problem_num}ë²ˆ ê´€ë ¨ ({len(sub_text)}ì)"
                        )
                current_block = [line]
                current_problem_num = new_num
            else:
                current_block.append(line)

        if current_block:
            sub_text = "\n".join(current_block).strip()
            if sub_text and len(sub_text) > 20:
                sub_blocks.append(sub_text)
                print(
                    f"   ğŸ“ í•˜ìœ„ ë¸”ë¡ {len(sub_blocks)}: ë¬¸ì œ {current_problem_num}ë²ˆ ê´€ë ¨ ({len(sub_text)}ì)"
                )

        if len(sub_blocks) <= 1:
            return [block_text]
        print(f"ğŸ”§ ë¬¸ì œ {problem_num}ë²ˆ ë³µí•© ë¬¸ì œ ë¶„ë¦¬: {len(sub_blocks)}ê°œ ë¸”ë¡")
        return sub_blocks

    def _find_missing_problems(self, lines: List[str], missing_numbers: set) -> List[str]:
        """ëˆ„ë½ëœ ë¬¸ì œë“¤ì„ ì°¾ì•„ì„œ ì¶”ê°€ ë¸”ë¡ ìƒì„± (ê²½ê³„ ë³´ê°•)"""
        additional_blocks = []
        for missing_num in sorted(missing_numbers):
            print(f"ğŸ” ëˆ„ë½ëœ ë¬¸ì œ {missing_num}ë²ˆ ê²€ìƒ‰ ì¤‘...")
            for i, line in enumerate(lines):
                if str(missing_num) in line and any(
                    keyword in line for keyword in ["ë¬¸ì œ", "ì„¤ëª…", "ê²ƒì€", "?", "ë‹¤ìŒ"]
                ):
                    print(f"   âœ… ë¬¸ì œ {missing_num}ë²ˆ í›„ë³´ ë°œê²¬ - ë¼ì¸ {i+1}: '{line[:50]}...'")
                    start_line = max(0, i - 1)
                    end_line = min(len(lines), i + 10)
                    for j in range(i + 1, min(len(lines), i + 40)):
                        # ê²½ê³„: "## 31." í˜•íƒœ í¬í•¨
                        if re.search(r"^(?:\s*##\s*)?\s*\d+\.", lines[j]):
                            end_line = j
                            break
                    block_text = "\n".join(lines[start_line:end_line]).strip()
                    if block_text and len(block_text) > 20:
                        additional_blocks.append(block_text)
                        print(
                            f"   ğŸ“¦ ì¶”ê°€ ë¸”ë¡ ìƒì„±: ë¼ì¸ {start_line+1}-{end_line} ({len(block_text)}ì)"
                        )
                        break
        return additional_blocks

    def _smart_truncate_block(self, text: str, body_limit: int = 4000) -> str:
        """ë„ˆë¬´ ê¸´ ë¸”ë¡ì„ ë³´ê¸° ì´í›„ ì ë‹¹í•œ ë¹ˆì¤„ì—ì„œ ì»·í•˜ì—¬ LLM ì•ˆì •ì„± í™•ë³´"""
        if len(text) <= body_limit:
            return text
        lines = text.split("\n")
        # ë³´ê¸° íŒ¨í„´ ì´í›„ ì²« ë¹ˆ ì¤„ì—ì„œ ì»·
        option_pat = re.compile(
            r"^\s*(?:-|\*|â€¢)?\s*(?:â‘ |â‘¡|â‘¢|â‘£|â‘¤|\d+\)|[ê°€-í•˜]\)|[A-E]\))"
        )
        cut_idx = None
        for k in range(len(lines)):
            if option_pat.match(lines[k].strip()):
                for m in range(k + 1, min(k + 80, len(lines))):
                    if not lines[m].strip():
                        cut_idx = m
                        break
                break
        if cut_idx:
            return "\n".join(lines[:cut_idx]).strip()
        return "\n".join(lines[: max(1, body_limit // 80)]).strip()

    # -------------------- ì •ê·œí™” --------------------

    def normalize_docling_markdown(self, md: str) -> str:
        """Docling ë§ˆí¬ë‹¤ìš´ ì •ê·œí™”"""
        s = md
        s = re.sub(r"(?m)^\s*(\d+)\.\s*\1\.\s*", r"\1. ", s)  # '1. 1.' -> '1.'
        s = re.sub(r"(?m)^\s*(\d+)\s*\.\s*", r"\1. ", s)  # '1 . ' -> '1. '
        s = re.sub(r"[ \t]+", " ", s).replace("\r", "")
        return s.strip()


# -------------------- ëª¨ë“ˆ ë ˆë²¨ í¸ì˜ í•¨ìˆ˜ --------------------

def extract_pdf_paths(text: str) -> List[str]:
    preprocessor = PDFPreprocessor()
    return preprocessor.extract_pdf_paths(text)


def extract_problem_range(text: str) -> Optional[Dict]:
    preprocessor = PDFPreprocessor()
    return preprocessor.extract_problem_range(text)


def determine_problem_source(text: str) -> Optional[str]:
    preprocessor = PDFPreprocessor()
    return preprocessor.determine_problem_source(text)
