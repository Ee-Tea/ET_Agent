"""
PDF ì „ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ ëª¨ë“ˆ
teacher_graph.pyì—ì„œ PDF ê´€ë ¨ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì„
"""

import os
import re
import json
from typing import List, Dict, Optional
from dotenv import load_dotenv
load_dotenv()

from docling.document_converter import DocumentConverter
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2048"))


class PDFPreprocessor:
    """PDF íŒŒì¼ ì „ì²˜ë¦¬ ë° ë¬¸ì œ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ê¶Œí•œ ë¬¸ì œ í•´ê²°
        os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
        os.environ['HF_HOME'] = 'C:\\temp\\huggingface_cache'
        
        # cv2 setNumThreads ë¬¸ì œ í•´ê²°
        try:
            import cv2
            if not hasattr(cv2, 'setNumThreads'):
                # setNumThreadsê°€ ì—†ìœ¼ë©´ ë”ë¯¸ í•¨ìˆ˜ ì¶”ê°€
                cv2.setNumThreads = lambda x: None
        except ImportError:
            pass
    
    def extract_pdf_paths(self, text: str) -> List[str]:
        """PDF íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ"""
        # PDF íŒŒì¼ ê²½ë¡œ íŒ¨í„´ ë§¤ì¹­
        pdf_patterns = [
            r'([^\s]+\.pdf)',  # ê¸°ë³¸ .pdf íŒŒì¼ ê²½ë¡œ
            r'([C-Z]:[\\\/][^\\\/\s]*\.pdf)',  # Windows ì ˆëŒ€ ê²½ë¡œ
            r'([\.\/][^\\\/\s]*\.pdf)',  # ìƒëŒ€ ê²½ë¡œ
        ]
        
        pdf_paths = []
        for pattern in pdf_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            pdf_paths.extend(matches)
        
        return list(set(pdf_paths))  # ì¤‘ë³µ ì œê±°
    
    def extract_problem_range(self, text: str) -> Optional[Dict]:
        """ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„ ì¶”ì¶œ"""
        # íŒ¨í„´ë“¤: "5ë²ˆ", "1-10ë²ˆ", "3ë²ˆë¶€í„° 7ë²ˆê¹Œì§€", "1,3,5ë²ˆ"
        patterns = [
            r'(\d+)ë²ˆë§Œ',  # "5ë²ˆë§Œ"
            r'(\d+)ë²ˆ\s*í’€',  # "5ë²ˆ í’€ì–´ì¤˜"
            r'(\d+)\s*[-~]\s*(\d+)ë²ˆ',  # "1-10ë²ˆ", "1~10ë²ˆ"
            r'(\d+)ë²ˆë¶€í„°\s*(\d+)ë²ˆ',  # "3ë²ˆë¶€í„° 7ë²ˆê¹Œì§€"
            r'(\d+(?:\s*,\s*\d+)*)ë²ˆ',  # "1,3,5ë²ˆ"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                groups = match.groups()
                if len(groups) == 1:
                    if ',' in groups[0]:
                        # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë²ˆí˜¸ë“¤
                        numbers = [int(x.strip()) for x in groups[0].split(',')]
                        return {"type": "specific", "numbers": numbers}
                    else:
                        # ë‹¨ì¼ ë²ˆí˜¸
                        return {"type": "single", "number": int(groups[0])}
                elif len(groups) == 2:
                    # ë²”ìœ„
                    start, end = int(groups[0]), int(groups[1])
                    return {"type": "range", "start": start, "end": end}
        return None
    
    def determine_problem_source(self, text: str) -> Optional[str]:
        """ë¬¸ì œ ì†ŒìŠ¤ ê²°ì •"""
        text_lower = text.lower()
        
        # ëª…ì‹œì  ì†ŒìŠ¤ ì§€ì •
        if any(keyword in text_lower for keyword in ['pdf', 'íŒŒì¼', 'ë¬¸ì„œ']):
            return "pdf_extracted"
        elif any(keyword in text_lower for keyword in ['ê¸°ì¡´', 'shared', 'ì €ì¥ëœ', 'ì´ì „']):
            return "shared"
        
        # PDF íŒŒì¼ì´ ëª…ì‹œë˜ì—ˆìœ¼ë©´ pdf_extracted ìš°ì„ 
        if self.extract_pdf_paths(text):
            return "pdf_extracted"
        
        # ì•„ë¬´ê²ƒë„ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ None (ìë™ ê²°ì •)
        return None
    
    def extract_problems_from_pdf(self, file_paths: List[str]) -> List[Dict]:
        """PDF íŒŒì¼ì—ì„œ ë¬¸ì œ ì¶”ì¶œ (Docling ì‚¬ìš©)"""
        try:
            # Docling ë³€í™˜ê¸° ì´ˆê¸°í™” - ì„¤ì • ê°œì„ 
            print("ğŸ”§ DocumentConverter ì´ˆê¸°í™” ì¤‘...")
            converter = DocumentConverter()
            
            # Docling ì„¤ì • ì¡°ì •
            try:
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™” ì‹œë„
                converter.config.image_processing = False
                print("âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™” ì„¤ì •")
            except:
                print("âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë³€ê²½ ë¶ˆê°€")
            
            try:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ì„¤ì •
                converter.config.text_extraction_priority = "text"
                print("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ì„¤ì •")
            except:
                print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ì„¤ì • ë¶ˆê°€")
                
            print("âœ… DocumentConverter ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ DocumentConverter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e)}")
            import traceback
            traceback.print_exc()
            return []
        
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"), 
            model=OPENAI_LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS
        )
        
        all_problems = []
        
        for path in file_paths:
            try:
                print(f"ğŸ“– íŒŒì¼ ì²˜ë¦¬ ì¤‘: {path}")
                
                # Doclingìœ¼ë¡œ PDF ë³€í™˜ - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
                doc_result = converter.convert(path)
                
                # ë°©ë²• 1: ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ
                raw_text = doc_result.document.export_to_markdown()
                print(f"ğŸ“ [ë°©ë²•1] ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ ê²°ê³¼ (ê¸¸ì´: {len(raw_text)}ì)")
                print(f"   ë¯¸ë¦¬ë³´ê¸°: '{raw_text[:200]}...'")
                
                # ë°©ë²• 2: í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ
                try:
                    raw_text2 = doc_result.document.text
                    print(f"ğŸ“ [ë°©ë²•2] í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ê²°ê³¼ (ê¸¸ì´: {len(raw_text2)}ì)")
                    print(f"   ë¯¸ë¦¬ë³´ê¸°: '{raw_text2[:200]}...'")
                    
                    # í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œì´ ë” ë‚˜ìœ¼ë©´ ì‚¬ìš©
                    if len(raw_text2) > len(raw_text) and not raw_text2.startswith('<!--'):
                        raw_text = raw_text2
                        print("âœ… í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ë°©ì‹ ì‚¬ìš©")
                except Exception as e:
                    print(f"âš ï¸ í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                
                # ë°©ë²• 3: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                try:
                    pages_text = []
                    for page in doc_result.document.pages:
                        page_text = page.text
                        if page_text and not page_text.startswith('<!--'):
                            pages_text.append(page_text)
                    
                    if pages_text:
                        raw_text3 = '\n\n'.join(pages_text)
                        print(f"ğŸ“ [ë°©ë²•3] í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ (ê¸¸ì´: {len(raw_text3)}ì)")
                        print(f"   ë¯¸ë¦¬ë³´ê¸°: '{raw_text3[:200]}...'")
                        
                        # í˜ì´ì§€ë³„ ì¶”ì¶œì´ ë” ë‚˜ìœ¼ë©´ ì‚¬ìš©
                        if len(raw_text3) > len(raw_text) and not raw_text3.startswith('<!--'):
                            raw_text = raw_text3
                            print("âœ… í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ ì‚¬ìš©")
                except Exception as e:
                    print(f"âš ï¸ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                
                # ë°©ë²• 4: ë§ˆí¬ë‹¤ìš´ì—ì„œ HTML íƒœê·¸ ì œê±°
                if raw_text.startswith('<!--'):
                    print("ğŸ”„ ë§ˆí¬ë‹¤ìš´ì—ì„œ HTML íƒœê·¸ ì œê±° ì‹œë„...")
                    try:
                        # HTML ì£¼ì„ê³¼ íƒœê·¸ ì œê±°
                        import re
                        cleaned_text = re.sub(r'<!--.*?-->', '', raw_text, flags=re.DOTALL)
                        cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)
                        cleaned_text = re.sub(r'^\s*-\s*', '', cleaned_text, flags=re.MULTILINE)
                        cleaned_text = re.sub(r'^\s*$', '', cleaned_text, flags=re.MULTILINE)
                        cleaned_text = '\n'.join(line for line in cleaned_text.split('\n') if line.strip())
                        
                        if cleaned_text and len(cleaned_text) > 50:
                            raw_text = cleaned_text
                            print(f"âœ… HTML íƒœê·¸ ì œê±° ì„±ê³µ (ê¸¸ì´: {len(raw_text)}ì)")
                            print(f"   ë¯¸ë¦¬ë³´ê¸°: '{raw_text[:200]}...'")
                        else:
                            print("âš ï¸ HTML íƒœê·¸ ì œê±° í›„ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
                    except Exception as e:
                        print(f"âš ï¸ HTML íƒœê·¸ ì œê±° ì‹¤íŒ¨: {e}")
                
                if not raw_text.strip() or raw_text.startswith('<!--'):
                    print(f"âŒ ëª¨ë“  Docling ë°©ë²•ìœ¼ë¡œë„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    print(f"âš ï¸ PDF íŒŒì¼ ìì²´ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ")
                    continue
                
                # ë””ë²„ê¹…: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¼ë¶€ ì¶œë ¥
                print(f"ğŸ“ ìµœì¢… ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):")
                print(f"'{raw_text[:500]}...'")
                print(f"ğŸ“Š ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)} ë¬¸ì")
                
                # 1ë‹¨/2ë‹¨ êµ¬ë¶„ ë° ì²˜ë¦¬
                blocks = self._process_pdf_text(raw_text, path)
                print(f"ğŸ“ {len(blocks)}ê°œ ë¸”ë¡ìœ¼ë¡œ ë¶„í• ")
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë¸”ë¡ ë¯¸ë¦¬ë³´ê¸°
                if blocks:
                    print(f"ğŸ” ì²« ë²ˆì§¸ ë¸”ë¡ ë¯¸ë¦¬ë³´ê¸°:")
                    print(f"'{blocks[0][:300]}...'")
                    if len(blocks) > 1:
                        print(f"ğŸ” ë‘ ë²ˆì§¸ ë¸”ë¡ ë¯¸ë¦¬ë³´ê¸°:")
                        print(f"'{blocks[1][:300]}...'")
                        print(f"ğŸ” ë§ˆì§€ë§‰ ë¸”ë¡ ë¯¸ë¦¬ë³´ê¸°:")
                        print(f"'{blocks[-1][:300]}...')")
                
                # ê° ë¸”ë¡ì„ LLMìœ¼ë¡œ íŒŒì‹±
                successful_parses = 0
                for i, block in enumerate(blocks):
                    block_len = len(block.strip())
                    if block_len < 20:  # í•„í„°ë§ ì¡°ê±´ì„ ì™„í™” (50 â†’ 20)
                        print(f"âš ï¸ ë¸”ë¡ {i+1} ìŠ¤í‚µ (ë„ˆë¬´ ì§§ìŒ: {block_len}ì): '{block[:50]}...'")
                        continue
                    
                    print(f"ğŸ”„ ë¸”ë¡ {i+1}/{len(blocks)} íŒŒì‹± ì¤‘ ({block_len}ì)...")
                    print(f"   ë¯¸ë¦¬ë³´ê¸°: '{block[:100]}...'")
                        
                    try:
                        problem = self._parse_block_with_llm(block, llm)
                        if problem:
                            all_problems.append(problem)
                            successful_parses += 1
                            print(f"âœ… ë¸”ë¡ {i+1} íŒŒì‹± ì„±ê³µ! (ì´ {successful_parses}ê°œ)")
                        else:
                            print(f"âŒ ë¸”ë¡ {i+1} íŒŒì‹± ì‹¤íŒ¨: LLMì´ ìœ íš¨í•œ ë¬¸ì œë¡œ ì¸ì‹í•˜ì§€ ëª»í•¨")
                    except Exception as e:
                        print(f"âš ï¸ ë¸”ë¡ {i+1} íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                        
                print(f"ğŸ“Š íŒŒì‹± ê²°ê³¼: {successful_parses}/{len(blocks)} ë¸”ë¡ ì„±ê³µ")
                        
            except Exception as e:
                print(f"âŒ íŒŒì¼ {path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"ğŸ¯ ì´ {len(all_problems)}ê°œ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
        return all_problems
    
    def _process_pdf_text(self, raw_text: str, pdf_path: str) -> List[str]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ 1ë‹¨/2ë‹¨ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬"""
        print("ğŸ” [ë ˆì´ì•„ì›ƒ ë¶„ì„] 1ë‹¨/2ë‹¨ êµ¬ì¡° íŒŒì•… ì¤‘...")
        
        # 1ë‹¨ êµ¬ì¡°ë¡œ ë¨¼ì € ì‹œë„
        blocks = self._split_problem_blocks(raw_text)
        
        # 1ë‹¨ íŒŒì‹± ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ 2ë‹¨ êµ¬ì¡°ë¡œ ì¬ì‹œë„
        if len(blocks) <= 2:
            print("âš ï¸ 1ë‹¨ íŒŒì‹± ê²°ê³¼ ë¶€ì¡± - 2ë‹¨ êµ¬ì¡°ë¡œ ì¬ì‹œë„")
            try:
                # 2ë‹¨ ì¬ì •ë ¬
                reordered_text = self._reorder_two_columns_with_pdfminer(pdf_path)
                reordered_text = self.normalize_docling_markdown(reordered_text)
                
                # 2ë‹¨ ì¬ì •ë ¬ í›„ íŒŒì‹± ì‹œë„
                blocks = self._split_problem_blocks(reordered_text)
                print(f"ğŸ”„ 2ë‹¨ ì¬ì •ë ¬ í›„: {len(blocks)}ê°œ ë¸”ë¡")
                
                # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ìˆ«ì í—¤ë” í´ë°± ì‚¬ìš©
                if len(blocks) <= 2:
                    print("âš ï¸ 2ë‹¨ íŒŒì‹±ë„ ë¶€ì¡± - ìˆ«ì í—¤ë” í´ë°± ì‚¬ìš©")
                    blocks = self._split_problem_blocks_without_keyword(reordered_text)
                    print(f"ğŸ”„ í´ë°± í›„: {len(blocks)}ê°œ ë¸”ë¡")
                    
            except Exception as e:
                print(f"âš ï¸ 2ë‹¨ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # 2ë‹¨ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ í´ë°±
                blocks = self._split_problem_blocks_without_keyword(raw_text)
        
        return blocks
    
    def _reorder_two_columns_with_pdfminer(self, pdf_path: str) -> str:
        """PDFMinerë¥¼ ì‚¬ìš©í•˜ì—¬ 2ë‹¨ PDFë¥¼ 1ë‹¨ìœ¼ë¡œ ì¬ì •ë ¬"""
        try:
            from pdfminer.high_level import extract_pages
            from pdfminer.layout import LTTextContainer
            
            print("ğŸ”„ [2ë‹¨ ì¬ì •ë ¬] PDFMinerë¡œ ì¢Œìš° ì»¬ëŸ¼ ì¬ì •ë ¬ ì¤‘...")
            
            pages_text = []
            for page_layout in extract_pages(pdf_path):
                left, right = [], []
                
                # x ë¶„í•  ê¸°ì¤€ê°’ì„ í˜ì´ì§€ í­ì˜ ì¤‘ê°„ì¯¤ìœ¼ë¡œ ì„¤ì • (íœ´ë¦¬ìŠ¤í‹±)
                # LTTextContainerì˜ bbox=(x0,y0,x1,y1)
                # ë¨¼ì € í‰ê·  x0ë¥¼ ë³´ê³  ì¤‘ì•™ê°’ì„ ì¶”ì •í•˜ëŠ” ë³´ì • ë¡œì§
                xs = []
                for el in page_layout:
                    if isinstance(el, LTTextContainer):
                        xs.append(el.bbox[0])
                
                if not xs:
                    continue
                    
                # ì¤‘ì•™ê°’ ê³„ì‚° (ë” ì•ˆì •ì ì¸ ë°©ë²•)
                sorted_xs = sorted(xs)
                mid = sorted_xs[len(sorted_xs)//2]
                
                # ì¢Œìš° ì»¬ëŸ¼ ë¶„ë¦¬
                for el in page_layout:
                    if isinstance(el, LTTextContainer):
                        (x0, y0, x1, y1) = el.bbox
                        text = el.get_text().strip()
                        if text:  # ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸
                            (left if x0 < mid else right).append((y1, text))
                
                # y1 ê¸°ì¤€ìœ¼ë¡œ ìœ„â†’ì•„ë˜ ì •ë ¬ (y1ì´ í´ìˆ˜ë¡ ìœ„ìª½)
                left.sort(key=lambda t: -t[0])
                right.sort(key=lambda t: -t[0])
                
                # ì™¼ìª½ ì „ì²´ â†’ ì˜¤ë¥¸ìª½ ì „ì²´ ìˆœìœ¼ë¡œ í•©ì¹˜ê¸°
                page_text = "".join(t for _, t in left) + "\n" + "".join(t for _, t in right)
                pages_text.append(page_text)
            
            result = "\n\n".join(pages_text)
            print(f"âœ… [2ë‹¨ ì¬ì •ë ¬ ì™„ë£Œ] ì´ {len(pages_text)}í˜ì´ì§€ ì²˜ë¦¬")
            return result
            
        except ImportError:
            print("âš ï¸ PDFMinerê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - 2ë‹¨ ì¬ì •ë ¬ ë¶ˆê°€")
            return ""
        except Exception as e:
            print(f"âš ï¸ 2ë‹¨ ì¬ì •ë ¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def _split_problem_blocks_without_keyword(self, text: str) -> List[str]:
        """ë¬¸ì œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ì‹œí—˜ì§€ì—ì„œ ë²ˆí˜¸(1., 2., â€¦)ë§Œìœ¼ë¡œ ë¬¸í•­ ë‹¨ìœ„ë¥¼ ë¶„í• """
        print("ğŸ”„ [í´ë°± íŒŒì‹±] ë¬¸ì œ í‚¤ì›Œë“œ ì—†ì´ ë²ˆí˜¸ë§Œìœ¼ë¡œ ë¶„í•  ì‹œë„")
        
        text = self.normalize_docling_markdown(text)
        lines = text.split('\n')
        n = len(lines)
        
        # ë¬¸í•­ í—¤ë” í›„ë³´ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        _QHEAD_CAND = re.compile(r'(?m)^\s*(\d{1,3})[.)]\s+\S')
        candidates = []
        
        for i, ln in enumerate(lines):
            m = _QHEAD_CAND.match(ln or '')
            if m:
                num = int(m.group(1))
                # ë³´ê¸° ë²ˆí˜¸ê°€ ì•„ë‹Œì§€ í™•ì¸ (1), 2), 3), 4)ëŠ” ë³´ê¸°)
                if not re.match(r'^\s*\d+\)\s*', ln):
                    # ì¶”ê°€ ê²€ì¦: ì‹¤ì œ ë¬¸ì œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                    if len(ln.strip()) > 10:  # ìµœì†Œ 10ì ì´ìƒ
                        candidates.append((i, num))
                        print(f"ğŸ” [í´ë°±] ë¼ì¸ {i}: '{ln[:50]}...' â†’ í›„ë³´ ë²ˆí˜¸ {num}")
                    else:
                        print(f"ğŸ” [í´ë°±] ë¼ì¸ {i}: '{ln[:50]}...' â†’ ë„ˆë¬´ ì§§ì•„ì„œ ì œì™¸")
                else:
                    print(f"ğŸ” [í´ë°±] ë¼ì¸ {i}: '{ln[:50]}...' â†’ ë³´ê¸° ë²ˆí˜¸ë¡œ íŒë‹¨í•˜ì—¬ ì œì™¸")
        
        print(f"ğŸ” [í´ë°±] ì´ í›„ë³´ ìˆ˜: {len(candidates)}")
        
        # ì „ì—­ ì¦ê°€ ì‹œí€€ìŠ¤ + ì„¹ì…˜ ë¦¬ì…‹ í—ˆìš©ìœ¼ë¡œ ì‹¤ì œ í—¤ë” ì„ ë³„
        headers = []
        prev_num = 0
        last_header_idx = -9999
        
        for i, num in candidates:
            if num == prev_num + 1:
                headers.append(i)
                prev_num = num
                last_header_idx = i
                print(f"âœ… [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ìˆœì°¨ ì¦ê°€ë¡œ í—¤ë” ì„ íƒ")
                continue
            
            # ì„¹ì…˜ ë¦¬ì…‹: num==1ì´ê³ , ìµœê·¼ í—¤ë”ì—ì„œ ì¶©ë¶„íˆ ë–¨ì–´ì ¸ ìˆê±°ë‚˜ ì„¹ì…˜ ëŠë‚Œì˜ ë¼ì¸ ì¡´ì¬ ì‹œ í—ˆìš©
            if num == 1:
                window = '\n'.join(lines[max(0, i-3): i+1])
                if (i - last_header_idx) >= 8 or re.search(r'(â… |â…¡|III|ê³¼ëª©|íŒŒíŠ¸|SECTION)', window):
                    headers.append(i)
                    prev_num = 1
                    last_header_idx = i
                    print(f"âœ… [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ì„¹ì…˜ ë¦¬ì…‹ìœ¼ë¡œ í—¤ë” ì„ íƒ")
                    continue
                else:
                    print(f"âŒ [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ì„¹ì…˜ ë¦¬ì…‹ ì¡°ê±´ ë¶ˆì¶©ì¡± (ê±°ë¦¬: {i - last_header_idx})")
            else:
                print(f"âŒ [í´ë°±] ë¼ì¸ {i}: ë²ˆí˜¸ {num} - ìˆœì°¨ ì¦ê°€ ì•„ë‹˜ (ì˜ˆìƒ: {prev_num + 1})")
        
        # í—¤ë”ê°€ í•˜ë‚˜ë„ ì•ˆ ì¡íˆë©´ í´ë°± ì „ëµ ì‚¬ìš©
        if not headers:
            print(f"âŒ [í´ë°±] í—¤ë”ê°€ í•˜ë‚˜ë„ ì„ íƒë˜ì§€ ì•ŠìŒ - í´ë°± ì „ëµ ì‚¬ìš©")
            if candidates:
                print(f"ğŸ”„ [í´ë°±] ìˆœì°¨ ì¡°ê±´ ì—†ì´ ëª¨ë“  í›„ë³´ë¥¼ í—¤ë”ë¡œ ì‚¬ìš©")
                headers = [i for i, num in candidates]
            else:
                # ê¸°ë³¸ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
                print(f"ğŸ”„ [í´ë°±] ê¸°ë³¸ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• ")
                simple_pattern = re.compile(r'(?m)^\s*(\d{1,2})\.\s+')
                for i, ln in enumerate(lines):
                    if simple_pattern.match(ln or ''):
                        # ë³´ê¸° ë²ˆí˜¸ê°€ ì•„ë‹Œì§€ í™•ì¸
                        if not re.match(r'^\s*\d+\)\s*', ln):
                            headers.append(i)
                            print(f"ğŸ“Œ [í´ë°±] ë¼ì¸ {i}: '{ln[:30]}...' â†’ í—¤ë” ì¶”ê°€")
                        else:
                            print(f"ğŸ“Œ [í´ë°±] ë¼ì¸ {i}: '{ln[:30]}...' â†’ ë³´ê¸° ë²ˆí˜¸ë¡œ íŒë‹¨í•˜ì—¬ ì œì™¸")
            
            if not headers:
                print(f"âŒ [í´ë°± ì‹¤íŒ¨] ì „ì²´ë¥¼ 1ê°œ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬")
                return [text] if text.strip() else []
        
        print(f"âœ… [í´ë°±] ìµœì¢… ì„ íƒëœ í—¤ë” ìˆ˜: {len(headers)}")
        
        # í—¤ë” ë²”ìœ„ë¡œ ë¸”ë¡ ë§Œë“¤ê¸°
        headers.append(n)  # sentinel
        blocks = []
        for a, b in zip(headers[:-1], headers[1:]):
            blk = '\n'.join(lines[a:b]).strip()
            if blk:
                blocks.append(blk)
                print(f"ğŸ“¦ [í´ë°±] ë¸”ë¡ {len(blocks)}: ë¼ì¸ {a}-{b-1} ({len(blk)}ì)")
        
        print(f"ğŸ¯ [í´ë°±] ìµœì¢… ë¸”ë¡ ìˆ˜: {len(blocks)}")
        return blocks
    
    def _split_problem_blocks(self, raw_text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì œ ë¸”ë¡ìœ¼ë¡œ ë¶„í•  (ì‹¤ì œ ë¬¸ì œ í—¤ë” ê¸°ë°˜)"""
        print("ğŸ” [êµ¬ì¡° ë¶„ì„] ì‹¤ì œ ë¬¸ì œ í—¤ë” ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹± ë°©ì‹ ê²°ì •")
        
        lines = raw_text.split('\n')
        
        # ë””ë²„ê¹…: ì „ì²´ ë¼ì¸ êµ¬ì¡° ë¶„ì„
        print(f"ğŸ“Š ì „ì²´ ë¼ì¸ ìˆ˜: {len(lines)}")
        print("ğŸ” ë¼ì¸ë³„ ë‚´ìš© ë¶„ì„ (ì²˜ìŒ 50ì¤„):")
        for i, line in enumerate(lines[:50]):
            if line.strip():
                print(f"   ë¼ì¸ {i+1:2d}: '{line.strip()}'")
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë“¤ ì°¾ê¸°
        print("\nğŸ” ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë“¤:")
        number_lines = []
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if line_stripped and re.match(r'^\d+\.', line_stripped):
                number_lines.append((i, line_stripped))
                print(f"   ë¼ì¸ {i+1:2d}: '{line_stripped[:100]}...'")
        
        print(f"ğŸ“Š ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ìˆ˜: {len(number_lines)}")
        
        # ì‹¤ì œ ë¬¸ì œ í—¤ë” íŒ¨í„´ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        problem_header_patterns = [
            r'^\s*##\s*ë¬¸ì œ\s*(\d+)\s*\.\s*',   # "## ë¬¸ì œ 1." (ë§ˆí¬ë‹¤ìš´ í—¤ë”)
            r'^\s*#+\s*ë¬¸ì œ\s*(\d+)\s*\.\s*',   # "# ë¬¸ì œ 1.", "### ë¬¸ì œ 1." ë“±
            r'^\s*ë¬¸ì œ\s*(\d+)\s*\.\s*',        # "ë¬¸ì œ 1." (ì ë§Œ)
            r'^\s*(\d+)\s*\.\s*',                # "1." (ìˆ«ì. + ê³µë°±)
            r'^\s*(\d+)\s*\.\s*\S',             # "1. í…ìŠ¤íŠ¸" (ìˆ«ì. + ê³µë°± + í…ìŠ¤íŠ¸)
            r'^\s*Q\s*(\d+)\s*\.\s*',           # "Q1." (ì ë§Œ)
            r'^\s*\[(\d+)\]\s*',                 # "[1]"
            # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì•ˆì˜ ìˆ«ì íŒ¨í„´ ì¶”ê°€
            r'^\s*#+\s*.*?(\d+)\s*\.\s*',       # "## ... 1. ..." (ë§ˆí¬ë‹¤ìš´ í—¤ë” ì•ˆì˜ ìˆ«ì)
        ]
        
        # ë³´ê¸° ë²ˆí˜¸ íŒ¨í„´ë“¤ (ë¬¸ì œ í—¤ë”ê°€ ì•„ë‹˜)
        option_patterns = [
            r'^\s*(\d+)\.\s*\1\.\s*',           # "4. 4." (ì¤‘ë³µ ë²ˆí˜¸)
            r'^\s*(\d+)\s*[)]\s*',              # "1)", "2)" (ë³´ê¸° ë²ˆí˜¸ - ê´„í˜¸ë§Œ)
            r'^\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*',      # ì›ë¬¸ì ë³´ê¸°
            r'^\s*[ê°€-í•˜]\s*[)]\s*',            # "ê°€)", "ë‚˜)" (ë³´ê¸°)
            r'^\s*[A-E]\s*[)]\s*',              # "A)", "B)" (ë³´ê¸°)
        ]
        
        # ë¬¸ì œ í—¤ë” ìœ„ì¹˜ ì°¾ê¸°
        problem_headers = []
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if not line_stripped:
                continue
                
            # ë³´ê¸° ë²ˆí˜¸ì¸ì§€ ë¨¼ì € í™•ì¸
            is_option = False
            for pattern in option_patterns:
                if re.match(pattern, line_stripped):
                    is_option = True
                    break
            
            if is_option:
                continue  # ë³´ê¸° ë²ˆí˜¸ëŠ” ìŠ¤í‚µ
            
            # ë¬¸ì œ í—¤ë”ì¸ì§€ í™•ì¸
            for pattern in problem_header_patterns:
                match = re.match(pattern, line_stripped)
                if match:
                    problem_num = int(match.group(1))
                    problem_headers.append((i, problem_num, line_stripped))
                    print(f"âœ… [ë¬¸ì œ í—¤ë” ë°œê²¬] ë¼ì¸ {i+1}: '{line_stripped}' (ë¬¸ì œ {problem_num}ë²ˆ)")
                    break
        
        if not problem_headers:
            print("âš ï¸ ë¬¸ì œ í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì „ì²´ë¥¼ 1ê°œ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬")
            return [raw_text] if raw_text.strip() else []
        
        print(f"ğŸ” ì´ {len(problem_headers)}ê°œ ë¬¸ì œ í—¤ë” ë°œê²¬")
        
        # ë¬¸ì œ í—¤ë”ë¥¼ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
        problem_headers.sort(key=lambda x: x[1])
        
        # ë¬¸ì œ ë¸”ë¡ ìƒì„±
        problem_blocks = []
        
        for i, (header_idx, problem_num, header_text) in enumerate(problem_headers):
            # í˜„ì¬ ë¬¸ì œì˜ ì‹œì‘
            start_line = header_idx
            
            # ë‹¤ìŒ ë¬¸ì œì˜ ì‹œì‘ (ë˜ëŠ” ë§ˆì§€ë§‰)
            if i + 1 < len(problem_headers):
                end_line = problem_headers[i + 1][0]
            else:
                end_line = len(lines)
            
            # ë¬¸ì œ ë¸”ë¡ í…ìŠ¤íŠ¸ ìƒì„±
            problem_text = '\n'.join(lines[start_line:end_line]).strip()
            
            if problem_text:
                problem_blocks.append(problem_text)
                print(f"ğŸ“¦ ë¬¸ì œ {problem_num}ë²ˆ: ë¼ì¸ {start_line+1}-{end_line} ({len(problem_text)}ì)")
                print(f"   í—¤ë”: '{header_text}'")
        
        print(f"âœ… ì´ {len(problem_blocks)}ê°œ ë¬¸ì œ ë¸”ë¡ ìƒì„± ì™„ë£Œ")
        return problem_blocks
    
    def normalize_docling_markdown(self, md: str) -> str:
        """Docling ë§ˆí¬ë‹¤ìš´ ì •ê·œí™”"""
        s = md
        s = re.sub(r'(?m)^\s*(\d+)\.\s*\1\.\s*', r'\1. ', s)  # '1. 1.' -> '1.'
        s = re.sub(r'(?m)^\s*(\d+)\s*\.\s*', r'\1. ', s)      # '1 . ' -> '1. '
        s = re.sub(r'[ \t]+', ' ', s).replace('\r', '')
        return s.strip()
    
    def _parse_block_with_llm(self, block_text: str, llm) -> Optional[Dict]:
        """LLMìœ¼ë¡œ ë¸”ë¡ì„ ë¬¸ì œ í˜•íƒœë¡œ íŒŒì‹±"""
        sys_prompt = (
            "ë„ˆëŠ” ì‹œí—˜ ë¬¸ì œ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
            "ë¬¸ì œ ì§ˆë¬¸ê³¼ ë³´ê¸°ë¥¼ êµ¬ë¶„í•´ì„œ questionê³¼ options ë°°ì—´ë¡œ ì¶œë ¥í•œë‹¤. "
            "optionsëŠ” ë³´ê¸° í•­ëª©ë§Œ í¬í•¨í•˜ê³ , ì„¤ëª…/í•´ì„¤/ì •ë‹µ ë“±ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. "
            "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ ë¬¸ì¥ì´ë‚˜ ì½”ë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ."
        )
        
        user_prompt = (
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸í•­ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ, ì •í™•íˆ ì¶”ì¶œí•´ JSONìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.\n"
            "ìš”êµ¬ ìŠ¤í‚¤ë§ˆ: {\"question\":\"...\",\"options\":[\"...\",\"...\"]}\n"
            "ê·œì¹™:\n"
            "- ë¬¸ì œ ì§ˆë¬¸ì—ì„œ ë²ˆí˜¸(ì˜ˆ: 'ë¬¸ì œ 1.' ë“±)ì™€ ë¶ˆí•„ìš”í•œ ë¨¸ë¦¬ê¸€ì€ ì œê±°.\n"
            "- ì˜µì…˜ì€ 4ê°œê°€ ì¼ë°˜ì ì„.\n"
            f"í…ìŠ¤íŠ¸:\n{block_text[:1000]}"  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ
        )
        
        try:
            response = llm.invoke([
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_prompt}
            ])
            
            content = response.content.strip()
            
            # JSON ì¶”ì¶œ
            if content.startswith('```'):
                content = re.sub(r'^```(?:json)?\s*', '', content)
                content = re.sub(r'\s*```$', '', content)
            
            data = json.loads(content)
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if isinstance(data, dict) and "question" in data and "options" in data:
                if data["question"].strip() and isinstance(data["options"], list) and len(data["options"]) > 0:
                    return data
                    
        except Exception as e:
            print(f"âš ï¸ LLM íŒŒì‹± ì‹¤íŒ¨: {e}")
            
        return None


# í¸ì˜ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±)
def extract_pdf_paths(text: str) -> List[str]:
    """PDF íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ (í¸ì˜ í•¨ìˆ˜)"""
    preprocessor = PDFPreprocessor()
    return preprocessor.extract_pdf_paths(text)


def extract_problem_range(text: str) -> Optional[Dict]:
    """ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„ ì¶”ì¶œ (í¸ì˜ í•¨ìˆ˜)"""
    preprocessor = PDFPreprocessor()
    return preprocessor.extract_problem_range(text)


def determine_problem_source(text: str) -> Optional[str]:
    """ë¬¸ì œ ì†ŒìŠ¤ ê²°ì • (í¸ì˜ í•¨ìˆ˜)"""
    preprocessor = PDFPreprocessor()
    return preprocessor.determine_problem_source(text)
