from openai import OpenAI
from typing import List
import os 
import json
import re
from dotenv import load_dotenv
load_dotenv()

groq_api_key = os.getenv("GROQAI_API_KEY")
client = OpenAI(base_url="https://api.groq.com/openai/v1", api_key=groq_api_key)

def parse_llama_json(result: str) -> dict:
    """
    LLaMA ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ì—¬ dictë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì½”ë“œ ë¸”ë¡(```json â€¦ ```) ê°ì‹¸ì§ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    cleaned = result.strip()

    # ì½”ë“œ ë¸”ë¡ ì œê±° (```json ë˜ëŠ” ```)
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```(?:json)?", "", cleaned, flags=re.IGNORECASE).strip()
        cleaned = re.sub(r"```$", "", cleaned, flags=re.IGNORECASE).strip()

    # BOM ì œê±°
    cleaned = cleaned.encode("utf-8").decode("utf-8-sig")

    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨")
        print("â†’ ì˜¤ë¥˜:", e)
        print("â†’ ì›ë³¸:", repr(cleaned))
        return {}

def extract_query_elements(user_question: str) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    system_prompt = """ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì•„ë˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
    - keyword: ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ì‚¬ìš©ìê°€ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ìš©ì–´

    JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆì‹œ:
    {
    "keyword": "LLM"
    }"""

    response = client.chat.completions.create(
        model="meta-llama/llama-4-scout-17b-16e-instruct",  # ë˜ëŠ” gpt-3.5-turbo ì‚¬ìš© ê°€ëŠ¥
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question}
        ],
        temperature=0.2
    )
    result = response.choices[0].message.content
    result_dict = parse_llama_json(result)
    keyword = result_dict.get("keyword", "")
    if isinstance(keyword, str):
        return [keyword]
    elif isinstance(keyword, list):
        return keyword
    else:
        return []

def query_rewrite(question: str, keywords: list[str]) -> str:
    """
    ì§ˆë¬¸ì„ LLMì— ë§ê²Œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    """
    keyword_text = ", ".join(keywords)
    original_question = question
    rewrite_prompt = f"""
    ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ê³¼ ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê²€ìƒ‰ì— ì í•©í•œ í˜•íƒœì˜ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.

    - ì‚¬ìš©ì ì§ˆë¬¸: {original_question}
    - ì¶”ì¶œ í‚¤ì›Œë“œ: {keyword_text}

    ==> ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì§ˆë¬¸:
    """
    rewritten_question = client.chat.completions.create(
        model="meta-llama/llama-4-scout-17b-16e-instruct",  # ì‹¤ì œ ì‚¬ìš© ëª¨ë¸ëª… í™•ì¸ í•„ìš”
        messages=[{"role": "user", "content": rewrite_prompt}],
        temperature=0.5,
    )
    return rewritten_question.choices[0].message.content.strip()

def query_reinforce(state: dict) -> dict:
    """
    ê²€ì¦ì— ì‹¤íŒ¨í•œ ê²½ìš° ì› ì§ˆë¬¸ì„ ë³´ì™„í•˜ì—¬ ì¬ì‘ì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    original_question = state["retrieval_question"]
    verdict = state.get("fact_check_result", {}).get("verdict", "NOT ENOUGH INFO")
    answer = state.get("answer", "")
    context = state.get("merged_context", "")

    prompt = f"""
    ì´ì „ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

    "{original_question}"

    í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ LLM ì‘ë‹µì€ ë‹¤ìŒê³¼ ê°™ì•˜ìŠµë‹ˆë‹¤:

    "{answer}"

    í•˜ì§€ë§Œ ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€í† í•œ ê²°ê³¼, ì´ ì‘ë‹µì€ "{verdict}" íŒì •ì„ ë°›ì•˜ìŠµë‹ˆë‹¤:

    "{context}"

    ë”°ë¼ì„œ, ì´ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  ì‚¬ì‹¤ ê²€ì¦ì´ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
    """

    response = client.chat.completions.create(
        model="meta-llama/llama-4-scout-17b-16e-instruct",  # Groq ë˜ëŠ” ì›í•˜ëŠ” ëª¨ë¸
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )

    rewritten_question = response.choices[0].message.content.strip()
    print(f"ğŸ” ì¬ì‘ì„±ëœ ì§ˆë¬¸ (ë³´ê°•): {rewritten_question}")
    return rewritten_question