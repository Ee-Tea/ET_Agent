import streamlit as st
from solution_agent import graph, RAGState, HuggingFaceEmbeddings, Milvus, connections
from pdf_generation import generate_pdf
import os
import json
from dotenv import load_dotenv

load_dotenv()
st.set_page_config(layout="wide")
st.title("ğŸ§  ë¬¸ì œ ìë™ í•´ë‹µ ìƒì„±ê¸°")

# âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„
@st.cache_resource
def init_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )
    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port": "19530"}
    )
    return vectorstore

vectorstore = init_vectorstore()

# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_question = st.text_input("ğŸ’¬ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­", value="ì´ ë¬¸ì œì˜ ì •ë‹µê³¼ ìì„¸í•œ í’€ì´ë¥¼ ì•Œë ¤ì¤˜.")

# âœ… JSON ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“‚ JSON íŒŒì¼ ì—…ë¡œë“œ (question, optionsë§Œ í¬í•¨)", type="json")

all_results = []

if uploaded_file:
    user_problems = json.load(uploaded_file)

    for i, problem in enumerate(user_problems):
        st.markdown(f"---\n### ğŸ“˜ ë¬¸ì œ {i+1}")
        st.markdown(f"**ë¬¸ì œ:** {problem['question']}")
        st.markdown(f"**ë³´ê¸°:** {', '.join(problem['options'])}")

        with st.spinner(f"ğŸ§  ë¬¸ì œ {i+1}ì— ëŒ€í•œ í•´ë‹µ ìƒì„± ì¤‘..."):
            try:
                # âœ… ìƒíƒœ ì„¤ì •
                state: RAGState = {
                    "user_question": user_question,
                    "user_problem": problem["question"],
                    "user_problem_options": problem["options"],
                    "vectorstore": vectorstore,
                    "docs": [],
                    "retrieved_docs": [],
                    "similar_questions_text": "",
                    "generated_answer": "",
                    "generated_explanation": "",
                    "validated": False,
                    "chat_history": []
                }

                # âœ… LangGraph ì‹¤í–‰
                result = graph.invoke(state)

                # âœ… ê²°ê³¼ ëˆ„ì 
                all_results.append({
                    "index": i + 1,
                    "question": problem["question"],
                    "options": problem["options"],
                    "answer": result["generated_answer"],
                    "explanation": result["generated_explanation"]
                })


                # âœ… í•´ë‹µ ë³´ê¸° ë²„íŠ¼ (Expander í˜•íƒœë¡œ ìˆ¨ê¹€)
                with st.expander("ğŸ“ í•´ë‹µ ë³´ê¸°"):
                    st.markdown(f"**âœ… ì •ë‹µ:** {result['generated_answer']}")
                    st.markdown(f"**ğŸ“– í’€ì´:**\n{result['generated_explanation']}")

                # âœ… ìœ ì‚¬ ë¬¸ì œ
                with st.expander("ğŸ“š ìœ ì‚¬ ë¬¸ì œ ë³´ê¸°"):
                    st.markdown(f"```\n{result['similar_questions_text']}\n```")

                # âœ… íˆìŠ¤í† ë¦¬
                with st.expander("ğŸ“œ ì „ì²´ íˆìŠ¤í† ë¦¬"):
                    for item in result["chat_history"]:
                        st.text(item)

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if all_results:
    st.markdown("---")
    st.subheader("ğŸ“„ ì „ì²´ ê²°ê³¼ PDF ë‹¤ìš´ë¡œë“œ")
    pdf_buffer = generate_pdf(all_results)
    st.download_button(
        label="ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ",
        data=pdf_buffer,
        file_name="generated_solutions.pdf",
        mime="application/pdf"
    )