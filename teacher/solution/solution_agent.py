import os
from typing import TypedDict, List, Dict, Literal, Optional
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
import json, re
from langchain_openai import ChatOpenAI
from teacher.base_agent import BaseAgent
from docling.document_converter import DocumentConverter

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")




# âœ… ìƒíƒœ ì •ì˜
class SolutionState(TypedDict):
    user_question: str
    user_problems: List[Dict]
    user_problem: str
    user_problem_options: List[str]

    source_type: Literal["internal", "external"]
    # ë‚´ë¶€/ì™¸ë¶€ ì›ì²œ
    short_term_memory: List[Dict]
    external_file_paths: List[str] 

    vectorstore: Milvus
    retrieved_docs: List[Document]
    similar_questions_text : str

    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    results: List[Dict]
    validated: bool

    exam_title: str
    difficulty: str
    subject: str

    chat_history: List[str]
class SolutionAgent(BaseAgent):
    """ë¬¸ì œ í•´ë‹µ/í’€ì´ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""

        # âœ… LangGraph êµ¬ì„±
        print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")
        
        graph = StateGraph(SolutionState)

        # ë¶„ê¸° & ë¡œë”©
        graph.add_node("route", self._route)
        graph.add_node("load_from_short_term_memory", self._load_from_stm)
        graph.add_node("load_from_external_docs", self._load_from_external)

        # ê³µí†µ ì²˜ë¦¬
        graph.add_node("search_similarity", self._search_similar_questions)
        graph.add_node("generate_solution", self._generate_solution)
        graph.add_node("validate", self._validate_solution)
        graph.add_node("store", self._store_to_vector_db)
        graph.add_node("next_problem", self._next_problem)

        graph.set_entry_point("route")
        graph.add_conditional_edges("route", lambda s: s["source_type"],
                                {"internal": "load_from_short_term_memory",
                                "external": "load_from_external_docs"})
        graph.add_edge("load_from_short_term_memory", "next_problem")
        graph.add_edge("load_from_external_docs", "next_problem")

        graph.add_edge("next_problem", "search_similarity")
        graph.add_edge("search_similarity", "generate_solution")
        graph.add_edge("generate_solution", "validate")

        graph.add_conditional_edges(
            "validate", 
            lambda s: "ok" if s["validated"] else "stop",
            {"ok": "store", "stop": END}
        )

        # ì €ì¥ í›„ ë‚¨ì€ ë¬¸ì œê°€ ìˆìœ¼ë©´ next_problemë¡œ ë£¨í”„
        graph.add_conditional_edges(
            "store",
            lambda s: "more" if len(s.get("user_problems", [])) > 0 else "done",
            {"more": "next_problem", "done": END}
        )

        return graph.compile()
    
    def _llm_extract_qas(self, text: str, llm) -> List[tuple]:
        """
        LLMì—ê²Œ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì£¼ê³ 
        [{"ë¬¸ì œ":"...","ì˜µì…˜":["...","..."]}, ...] ë§Œ ë°›ëŠ”ë‹¤.
        ì‹¤íŒ¨ ì‹œ [] ë°˜í™˜.
        """
        sys_prompt = (
            "ë„ˆëŠ” ì‹œí—˜ ë¬¸ì œ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
            "ë‹¤ì–‘í•œ ë²ˆí˜¸/ë¶ˆë¦¿(1., (1), â‘ , ê°€., -, â€¢ ë“±)ì„ ì´í•´í•˜ê³ , "
            "ë¬¸í•­ì„ 'ë¬¸ì œ'ì™€ 'ì˜µì…˜'ìœ¼ë¡œë§Œ ë¬¶ì–´ JSON ë°°ì—´ë¡œ ì¶œë ¥í•œë‹¤. "
            "ì˜µì…˜ì€ ë³´ê¸° í•­ëª©ë§Œ í¬í•¨í•˜ê³ , ì„¤ëª…/í•´ì„¤/ì •ë‹µ ë“±ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. "
            "ì‘ë‹µì€ ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ ë¬¸ì¥ì´ë‚˜ ì½”ë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ."
        )
        user_prompt = (
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸í•­ì„ ìµœëŒ€í•œ ì •í™•íˆ ì¶”ì¶œí•´ JSON ë°°ì—´ë¡œ ë§Œë“¤ì–´ì¤˜.\n"
            "ìš”êµ¬ ìŠ¤í‚¤ë§ˆ: [{\"ë¬¸ì œ\":\"...\",\"ì˜µì…˜\":[\"...\",\"...\"]}]\n"
            "ê·œì¹™:\n"
            "- ì§ˆë¬¸ ë³¸ë¬¸ì—ì„œ ë²ˆí˜¸(ì˜ˆ: '1.', '(1)', 'â‘ ', 'ê°€.' ë“±)ì™€ ë¶ˆí•„ìš”í•œ ë¨¸ë¦¬ê¸€ì€ ì œê±°.\n"
            "- ì˜µì…˜ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ë²ˆí˜¸/ë¶ˆë¦¿ ì œê±° í›„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€.\n"
            "- ì˜µì…˜ì€ 2~6ê°œê°€ ì¼ë°˜ì ì´ë©°, ê·¸ë³´ë‹¤ ë§ìœ¼ë©´ ìƒìœ„ 6ê°œê¹Œì§€ë§Œ ì‚¬ìš©.\n"
            "- ì¶”ì¶œì´ ë¶ˆê°€í•˜ë©´ ë¹ˆ ë°°ì—´([])ì„ ì¶œë ¥.\n\n"
            f"í…ìŠ¤íŠ¸:\n{text}"
        )

        try:
            resp = llm.invoke([{"role":"system","content":sys_prompt},
                            {"role":"user","content":user_prompt}])
            content = (resp.content or "").strip()

            # JSONë§Œ ë‚¨ê¸°ê¸° (í˜¹ì‹œ ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì˜€ì„ ë•Œ ëŒ€ë¹„)
            m = re.search(r"\[.*\]", content, re.S)
            if not m:
                return []
            arr = json.loads(m.group(0))

            results = []
            for item in arr:
                q = (item.get("ë¬¸ì œ") or "").strip()
                opts = [str(o).strip() for o in (item.get("ì˜µì…˜") or [])]
                if q:
                    results.append((q, opts))
            return results
        except Exception:
            return []

    def _clean_numbering(self, s: str) -> str:
        if not s:
            return s
        s = s.strip()
        # ì„ í–‰ ë²ˆí˜¸/ë¶ˆë¦¿ íŒ¨í„´ ì œê±°
        s = re.sub(r"^\s*(?:\(?\d{1,3}\)?[.)]|[â‘ -â‘³]|[A-Za-zê°€-í£][.)]|[-â€¢])\s*", "", s)
        # ë‚´ë¶€ ì´ì¤‘ ê³µë°± ì •ë¦¬
        s = re.sub(r"\s{2,}", " ", s)
        return s.strip()
    
    # --------- ë¶„ê¸° ----------
    def _route(self, state: SolutionState) -> SolutionState:
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ì±„ì›Œì¤€ source_typeì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        st = state["source_type"]
        print(f"ğŸ§­ ë¶„ê¸°: {st}")
        return state

    # --------- ë‚´ë¶€: STMì—ì„œ ë¬¸ì œ 1ê°œ êº¼ë‚´ì™€ stateì— ì„¸íŒ… ----------
    def _load_from_stm(self, state: SolutionState) -> SolutionState:
        stm = state.get("short_term_memory", [])
        state["user_problems"] = [{"question": x.get("question",""),
                                "options": x.get("options",[])} for x in stm]
        state["short_term_memory"] = []  # íë¡œ ì´ê´€
        return state
    
    # --------- ì™¸ë¶€: Doclingìœ¼ë¡œ ë¬¸ì„œ â†’ í…ìŠ¤íŠ¸ â†’ JSON(ë¬¸ì œ/ì˜µì…˜) â†’ stateì— ì„¸íŒ… ----------
    def _load_from_external(self, state: SolutionState) -> SolutionState:
        print("ğŸ“„ [ì™¸ë¶€] ì²¨ë¶€ ë¬¸ì„œ ë¡œë“œ ë° Docling ë³€í™˜")
        paths = state.get("external_file_paths", [])
        if not paths:
            raise ValueError("external_file_paths ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ë¶„ê¸°ì—ì„œëŠ” íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        converter = DocumentConverter()
        extracted_pairs: List[Dict[str, object]] = []

        # LLM (êµ¬ì¡°í™” ì „ìš©, temperature 0)
        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0
        )

        for p in paths:
            result = converter.convert(p)
            doc = result.document

            # í˜ì´ì§€ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°€ëŠ¥í•˜ë©´ í˜ì´ì§€ ê²½ê³„ ë³´ì¡´)
            if hasattr(doc, "export_to_markdown"):
                raw = doc.export_to_markdown(strict_text=True)
            elif hasattr(doc, "export_to_text"):
                raw = doc.export_to_text()
            else:
                raw = ""

            pages = [pg for pg in raw.split("\f")] if "\f" in raw else raw.split("\n\n\n")  # ê°„ë‹¨í•œ í˜ì´ì§€ ë¶„ë¦¬ í´ë°±

            for page_text in pages:
                page_text = page_text.strip()
                if not page_text:
                    continue

                # 1ì°¨: ìë™ íŒ¨í„´ íŒŒì„œ
                blocks = self._split_by_questions_auto(page_text)

                # ë¬¸í•­ ìˆ˜ê°€ ë„ˆë¬´ ì ê±°ë‚˜(ì˜ˆ: 0~1ê°œ) ì˜µì…˜ ì—†ëŠ” í•­ëª©ì´ ë§ìœ¼ë©´ LLMìœ¼ë¡œ ì¬ì‹œë„
                need_llm = (len(blocks) <= 1) or (sum(1 for q, opts in blocks if opts) <= 0)

                if need_llm:
                    llm_items = self._llm_extract_qas(page_text, llm)
                    blocks = llm_items if llm_items else blocks  # LLM ì‹¤íŒ¨í•˜ë©´ 1ì°¨ ê²°ê³¼ ìœ ì§€

                for qtext, opts in blocks:
                    qtext = self._clean_numbering(qtext)
                    opts = [self._clean_numbering(o) for o in (opts or [])]
                    # ìµœì†Œ í’ˆì§ˆ í•„í„°
                    if len(qtext) < 3:
                        continue
                    if opts and not (2 <= len(opts) <= 6):
                        opts = opts[:6]
                    extracted_pairs.append({"ë¬¸ì œ": qtext.strip(), "ì˜µì…˜": [o.strip() for o in opts]})

        if not extracted_pairs:
            raise ValueError("ë¬¸ì„œì—ì„œ ë¬¸ì œ/ë³´ê¸°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDF í¬ë§·ì„ í™•ì¸í•˜ì„¸ìš”.")

        # ì¤‘ë³µ ì œê±°(ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ì¤€)
        seen = set()
        deduped = []
        for it in extracted_pairs:
            key = it["ë¬¸ì œ"]
            if key in seen:
                continue
            seen.add(key)
            deduped.append(it)

        state["user_problems"] = [{"question": it["ë¬¸ì œ"], "options": it["ì˜µì…˜"]} for it in deduped]
        print(f"âœ… ì¶”ì¶œëœ ë¬¸í•­ ìˆ˜: {len(state['user_problems'])}")
        return state
    
    # ê°„ë‹¨í•œ ë¬¸ì œ/ë³´ê¸° íŒŒì„œ (ë¬¸ì„œ í¬ë§·ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
    def _split_by_questions(self, text: str) -> List[tuple]:
        blocks = re.split(r"\n\s*\n", text)  # ë¹ˆ ì¤„ ê¸°ì¤€ ê±°ì¹ ê²Œ ë¶„í• 
        results = []
        for b in blocks:
            lines = [ln.strip() for ln in b.splitlines() if ln.strip()]
            if not lines:
                continue
            # ì˜µì…˜ ë¼ì¸ ê°ì§€ (ìˆ«ì. ë˜ëŠ” ìˆ«ì) íŒ¨í„´)
            opts = [ln for ln in lines if re.match(r"^\(?\d+\)?[).]\s*", ln)]
            if opts:
                # ë¬¸ì œë¬¸ì€ ì˜µì…˜ ë¼ì¸ ì œì™¸ ì²« ì¤„ ìœ„ì£¼ë¡œ ì‚¬ìš©
                question_lines = [ln for ln in lines if ln not in opts]
                qtext = " ".join(question_lines) if question_lines else lines[0]
                # ì˜µì…˜ í…ìŠ¤íŠ¸ ì •ì œ: "1) ..." â†’ "..." ë¡œ
                clean_opts = [re.sub(r"^\(?\d+\)?[).]\s*", "", o) for o in opts]
                results.append((qtext, clean_opts))
        return results


    def _search_similar_questions(self, state: SolutionState) -> SolutionState:
        print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
        results = state["vectorstore"].similarity_search(state["user_problem"], k=3)

        similar_questions = []
        for i, doc in enumerate(results):
            metadata = doc.metadata
            options = json.loads(metadata.get("options", "[]"))
            answer = metadata.get("answer", "")
            explanation = metadata.get("explanation", "")

            formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i+1}]
                ë¬¸ì œ: {doc.page_content}
                ë³´ê¸°:
                """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
                ì •ë‹µ: {answer}
                í’€ì´: {explanation}
                """
            similar_questions.append(formatted)
        
        state["retrieved_docs"] = results
        state["similar_questions_text"] = "\n\n".join(similar_questions) 

        print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ.")
        print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
        return state

    def _generate_solution(self, state: SolutionState) -> SolutionState:

        print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.5
        )

        similar_problems = state.get("similar_questions_text", "")
        print("ìœ ì‚¬ ë¬¸ì œë“¤:\n", similar_problems)

        prompt = f"""
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
            {state['user_question']}
            ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
            {state['user_problem']}
            {state['user_problem_options']}

            ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
            {similar_problems}

            1. ì´ ë¬¸ì œì˜ **ì •ë‹µ**ë§Œ ê°„ê²°í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¨¼ì € ì‘ì„±í•´ ì£¼ì„¸ìš”.
            2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

            ì¶œë ¥ í˜•ì‹:
            ì •ë‹µ: ...
            í’€ì´: ...
        """

        response = llm.invoke(prompt)
        result = response.content.strip()
        print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

        answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
        explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
        state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
        state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
        state["chat_history"].append(f"Q: {state['user_question']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

        return state

    # âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

    def _validate_solution(self, state: SolutionState) -> SolutionState:
        print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
        
        llm = ChatOpenAI(
            api_key=GROQ_API_KEY,
            base_url="https://api.groq.com/openai/v1",
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0
        )

        validation_prompt = f"""
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_question']}

        ì§ˆë¬¸: {state['user_problem']}
        ì •ë‹µ: {state['generated_answer']}
        í’€ì´: {state['generated_explanation']}

        ìœ„ í•´ë‹µê³¼ í’€ì´ê°€ ë¬¸ì œ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
        ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        validation_response = llm.invoke(validation_prompt)
        result_text = validation_response.content.strip().lower()

        # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
        print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
        state["validated"] = "ë„¤" in result_text
        print(f"âœ… ê²€ì¦ ê²°ê³¼: {'í†µê³¼' if state['validated'] else 'ë¶ˆí†µê³¼'}")
        return state


    # âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
    def _store_to_vector_db(self, state: SolutionState) -> SolutionState:  
        print("\nğŸ§© [4ë‹¨ê³„] ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ì‹œì‘")

        if state["source_type"] == "external":

            vectorstore = state["vectorstore"] 

            # ì¤‘ë³µ ë¬¸ì œ í™•ì¸
            similar = vectorstore.similarity_search(state["user_problem"], k=1)
            if similar and state["user_problem"].strip() in similar[0].page_content:
                print("âš ï¸ ë™ì¼í•œ ë¬¸ì œê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥ ìƒëµ")
                return state

            # ë¬¸ì œ, í•´ë‹µ, í’€ì´ë¥¼ ê°ê° metadataë¡œ ì €ì¥
            doc = Document(
                page_content=state["user_problem"],
                metadata={
                    "options": json.dumps(state.get("user_problem_options", [])), 
                    "answer": state["generated_answer"],
                    "explanation": state["generated_explanation"]
                }
            )
            vectorstore.add_documents([doc])
            print("âœ… ë¬¸ì œ+í•´ë‹µ+í’€ì´ ì €ì¥ ì™„ë£Œ")

            return state
        else:
            print("âš ï¸ ë‚´ë¶€ ì €ì¥ì†ŒëŠ” ë²¡í„° DB ì €ì¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‚´ë¶€ ë¬¸ì œë¡œë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            # ë‚´ë¶€: ìš”êµ¬ ìŠ¤í‚¤ë§ˆ(JSON)ë¡œ íŒŒì¼ ëˆ„ì  ì €ì¥
            store_path = "./internal_store.json"
            data = {
                "exam_title": state.get("exam_title", "ë‚´ë¶€ ë¬¸ì œ ëª¨ìŒ"),
                "total_questions": 0,
                "difficulty": state.get("difficulty", "ì¤‘ê¸‰"),
                "subjects": {},  # subjectëª…: {"requested_count":0,"actual_count":n,"questions":[...]}
            }

            if os.path.exists(store_path):
                try:
                    with open(store_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                except Exception:
                    pass

            subj = state.get("subject", "ê¸°íƒ€")
            subjects = data.setdefault("subjects", {})
            bucket = subjects.setdefault(subj, {"requested_count": 0, "actual_count": 0, "questions": []})

            bucket["questions"].append({
                "question": state["user_problem"],
                "options": [f"  {i+1}. {opt}" for i, opt in enumerate(state.get("user_problem_options", []))],
                "answer": state["generated_answer"],
                "explanation": state["generated_explanation"],
                "subject": subj,
            })
            bucket["actual_count"] = len(bucket["questions"])

            # ì´ ë¬¸í•­ ìˆ˜ ì¬ê³„ì‚°
            total = 0
            for v in subjects.values():
                total += len(v.get("questions", []))
            data["total_questions"] = total

            with open(store_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ë‚´ë¶€ ë¬¸ì œ ì €ì¥(JSON ìŠ¤í‚¤ë§ˆ) ì™„ë£Œ â†’ {store_path}")

        item = {
            "question": state["user_problem"],
            "options": state["user_problem_options"],
            "generated_answer": state["generated_answer"],
            "generated_explanation": state["generated_explanation"],
            "validated": state["validated"],
            "chat_history": state.get("chat_history", []),
        }
        state.setdefault("results", []).append(item)
        return state
    
    def _next_problem(self, state: SolutionState) -> SolutionState:
        queue = state.get("user_problems", [])
        if not queue:
            raise ValueError("ì²˜ë¦¬í•  ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. user_problemsê°€ ë¹„ì–´ìˆì–´ìš”.")
        current = queue.pop(0)
        state["user_problem"] = current.get("question", "")
        state["user_problem_options"] = current.get("options", [])
        state["user_problems"] = queue
        return state

    def execute(
            self, 
            user_question: str, 
            source_type: Literal["internal", "external"],
            vectorstore: Optional[Milvus] = None,
            short_term_memory: Optional[List[Dict]] = None,
            external_file_paths: Optional[List[str]] = None,
            exam_title: str = "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ëª¨ì˜ê³ ì‚¬ (Groq ìˆœì°¨ ë²„ì „)",
            difficulty: str = "ì¤‘ê¸‰",
            subject: str = "ê¸°íƒ€",
            recursion_limit: int = 1000,
        ) -> Dict:

        # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if vectorstore is None:

            embedding_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={"device": "cpu"}
            )

            if "default" in connections.list_connections():
                connections.disconnect("default")
            connections.connect(alias="default", host="localhost", port="19530")

            vectorstore = Milvus(
                embedding_function=embedding_model,
                collection_name="problems",
                connection_args={"host": "localhost", "port": "19530"}
            )
        
        initial_state: SolutionState = {
            "user_question": user_question,
            "user_problems": [], 
            "user_problem": "",
            "user_problem_options": [],

            "source_type": source_type,
            "short_term_memory": short_term_memory or [],
            "external_file_paths": external_file_paths or [],

            "vectorstore": vectorstore,
            "retrieved_docs": [],
            "similar_questions_text": "",

            "generated_answer": "",
            "generated_explanation": "",
            "validated": False,
            "results": [],
            
            "exam_title": exam_title,
            "difficulty": difficulty,
            "subject": subject,

            "chat_history": []
        }
        
        final_state = self.graph.invoke(initial_state, config={"recursion_limit": recursion_limit})
        return final_state["results"]

if __name__ == "__main__":
    # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )

    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port": "19530"}
    )

    # âœ… JSON íŒŒì¼ ë¡œë”©
    with open("./sample_user.json", "r", encoding="utf-8") as f:
        user_problems = json.load(f)

    # âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    user_question = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” : ").strip()

    agent = SolutionAgent()
    results = agent.execute(user_question, user_problems, vectorstore)

    # ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(agent.get_graph().draw_mermaid_png())
        print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
   
    for i, result in enumerate(results):
        print(f"\n==== ë¬¸ì œ {i + 1} ====")
        print("Q:", result["question"])
        print("A:", result["generated_answer"])
        print("E:", result["generated_explanation"])
        print("ê²€ì¦:", "í†µê³¼" if result["validated"] else "ë¶ˆí†µê³¼")
        print("íˆìŠ¤í† ë¦¬:", result["chat_history"])
