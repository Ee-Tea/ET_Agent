import os
from typing import TypedDict, List
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import connections
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
import re
import json
from langchain_openai import ChatOpenAI



load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… ìƒíƒœ ì •ì˜
class RAGState(TypedDict):
    user_question: str
    user_problem: str
    user_problem_options: List[str]
    chat_history: List[str]

    vectorstore: Milvus
    docs: List[Document]
    retrieved_docs: List[Document]
    similar_questions_text : str

    generated_answer: str         # í•´ë‹µ
    generated_explanation: str   # í’€ì´
    validated: bool

def search_similar_questions(state: RAGState) -> RAGState:
    print("\nğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ ì‹œì‘")
    results = state["vectorstore"].similarity_search(state["user_problem"], k=3)

    similar_questions = []
    for i, doc in enumerate(results):
        metadata = doc.metadata
        options = json.loads(metadata.get("options", "[]"))
        answer = metadata.get("answer", "")
        explanation = metadata.get("explanation", "")

        formatted = f"""[ìœ ì‚¬ë¬¸ì œ {i+1}]
            ë¬¸ì œ: {doc.page_content}
            ë³´ê¸°:
            """ + "\n".join([f"{idx + 1}. {opt}" for idx, opt in enumerate(options)]) + f"""
            ì •ë‹µ: {answer}
            í’€ì´: {explanation}
            """
        similar_questions.append(formatted)
    
        state["retrieved_docs"] = results
        state["similar_questions_text"] = "\n\n".join(similar_questions) 

    print(f"ìœ ì‚¬ ë¬¸ì œ {len(results)}ê°œ ê²€ìƒ‰ ì™„ë£Œ.")
    print("ğŸ” [1ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ê²€ìƒ‰ í•¨ìˆ˜ ì¢…ë£Œ")
    return state

def generate_solution(state: RAGState) -> RAGState:

    print("\nâœï¸ [2ë‹¨ê³„] í•´ë‹µ ë° í’€ì´ ìƒì„± ì‹œì‘")

    llm = ChatOpenAI(
        api_key=OPENAI_API_KEY,
        base_url="https://api.groq.com/openai/v1",
        model="meta-llama/llama-4-scout-17b-16e-instruct",
        temperature=0.5
    )

    similar_problems = state.get("similar_questions_text", "")
    print("ìœ ì‚¬ ë¬¸ì œë“¤:\n", similar_problems)

    prompt = f"""
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸:
        {state['user_question']}
        ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ:
        {state['user_problem']}
        {state['user_problem_options']}

        ì•„ë˜ëŠ” ì´ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤:
        {similar_problems}

        1. ì´ ë¬¸ì œì˜ **ì •ë‹µ**ë§Œ ê°„ê²°í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¨¼ì € ì‘ì„±í•´ ì£¼ì„¸ìš”.
        2. ì´ì–´ì„œ ê·¸ ì •ë‹µì¸ ê·¼ê±°ë¥¼ ë‹´ì€ **í’€ì´ ê³¼ì •**ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

        ì¶œë ¥ í˜•ì‹:
        ì •ë‹µ: ...
        í’€ì´: ...
    """

    response = llm.invoke(prompt)
    result = response.content.strip()
    print("ğŸ§  LLM ì‘ë‹µ ì™„ë£Œ")

    answer_match = re.search(r"ì •ë‹µ:\s*(.+)", result)
    explanation_match = re.search(r"í’€ì´:\s*(.+)", result, re.DOTALL)
    state["generated_answer"] = answer_match.group(1).strip() if answer_match else ""
    state["generated_explanation"] = explanation_match.group(1).strip() if explanation_match else ""
    state["chat_history"].append(f"Q: {state['user_question']}\nP: {state['user_problem']}\nA: {state['generated_answer']}\nE: {state['generated_explanation']}")

    return state

# âœ… ì •í•©ì„± ê²€ì¦ (ê°„ë‹¨íˆ ê¸¸ì´ ê¸°ì¤€ ì‚¬ìš©)

def validate_solution(state: RAGState) -> RAGState:
    print("\nğŸ§ [3ë‹¨ê³„] ì •í•©ì„± ê²€ì¦ ì‹œì‘")
    
    llm = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.groq.com/openai/v1",
        model="meta-llama/llama-4-scout-17b-16e-instruct",
        temperature=0
    )

    validation_prompt = f"""
    ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {state['user_question']}

    ì§ˆë¬¸: {state['user_problem']}
    ì •ë‹µ: {state['generated_answer']}
    í’€ì´: {state['generated_explanation']}

    ìœ„ í•´ë‹µê³¼ í’€ì´ê°€ ë¬¸ì œ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ê³ , ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆê¹Œ?
    ì ì ˆí•˜ë‹¤ë©´ 'ë„¤', ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
    """

    validation_response = llm.invoke(validation_prompt)
    result_text = validation_response.content.strip().lower()

    # âœ… 'ë„¤'ê°€ í¬í•¨ëœ ì‘ë‹µì¼ ê²½ìš°ì—ë§Œ ìœ íš¨í•œ í’€ì´ë¡œ íŒë‹¨
    print("ğŸ“Œ ê²€ì¦ ì‘ë‹µ:", result_text)
    state["validated"] = "ë„¤" in result_text
    print(f"âœ… ê²€ì¦ ê²°ê³¼: {'í†µê³¼' if state['validated'] else 'ë¶ˆí†µê³¼'}")
    return state


# âœ… ì„ë² ë”© í›„ ë²¡í„° DB ì €ì¥
def store_to_vector_db(state: RAGState) -> RAGState:  
    print("\nğŸ§© [4ë‹¨ê³„] ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ì‹œì‘")

    vectorstore = state["vectorstore"] 

     # ì¤‘ë³µ ë¬¸ì œ í™•ì¸
    similar = vectorstore.similarity_search(state["user_problem"], k=1)
    if similar and state["user_problem"].strip() in similar[0].page_content:
        print("âš ï¸ ë™ì¼í•œ ë¬¸ì œê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥ ìƒëµ")
        return state

    # ë¬¸ì œ, í•´ë‹µ, í’€ì´ë¥¼ ê°ê° metadataë¡œ ì €ì¥
    doc = Document(
        page_content=state["user_problem"],
        metadata={
            "options": json.dumps(state.get("user_problem_options", [])), 
            "answer": state["generated_answer"],
            "explanation": state["generated_explanation"]
        }
    )
    vectorstore.add_documents([doc])
    print("âœ… ë¬¸ì œ+í•´ë‹µ+í’€ì´ ì €ì¥ ì™„ë£Œ")

    return state

# âœ… LangGraph êµ¬ì„±
print("ğŸ“š LangGraph íë¦„ êµ¬ì„± ì¤‘...")

graph = StateGraph(RAGState)

graph.add_node("search_similarity", RunnableLambda(search_similar_questions))
graph.add_node("generate_solution", RunnableLambda(generate_solution))
graph.add_node("validate", RunnableLambda(validate_solution))
graph.add_node("store", RunnableLambda(store_to_vector_db))

graph.set_entry_point("search_similarity")

graph.add_edge("search_similarity", "generate_solution")
graph.add_edge("generate_solution", "validate")
graph.add_conditional_edges(
    "validate",
    lambda s: "true" if s["validated"] else "false",
    {
        "true": "store",
        "false": END
    }
)
graph.set_finish_point("store")

graph = graph.compile()

if __name__ == "__main__":

    # âœ… Milvus ì—°ê²° ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"}
    )

    if "default" in connections.list_connections():
        connections.disconnect("default")
    connections.connect(alias="default", host="localhost", port="19530")

    vectorstore = Milvus(
        embedding_function=embedding_model,
        collection_name="problems",
        connection_args={"host": "localhost", "port": "19530"}
    )

    # âœ… JSON íŒŒì¼ ë¡œë”©
    with open("./sample_user.json", "r", encoding="utf-8") as f:
        user_problems = json.load(f)

    # âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    user_question = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” : ").strip()

    for i, problem in enumerate(user_problems):
        print(f"\n===== ë¬¸ì œ {i + 1} ì²˜ë¦¬ ì‹œì‘ =====")

        state: RAGState = {
            "user_question": user_question,
            "user_problem": problem["question"],
            "user_problem_options": problem["options"],
            "vectorstore": vectorstore,
            "docs": [],
            "retrieved_docs": [],
            "similar_questions_text": "",
            "generated_answer": "",
            "generated_explanation": "",
            "validated": False,
            "chat_history": []
        }

        # âœ… LangGraph ì‹¤í–‰
        state = graph.invoke(state)

        # âœ… ê²°ê³¼ ì¶œë ¥
        print("\nìƒì„±ëœ í•´ë‹µ:", state["generated_answer"])
        print("\nìƒì„±ëœ í’€ì´:", state["generated_explanation"])

        print("\nâœ… íˆìŠ¤í† ë¦¬:")
        for entry in state["chat_history"]:
            print(entry)

    # # ê·¸ë˜í”„ ì‹œê°í™”
    # try:
    #     graph_image_path = "agent_workflow.png"
    #     with open(graph_image_path, "wb") as f:
    #         f.write(graph.get_graph().draw_mermaid_png())
    #     print(f"\nLangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")