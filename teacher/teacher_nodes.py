from openai import OpenAI
from typing import List, Dict, Any, Optional
import os 
import json
import re
from dotenv import load_dotenv
load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
  raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì— í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

# LLM ëª¨ë¸ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "moonshotai/kimi-k2-instruct")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.2"))

client = OpenAI(base_url=os.getenv("OPENAI_BASE_URL", "https://api.groq.com/openai/v1"), api_key=openai_api_key)


def user_intent(user_question: str) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    system_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
    ì‚¬ìš©ì ì§ˆë¬¸ : {user_question}
    ì§ˆë¬¸ì˜ ì˜ë„ ì¢…ë¥˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
     - generate: ì‹œí—˜ ë¬¸ì œ ìƒì„± - ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ë§Œë“¤ê±°ë‚˜ ìƒì„±í•˜ëŠ” ê²ƒ (ì˜ˆ: "ë¬¸ì œ ë§Œë“¤ì–´ì¤˜", "5ë¬¸ì œ ì¶œì œí•´ì¤˜")
     - retrieve: ì •ë³´ ê²€ìƒ‰ - ëª¨ë¥´ëŠ” ë‹¨ì–´ ë° ìš©ì–´ì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ê²ƒ
     - analyze: ì˜¤ë‹µ ë¶„ì„ - í‹€ë¦° ë¬¸ì œë¥¼ ì •ë¦¬í•˜ê³  ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ ë³´ì™„ì  ë° ì „ëµ ìƒì„±ì„ ì¶”ì²œí•˜ëŠ” ê²ƒ
     - solution: ë¬¸ì œ í’€ì´ - ê¸°ì¡´ ë¬¸ì œì— ëŒ€í•œ ë‹µê³¼ í’€ì´, í•´ì„¤ì„ ì œê³µí•˜ëŠ” ê²ƒ (ì˜ˆ: "ë¬¸ì œ í’€ì–´ì¤˜", "ì´ê±° í’€ì–´ì¤˜", "ì´ê±° í•´ì„¤ í•´ì¤˜", "PDF í’€ì´í•´ì¤˜")
     - score: ì±„ì  - ë¬¸ì œ í’€ì´ì— ëŒ€í•œ ì±„ì  ë° í•©ê²© ì—¬ë¶€ë¥¼ íŒë‹¨ í•˜ëŠ” ê²ƒ
     
    ì¤‘ìš”í•œ êµ¬ë¶„:
    - PDF íŒŒì¼ì´ë‚˜ ê¸°ì¡´ ë¬¸ì œë¥¼ í’€ì–´ë‹¬ë¼ê³  í•˜ë©´ "solution"
    - ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ë‹¬ë¼ê³  í•˜ë©´ "generate"
    - "í’€ì–´", "í’€ì´", "í•´ì„¤" ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëŒ€ë¶€ë¶„ "solution"
    - ì‚¬ìš©ìê°€ ë³¸ì¸ì´ ì…ë ¥í•œ ë‹µì— ëŒ€í•´ ë¬»ëŠ” ê²½ìš° "score"
     
    ìœ„ 5ê°€ì§€ ì˜ë„ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ê³ , ê·¸ ì™¸ì— ë‹¨ì–´ë‚˜ ë¬¸ì¥ì€ í¬í•¨ë˜ì§€ ì•Šê²Œ ë‹µë³€í•˜ì„¸ìš”.
    string í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆì‹œ:
    "solution"
    """

    response = client.chat.completions.create(
        model=OPENAI_LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question}
        ],
        temperature=LLM_TEMPERATURE
    )
    result = response.choices[0].message.content
    
    return result.strip().lower()

def get_user_answer(user_question: str) -> list:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    system_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•˜ì„¸ìš”:
    ì‚¬ìš©ì ì§ˆë¬¸ : {user_question}
    
    ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ëª¨ë“  ë¬¸ì œëŠ” ê°ê´€ì‹ì´ë©°, ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë³´ê¸°ì˜ ìˆ«ìë§Œì„ ë¦¬ìŠ¤íŠ¸(list[str]) í˜•íƒœë¡œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    "[4, 3, 1, 2, 4, 3, 3, 2, 1, 1, 4]"
    
    ì˜ˆì‹œ:
    ì‚¬ìš©ì ì…ë ¥: "ì •ë‹µì€ 4ë²ˆ, 3ë²ˆ, 1ë²ˆ, 2ë²ˆ, 4ë²ˆ, 3ë²ˆ, 3ë²ˆ, 2ë²ˆ, 1ë²ˆ, 1ë²ˆ, 4ë²ˆì…ë‹ˆë‹¤."
    ì¶”ì¶œí•  ë‚´ìš©: "[4, 3, 1, 2, 4, 3, 3, 2, 1, 1, 4]"
    ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”. ì˜ˆì‹œ: "[]"
    """

    response = client.chat.completions.create(
        model=OPENAI_LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question}
        ],
        temperature=LLM_TEMPERATURE
    )
    
    result = response.choices[0].message.content.strip()
    
    # LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    try:
        # JSON í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì„ íŒŒì‹±
        import json
        if result.startswith('[') and result.endswith(']'):
            parsed_list = json.loads(result)
            if isinstance(parsed_list, list):
                # â˜… í•­ìƒ ë¬¸ìì—´ë¡œ ì •ê·œí™”í•´ì„œ ë°˜í™˜
                return [str(x).strip() for x in parsed_list]
        
        # JSON íŒŒì‹±ì´ ì‹¤íŒ¨í•œ ê²½ìš°, ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ìˆ«ì ì¶”ì¶œ
        import re
        numbers = re.findall(r'\d+', result)
        if numbers:
            # â˜… ì—¬ê¸°ì„œë„ ë¬¸ìì—´ ë³´ì¥
            return [n.strip() for n in numbers]
        
        # ìˆ«ìë„ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []
        
    except Exception as e:
        print(f"âš ï¸ ë‹µë³€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def extract_problem_and_options(user_query: str) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë¬¸ì œì™€ ë³´ê¸°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        user_query: ì‚¬ìš©ì ì…ë ¥ ë¬¸ìì—´
        
    Returns:
        Dict containing:
        - problem: ë¬¸ì œ ë¬¸ìì—´
        - options: ë³´ê¸° ë¦¬ìŠ¤íŠ¸
        - has_problem: ë¬¸ì œê°€ ìˆëŠ”ì§€ ì—¬ë¶€
    """
    print(f"ğŸ” [extract_problem_and_options] ì…ë ¥: {user_query}")
    
    if not user_query or not user_query.strip():
        print("âš ï¸ [extract_problem_and_options] user_queryê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {
            "problem": "",
            "options": [],
            "has_problem": False
        }
    
    system_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë¬¸ì œì™€ ë³´ê¸°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
    ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
    
    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
    {{
        "problem": "ì¶”ì¶œëœ ë¬¸ì œ ë‚´ìš©",
        "options": ["ë³´ê¸°1", "ë³´ê¸°2", "ë³´ê¸°3", "ë³´ê¸°4"],
        "has_problem": true/false
    }}
    
    ì¶”ì¶œ ê·œì¹™:
    1. ë¬¸ì œê°€ ëª…í™•í•˜ê²Œ ë³´ì´ë©´ "problem"ì— ì¶”ì¶œ
    2. ë³´ê¸°ê°€ ëª…í™•í•˜ê²Œ ë³´ì´ë©´ "options"ì— ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
    3. ë¬¸ì œë‚˜ ë³´ê¸°ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ "has_problem": false
    4. ë³´ê¸°ëŠ” ë³´í†µ 4ê°œì´ì§€ë§Œ, 3ê°œë‚˜ 5ê°œì¼ ìˆ˜ë„ ìˆìŒ
    5. ë¬¸ì œë‚˜ ë³´ê¸°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
    
    ì˜ˆì‹œ:
    - "ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ì¶”ìƒí™” ê¸°ë²•ì´ ì•„ë‹Œ ê²ƒì€? ìë£Œ ì¶”ìƒí™”, ì œì–´ ì¶”ìƒí™”, ê³¼ì • ì¶”ìƒí™”, ê°•ë„ ì¶”ìƒí™”"
    â†’ {{
        "problem": "ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ì¶”ìƒí™” ê¸°ë²•ì´ ì•„ë‹Œ ê²ƒì€?",
        "options": ["ìë£Œ ì¶”ìƒí™”", "ì œì–´ ì¶”ìƒí™”", "ê³¼ì • ì¶”ìƒí™”", "ê°•ë„ ì¶”ìƒí™”"],
        "has_problem": true
    }}
    
    - "ì•ˆë…•í•˜ì„¸ìš”"
    â†’ {{
        "problem": "",
        "options": [],
        "has_problem": false
    }}
    
    ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
    """

    try:
        print(f"ğŸ” [extract_problem_and_options] LLM í˜¸ì¶œ ì‹œì‘...")
        response = client.chat.completions.create(
            model=OPENAI_LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            temperature=LLM_TEMPERATURE
        )
        
        result = response.choices[0].message.content.strip()
        print(f"ğŸ” [extract_problem_and_options] LLM ì‘ë‹µ: {result}")
        
        # JSON íŒŒì‹± ì‹œë„
        import json
        try:
            # ë°±í‹±(```)ìœ¼ë¡œ ê°ì‹¸ì§„ JSON ì²˜ë¦¬
            cleaned_result = result.strip()
            if cleaned_result.startswith('```') and cleaned_result.endswith('```'):
                # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë°±í‹± ì œê±°
                cleaned_result = cleaned_result[3:-3].strip()
                # ì²« ë²ˆì§¸ ì¤„ì´ jsonì´ ì•„ë‹Œ ê²½ìš° ì œê±°
                if not cleaned_result.startswith('{'):
                    lines = cleaned_result.split('\n')
                    cleaned_result = '\n'.join([line for line in lines if line.strip() and not line.strip().startswith('```')])
            
            parsed_result = json.loads(cleaned_result)
            print(f"ğŸ” [extract_problem_and_options] íŒŒì‹± ê²°ê³¼: {parsed_result}")
            
            if isinstance(parsed_result, dict):
                final_result = {
                    "problem": str(parsed_result.get("problem", "")).strip(),
                    "options": parsed_result.get("options", []) if isinstance(parsed_result.get("options"), list) else [],
                    "has_problem": bool(parsed_result.get("has_problem", False))
                }
                
                # optionsê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  ì •ê·œí™”
                if final_result["options"]:
                    normalized_options = []
                    for opt in final_result["options"]:
                        if isinstance(opt, str) and opt.strip():
                            normalized_options.append(opt.strip())
                    final_result["options"] = normalized_options
                
                print(f"ğŸ” [extract_problem_and_options] ìµœì¢… ê²°ê³¼: {final_result}")
                return final_result
            else:
                print("âš ï¸ [extract_problem_and_options] íŒŒì‹±ëœ ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return {
                    "problem": "",
                    "options": [],
                    "has_problem": False
                }
                
        except json.JSONDecodeError as json_error:
            print(f"âŒ [extract_problem_and_options] JSON íŒŒì‹± ì˜¤ë¥˜: {json_error}")
            print(f"ğŸ” íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ: {result}")
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì¶”ì¶œ ì‹œë„
            if "?" in user_query and any(word in user_query for word in ["ë³´ê¸°", "ì„ íƒ", "1", "2", "3", "4"]):
                # ë¬¸ì œì™€ ë³´ê¸°ê°€ ìˆëŠ” ê²ƒ ê°™ìŒ
                return {
                    "problem": user_query.split("?")[0] + "?",
                    "options": user_query.split("?")[1].strip().split(", ") if "?" in user_query else [],
                    "has_problem": True
                }
            else:
                return {
                    "problem": "",
                    "options": [],
                    "has_problem": False
                }
        
    except Exception as e:
        print(f"âŒ [extract_problem_and_options] ë¬¸ì œ/ë³´ê¸° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {
            "problem": "",
            "options": [],
            "has_problem": False
        }

def parse_generator_input(user_question: str) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê³¼ëª©/ë¬¸í•­ìˆ˜/ë‚œì´ë„ íŒŒì‹±
    """
    system_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê³¼ëª©/ë¬¸í•­ìˆ˜/ë‚œì´ë„ë¥¼ íŒŒì‹±í•˜ì„¸ìš”:
    ì§€ì •ëœ í˜•ì‹ì˜ ì‘ë‹µ ì™¸ì— ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´, íŠ¹ìˆ˜ë¬¸ìëŠ” ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
    ì‚¬ìš©ì ì§ˆë¬¸ : {user_question}
    
    ì§€ì›í•˜ëŠ” ëª¨ë“œ:
    1. subject_quiz: ë‹¨ì¼ ê³¼ëª© ë¬¸ì œ ìƒì„±
    2. partial_exam: ì„ íƒëœ ê³¼ëª©ë“¤ì— ëŒ€í•´ ì§€ì •ëœ ë¬¸ì œ ìˆ˜ë§Œí¼ ìƒì„±
    3. full_exam: 5ê³¼ëª© ì „ì²´ ë¬¸ì œ ìƒì„± (ê° 20ë¬¸ì œì”©)
    
    ê³¼ëª©ì€ 5ê°€ì§€ ì¤‘ ì„ íƒ: (ê³¼ëª©ì€ ë¬´ì¡°ê±´ ì•„ë˜ 5ê°€ì§€ ì¤‘ ì„ íƒì„ í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, 'ê³¼ëª© ë‹¹', 'ì „ê³¼ëª©'ì„ ìš”ì²­í•  ê²½ìš° ëª¨ë“  ê³¼ëª©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.)
    - ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„, ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•, í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©, ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬
    
    ë¬¸í•­ ìˆ˜ëŠ” ê³¼ëª©ë³„ ìµœëŒ€ 40ë¬¸ì œê¹Œì§€ ê°€ëŠ¥
    ë‚œì´ë„ëŠ” ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰ ì¤‘ í•˜ë‚˜, ì–¸ê¸‰ ì—†ìœ¼ë©´ ì¤‘ê¸‰ìœ¼ë¡œ ê°„ì£¼
    ê³¼ëª© ì„ ì • ì—†ì´ ë¬¸ì œ ìˆ˜ ë§Œìœ¼ë¡œ ë¬¸ì œ ìƒì„± ìš”ì²­ ì‹œ ì „ì²´ ê³¼ëª© ìƒì„±. ì´ ë•Œ, ê³¼ëª© ë‹¹ ë¬¸ì œ ìˆ˜ëŠ” ì „ì²´ ë¬¸ì œ ìˆ˜ì˜ 1/5ë¡œ ê°„ì£¼
    ë‹¤ë¥¸ ë³€ìˆ˜ ì—†ì´ ë‹¨ìˆœí•œ ë¬¸ì œ ìƒì„± ìš”ì²­ì€ ì „ì²´ ì‹œí—˜ ë¬¸ì œ ìƒì„±ìœ¼ë¡œ ê°„ì£¼ 
    íŒŒì‹± ì˜ˆì‹œ:
    1. ê³¼ëª© ë‹¹ 3ë¬¸ì œ ë§Œë“¤ì–´ì¤˜ -> ê³¼ëª© ë‹¹ 3ë¬¸ì œ ìƒì„±
        {{
            "mode": "partial_exam",
            "selected_subjects": ["ì†Œí”„íŠ¸ì›¨ì–´ì„¤ê³„", "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œ", "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•", "í”„ë¡œê·¸ë˜ë°ì–¸ì–´í™œìš©", "ì •ë³´ì‹œìŠ¤í…œêµ¬ì¶•ê´€ë¦¬"],
            "questions_per_subject": 3,
            "difficulty": "ì¤‘ê¸‰"
        }}
    
    2. ë°ì´í„°ë² ì´ìŠ¤ 10ë¬¸ì œ ë§Œë“¤ì–´ì¤˜ -> ë°ì´í„°ë² ì´ìŠ¤ ê³¼ëª© 10ë¬¸ì œ ìƒì„±
        {{
            "mode": "subject_quiz",
            "subject_area": "ë°ì´í„°ë² ì´ìŠ¤êµ¬ì¶•",
            "target_count": 10,
            "difficulty": "ì¤‘ê¸‰"
        }}
    
    íŒŒì‹± ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
    
    ë‹¨ì¼ ê³¼ëª©ì˜ ê²½ìš°:
    {{
        "mode": "subject_quiz",
        "subject_area": "ê³¼ëª©ëª…",
        "target_count": "ë¬¸í•­ ìˆ˜",
        "difficulty": "ë‚œì´ë„"
    }}
    
    ì—¬ëŸ¬ ê³¼ëª© ì„ íƒì˜ ê²½ìš°:
    {{
        "mode": "partial_exam",
        "selected_subjects": ["ê³¼ëª©1", "ê³¼ëª©2", "ê³¼ëª©3"],
        "questions_per_subject": "ê³¼ëª©ë‹¹ ë¬¸í•­ ìˆ˜",
        "difficulty": "ë‚œì´ë„"
    }}
    
    ì „ì²´ ê³¼ëª©ì˜ ê²½ìš°:
    {{
        "mode": "full_exam",
        "difficulty": "ë‚œì´ë„"
    }}
    """

    try:
        response = client.chat.completions.create(
            model=OPENAI_LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_question}
            ],
            temperature=LLM_TEMPERATURE
        )
        
        result = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            parsed = json.loads(result)
            return parsed
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {result}")
            return {
                "mode": "full_exam",
                "difficulty": "ì¤‘ê¸‰"
            }
            
    except Exception as e:
        print(f"âš ï¸ parse_generator_input ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "mode": "full_exam",
            "difficulty": "ì¤‘ê¸‰"
        }

# teacher_nodes.py
# ë…¸ë“œ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ëŠ” í—¬í¼ í•¨ìˆ˜ë“¤ê³¼ ë¼ìš°íŒ… ë¡œì§

# ========== ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ ==========
def route_solution(state: Dict[str, Any]) -> Dict[str, Any]:
    """solution ë…¸ë“œ ë¼ìš°íŒ… - í•­ìƒ preprocessë¥¼ ë¨¼ì € ê±°ì¹¨"""
    from teacher_util import has_questions, extract_image_paths
    from pdf_preprocessor import extract_pdf_paths
    
    print(f"ğŸ” [route_solution] ìƒíƒœ í™•ì¸:")
    print(f"   user_query: {state.get('user_query', '')}")
    print(f"   artifacts: {state.get('artifacts', {})}")
    print(f"   has_questions: {has_questions(state)}")
    
    # ì‚¬ìš©ì ì…ë ¥ì—ì„œ íŒŒì¼ íƒìƒ‰
    user_query = state.get("user_query", "")
    current_artifacts = state.get("artifacts", {}) or {}
    
    # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
    extracted_images = extract_image_paths(user_query)
    if extracted_images:
        image_filenames = []
        for path in extracted_images:
            filename = path.split('\\')[-1].split('/')[-1]  # Windows/Unix ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬
            image_filenames.append(filename)
        
        current_artifacts["image_ids"] = image_filenames
        print(f"ğŸ–¼ï¸ [route_solution] ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬: {image_filenames}")
    
    # PDF íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
    extracted_pdfs = extract_pdf_paths(user_query)
    if extracted_pdfs:
        pdf_filenames = []
        for path in extracted_pdfs:
            filename = path.split('\\')[-1].split('/')[-1]  # Windows/Unix ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬
            pdf_filenames.append(filename)
        
        current_artifacts["pdf_ids"] = pdf_filenames
        print(f"ğŸ“„ [route_solution] PDF íŒŒì¼ ë°œê²¬: {pdf_filenames}")
    
    # í•­ìƒ preprocessë¥¼ ë¨¼ì € ê±°ì¹˜ë„ë¡ ì„¤ì •
    next_node = "preprocess"
    print("ğŸ”„ í•­ìƒ preprocessë¥¼ ë¨¼ì € ê±°ì³ì„œ ë¬¸ì œë¥¼ í™•ì¸í•©ë‹ˆë‹¤")
    
    print(f"ğŸ” [route_solution] ë‹¤ìŒ ë…¸ë“œ: {next_node}")
    
    # artifacts ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë°˜í™˜
    new_state = {**state}
    new_state["artifacts"] = current_artifacts
    new_state.setdefault("routing", {})
    new_state["routing"]["solution_next"] = next_node
    
    print(f"ğŸ” [route_solution] ì—…ë°ì´íŠ¸ëœ artifacts: {new_state['artifacts']}")
    print(f"ğŸ” [route_solution] ì—…ë°ì´íŠ¸ëœ routing: {new_state['routing']}")
    return new_state

def route_score(state: Dict[str, Any]) -> Dict[str, Any]:
    """score ë…¸ë“œ ë¼ìš°íŒ…"""
    from teacher_util import has_solution_answers
    
    next_node = "score" if has_solution_answers(state) else "mark_after_solution_score"
    new_state = {**state}
    new_state.setdefault("routing", {})
    new_state["routing"]["score_next"] = next_node
    return new_state

def route_analysis(state: Dict[str, Any]) -> Dict[str, Any]:
    """analysis ë…¸ë“œ ë¼ìš°íŒ…"""
    from teacher_util import has_score
    
    next_node = "analysis" if has_score(state) else "mark_after_score_analysis"
    new_state = {**state}
    new_state.setdefault("routing", {})
    new_state["routing"]["analysis_next"] = next_node
    return new_state

# ========== ë§ˆí‚¹ í•¨ìˆ˜ë“¤ ==========
def mark_after_generator_solution(state: Dict[str, Any]) -> Dict[str, Any]:
    """generator í›„ solution ì‹¤í–‰ì„ ìœ„í•œ ë§ˆí‚¹"""
    ns = {**state}
    ns.setdefault("routing", {})
    ns["routing"]["after_generator"] = "solution"
    return ns

def mark_after_solution_score(state: Dict[str, Any]) -> Dict[str, Any]:
    """solution í›„ score ì‹¤í–‰ì„ ìœ„í•œ ë§ˆí‚¹"""
    ns = {**state}
    ns.setdefault("routing", {})
    ns["routing"]["after_solution"] = "score"
    return ns

def mark_after_score_analysis(state: Dict[str, Any]) -> Dict[str, Any]:
    """score í›„ analysis ì‹¤í–‰ì„ ìœ„í•œ ë§ˆí‚¹"""
    ns = {**state}
    ns.setdefault("routing", {})
    ns["routing"]["after_score"] = "analysis"
    return ns

# ========== í¬ìŠ¤íŠ¸ ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ ==========
def post_generator_route(state: Dict[str, Any]) -> str:
    """generator ì‹¤í–‰ í›„ ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    nxt = ((state.get("routing") or {}).get("after_generator") or "").strip()
    return nxt if nxt else "await_output_mode"  # ê¸°ë³¸: PDF vs Form ì„ íƒ ëŒ€ê¸° ë…¸ë“œë¡œ ì´ë™

def post_solution_route(state: Dict[str, Any]) -> str:
    """solution ì‹¤í–‰ í›„ ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    nxt = ((state.get("routing") or {}).get("after_solution") or "").strip()
    return nxt if nxt else "generate_answer_pdf"  # ê¸°ë³¸ì ìœ¼ë¡œ ë‹µì•ˆì§‘ PDF ìƒì„±

def post_score_route(state: Dict[str, Any]) -> str:
    """score ì‹¤í–‰ í›„ ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    nxt = ((state.get("routing") or {}).get("after_score") or "").strip()
    return nxt if nxt else "analysis"  # ê¸°ë³¸ì ìœ¼ë¡œ ë¶„ì„ ì§„í–‰

def post_analysis_route(state: Dict[str, Any]) -> str:
    """analysis ì‹¤í–‰ í›„ ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    nxt = ((state.get("routing") or {}).get("after_analysis") or "").strip()
    return nxt if nxt else "generate_analysis_pdf"  # ê¸°ë³¸ì ìœ¼ë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ PDF ìƒì„±

def generate_user_response(state: Dict[str, Any]) -> str:
    """
    ì‚¬ìš©ìì—ê²Œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì„œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜
    """
    system_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì±—ë´‡ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë‹µë³€ í˜•ì‹:
    1. ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¸ì‚¬
    2. ì‹¤í–‰ëœ ì‘ì—…ë“¤ì˜ ìš”ì•½ (ê°„ê²°í•˜ê²Œ)
    3. ì£¼ìš” ê²°ê³¼ ìš”ì•½
    4. ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•ˆë‚´
    
    ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """
    
    user_query = state.get("user_query", "")
    intent = state.get("intent", "")
    shared = state.get("shared", {})
    generation = state.get("generation", {})
    solution = state.get("solution", {})
    score = state.get("score", {})
    analysis = state.get("analysis", {})
    retrieval = state.get("retrieval", {})
    artifacts = state.get("artifacts", {})
    
    # ì‹¤í–‰ëœ ì‘ì—…ë“¤ íŒŒì•…
    executed_tasks = []
    results_summary = []
    
    if intent == "retrieve" and retrieval:
        executed_tasks.append("ì •ë³´ ê²€ìƒ‰")
        if shared.get("retrieve_answer"):
            results_summary.append("ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤")
    
    if intent == "generate" and generation:
        executed_tasks.append("ë¬¸ì œ ìƒì„±")
        question_count = len(shared.get("question", []))
        if question_count > 0:
            results_summary.append(f"{question_count}ê°œì˜ ë¬¸ì œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤")
    
    if intent == "solution" or "solution" in executed_tasks:
        executed_tasks.append("ë¬¸ì œ í’€ì´")
        answer_count = len(shared.get("answer", []))
        if answer_count > 0:
            results_summary.append(f"{answer_count}ê°œ ë¬¸ì œì˜ ë‹µì•ˆê³¼ í•´ì„¤ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤")
    
    if intent == "score" or "score" in executed_tasks:
        executed_tasks.append("ì±„ì ")
        correct_count = shared.get("correct_count", 0)
        total_count = shared.get("total_count", 0)
        if total_count > 0:
            accuracy = (correct_count / total_count) * 100
            results_summary.append(f"ì±„ì  ê²°ê³¼: {correct_count}/{total_count} ì •ë‹µ ({accuracy:.1f}%)")
    
    if intent == "analyze" or "analysis" in executed_tasks:
        executed_tasks.append("ì˜¤ë‹µ ë¶„ì„")
        weak_types = shared.get("weak_type", [])
        if weak_types:
            results_summary.append(f"ì·¨ì•½ì  ë¶„ì„ ì™„ë£Œ: {', '.join(map(str, weak_types[:3]))}{'...' if len(weak_types) > 3 else ''}")
    
    # PDF ìƒì„± í™•ì¸
    generated_pdfs = artifacts.get("generated_pdfs", [])
    if generated_pdfs:
        executed_tasks.append("PDF ìƒì„±")
        pdf_count = len(generated_pdfs)
        results_summary.append(f"{pdf_count}ê°œì˜ PDF íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤")
    
    # ì‚¬ìš©ì ì¹œí™”ì ì¸ ë‹µë³€ ìƒì„±
    user_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
    
ì‹¤í–‰ëœ ì‘ì—…ë“¤: {', '.join(executed_tasks) if executed_tasks else 'ì—†ìŒ'}
ì£¼ìš” ê²°ê³¼: {'; '.join(results_summary) if results_summary else 'ê²°ê³¼ ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model=OPENAI_LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=LLM_TEMPERATURE
        )
        
        result = response.choices[0].message.content.strip()
        return result if result else "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        
    except Exception as e:
        print(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ ë‹µë³€ ë°˜í™˜
        if executed_tasks:
            return f"ì•ˆë…•í•˜ì„¸ìš”! {', '.join(executed_tasks)} ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. {'; '.join(results_summary)}"
        else:
            return "ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."